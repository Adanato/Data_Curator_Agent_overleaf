\meetingheader
    {Feb 24, 2026}
    {baseline-runs}
    {\meetinglinks{}}
    {\begin{itemize}
        \item DataAgent (gpt-oss-120b) first runs: tool neutrality, prompt explicitness, failure modes
        \item One-shot baseline evaluation: Claude Code (Opus 4.6) vs OpenAI Codex (gpt-5.3, xhigh)
        \item Setting up docker environments.
    \end{itemize}}
    {ARC API was down yesterday. ARC should be back.}

%% ============================================================
%% SECTION 1: DATA AGENT EXPERIMENTS
%% ============================================================

\newpage
\section{DataAgent Experiments (gpt-oss-120b)}
No major changes just running the 
Single-agent reason--act--observe loop backed by \texttt{gpt-oss-120b},
with typed Python tools and a composable system prompt from the task YAML's \texttt{prompt\_style} field.
Three failure modes surfaced:

\begin{enumerate}
    \item \textbf{Tool presentation bias. (minor)}
        If we aren't careful about the tool description itself, the agent will treat tool descriptions as an implicit pipeline ordering and a lack of detail could cause the agent to ignore the tool or call it at the wrong time. 
    \item \textbf{System Prompt explicitness.}
    Without step-by-step pipeline instructions, \texttt{gpt-oss-120b} skips tools, hallucinates arguments,
    or terminates early.
    The \texttt{preselection} prompt required the following six explicit steps:
    \begin{enumerate}
        \item Profile
        \item Image Filter
        \item Assess
        \item Compute Mix
        \item Mix
        \item Submit (custom tool I added for the model to terminate itself)
    \end{enumerate}

    \item \textbf{Tool environment and parameters}
        The action space for deciding the arguements of the tools are so big and how do we allow the agent to stay within the environment? We have to hard code the tools for now which reduces the flexibility of our approach.
\end{enumerate}


However there is hope...
%% ============================================================
%% SECTION 2: CODING AGENT BASELINES
%% ============================================================

\newpage
\section{Coding Agent Baselines}
\label{sec:baselines}

Spent my weekend on setting this up and making sure it could prove the potential of our specialized data agent.

\subsection{Experimental Setup}

Docker containers mount FineVision5 \textbf{read-only} with no access to our curation agent codebase.
Pre-installed: \texttt{datasets}, \texttt{pyarrow}, \texttt{numpy}, \texttt{Pillow}, \texttt{uv}.
Same one-shot guided prompt; each agent wrote its own pipeline from scratch.
Claude: 50 max turns, adaptive thinking (high).
Codex: \texttt{-{}-full-auto}, \texttt{xhigh} reasoning effort.

I gave these agents full automated control of their environments with unrestricted native tool calling.

The system prompt given to both agents:

\begin{verbatim}
You are a data curation agent. Your job is to prepare a
high-quality fine-tuning dataset.

## Task
Curate visual instruction-tuning data for
Qwen2.5-VL-7B-Instruct to maximize performance on MMStar,
OCRBench, MathVista, HallusionBench, MMVet, and MMMU.

## Target Model
Qwen/Qwen2.5-VL-7B-Instruct

## Sample Budget
100000 total samples

## Available Datasets
HuggingFace Arrow format on disk at
/helios-storage/.../FineVision5/:
- LLaVA_Instruct_150K
- vqav2
- ocrvqa
- captcha
- CoSyn_400k_chart

Each dataset has an "images" column (list of PIL images)
and a "texts" column (list of {user, assistant} dicts).

## Pipeline
1. Profile - Load and inspect each dataset.
2. Image Filter - Remove rows with missing/corrupt images,
   min side < 200px, or aspect ratio > 3:1.
3. Assess - Assign a quality score (0-1) per dataset.
4. Compute Mix - Quality-weighted ratios. Total <= 100000.
5. Mix - Sample and concatenate into one combined dataset.
6. Submit - Save and print JSON summary.

## Output
Save to /workspace/output/benchmark/dataset/ in Arrow format.
Print JSON summary: datasets, ratios, sample_counts, total,
reasoning.
\end{verbatim}

\noindent CLI invocations:

\begin{verbatim}
# Claude
claude -p "$(cat task_prompt.txt)" \
  --dangerously-skip-permissions \
  --max-turns 50 \
  --output-format stream-json \
  2>&1 | tee run.jsonl

# Codex (xhigh)
codex exec "$(cat task_prompt.txt)" \
  --full-auto -m gpt-5.3-codex \
  -c model_reasoning_effort=xhigh \
  --json 2>&1 | tee run.jsonl
\end{verbatim}

\noindent Full JSONL logs are in \texttt{2026-02-24/logs/}.

\subsection{Runtime and Resource Usage}

it is quite costly and time consuming to run these and codex seems to quite early even though.

\begin{table}[H]
\centering
\caption{Baseline agent runtime comparison}
\label{tab:baseline-runtime}
\begin{tabular}{lcc}
\toprule
                        & \textbf{Claude Code} & \textbf{OpenAI Codex} \\
\midrule
Model                   & claude-opus-4-6     & gpt-5.3-codex \\
Reasoning               & adaptive (high)     & xhigh \\
Wall time               & 44.9 min            & 11.0 min \\
API turns               & 51                  & 1 \\
Shell commands          & $\sim$40            & 42 \\
Cost                    & \$2.16              & --- \\
Exit code               & 0                   & 0 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Dataset Mix}

Both agents invented quality formulas from scratch and reported their reasoning in their output logs.

\textbf{Claude's formula} (L863): two-stage scoring.
First, a per-dataset composite quality score:
\[
q = 0.30 \cdot \frac{\text{relevance}}{5} + 0.30 \cdot \frac{\text{img\_corr}}{5} + 0.25 \cdot \frac{\text{vis\_dep}}{5} + 0.15 \cdot \frac{\text{formatting}}{5}
\]
Then benchmark-relevance multipliers: LLaVA $\times$1.2 (``covers 4/6 benchmarks''),
CoSyn $\times$1.3 (``unique MathVista coverage, highest image\_correspondence''),
ocrvqa $\times$1.1 (``critical for OCRBench''),
vqav2 $\times$0.8 (``supplementary short-form VQA'').
Final ratio = adjusted score / total. Within each dataset, rows ranked by per-row composite score and top-$k$ selected.

\textbf{Codex's formula} (run.jsonl): single-stage blend of empirical retention and hardcoded relevance priors:
\[
q = 0.45 \cdot \text{retention} + 0.55 \cdot \text{relevance\_prior}
\]
where \texttt{relevance\_prior} values were invented inline (LLaVA: 0.95, CoSyn: 0.93, ocrvqa: 0.92, vqav2: 0.78, captcha: 0.35).
Allocation weights are quality scores directly (no sqrt). Sampling is random within each filtered dataset.

\begin{table}[H]
\centering
\caption{Final 100k sample allocations by agent}
\label{tab:baseline-mix}
\begin{tabular}{lrrrr}
\toprule
\textbf{Dataset} & \multicolumn{2}{c}{\textbf{Claude}} & \multicolumn{2}{c}{\textbf{Codex (xhigh)}} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5}
                        & Samples & Ratio & Samples & Ratio \\
\midrule
LLaVA\_Instruct\_150K  & 30,350  & 30.4\% & 25,855  & 25.9\% \\
CoSyn\_400k\_chart      & 28,770  & 28.8\% & 25,525  & 25.5\% \\
ocrvqa                  & 22,989  & 23.0\% & 25,254  & 25.3\% \\
vqav2                   & 17,891  & 17.9\% & 23,366  & 23.4\% \\
captcha                 & 0       & 0\%    & 0       & 0\% \\
\midrule
\textbf{Total}          & \textbf{100,000} & & \textbf{100,000} & \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Image Filter Statistics}

Same thresholds across all agents ($\geq$200px min side, $\leq$3:1 aspect). All 113k captcha images rejected (150$\times$40px).

\begin{table}[H]
\centering
\caption{Image filter pass rates (consistent across all agents)}
\label{tab:baseline-filter}
\begin{tabular}{lrrl}
\toprule
\textbf{Dataset} & \textbf{Total} & \textbf{Valid} & \textbf{Rejected} \\
\midrule
LLaVA\_Instruct\_150K & 157,710 & 157,292 & 367 small, 51 aspect \\
vqav2                  &  82,772 &  82,557 & 186 small, 29 aspect \\
ocrvqa                 & 165,746 & 163,078 & 2,668 small \\
captcha                & 113,062 &       0 & 113,062 small \\
CoSyn\_400k\_chart     & 116,814 & 116,143 & 671 aspect \\
\bottomrule
\end{tabular}
\end{table}

%% ============================================================
%% SECTION 3: LOG ANALYSIS
%% ============================================================

\newpage
\section{Log Analysis}
After reading the logs, we can see three inefficient patterns the agents take to grasp the data selection domain.

\begin{enumerate}
    \item \textbf{Domain \& Exploration tax} Both agents spent 8--10 turns on environment discovery
        (\texttt{python} not found, schema crashes, permission errors, read-only FS)
        before producing any data insight. Our agent has a guided pipeline which reduces this kind of exploration tax and domain awareness cost that codex and claude have to pay.
        
    \item \textbf{No principled approach} Despite having web access, neither agent
        performed a single search to inform its allocation. Both invented quality score formulas on the fly:
        Claude used rating-column weights with benchmark multipliers (L863);
        Codex used $0.45 \cdot \text{retention} + 0.55 \cdot \text{relevance\_prior}$ (run.jsonl).
\end{enumerate}


%% ============================================================
%% SECTION 4: DISCUSSION
%% ============================================================



\subsection{Strategy Comparison}

\begin{table}[H]
\centering
\caption{Agent strategy comparison}
\label{tab:baseline-strategies}
\begin{tabular}{p{2.8cm} p{5.5cm} p{5.5cm}}
\toprule
                        & \textbf{Claude (Opus 4.6)} & \textbf{Codex (gpt-5.3)} \\
\midrule
Top allocation
    & LLaVA 30.4\%, benchmark-relevance multipliers (chart reasoning for MathVista/MMMU)
    & Near-uniform split, retention/relevance blend \\
\addlinespace
Quality scoring
    & Rating columns with per-benchmark multipliers (L863)
    & $0.45 \cdot \text{retention} + 0.55 \cdot \text{relevance\_prior}$ (run.jsonl) \\
\addlinespace
Sample selection
    & Top-$k$ by composite quality score per dataset
    & Random sampling with strict per-row image validation \\
\addlinespace
Web research
    & None (0 web searches)
    & None (0 web searches) \\
\addlinespace
Output columns
    & \texttt{images}, \texttt{texts} only
    & All metadata preserved including \texttt{source} \\
\addlinespace
Error recovery
    & Sequential iteration avoids most sandbox issues
    & 3 self-corrected failures (L215: read-only FS, semaphore, shuffle) \\
\addlinespace
Image parsing
    & \multicolumn{2}{c}{Both independently discovered raw Arrow byte parsing ($\sim$1000$\times$ faster than PIL)} \\
\bottomrule
\end{tabular}
\end{table}

\newpage

\keyfindings{
    \item \textbf{DataAgent (gpt-oss-120b):} Tool neutrality and prompt explicitness are critical.
        Without explicit workflow steps, the model skips \texttt{compute\_mix\_ratio} entirely.
        Tool ordering biases the model's pipeline decisions.
    \item \textbf{Baselines completed the task} but spent 8--10 turns on boilerplate
        (permission errors, \texttt{python} not found, schema crashes) that the DataAgent avoids.
    \item \textbf{No agent performed web research} despite having access. All quality formulas
        are improvised LLM intuition with no empirical grounding.
    \item \textbf{Codex xhigh converged to near-uniform} (25/25/25/23) while Claude
        was moderately more opinionated (30/29/23/18); the two agents diverge by up to 5.5pp
        per dataset, motivating the ensemble approach in the orchestrator.
}

For the next steps, I think pushing forward with the data agent has high promise. I talked with feiyang and believe the following tasks would be relevant on the agent design side.

\meetingtasks{
    \item \todo{}
    \item \todo{I}
    \item \todo{Explore the baesline setup: In addition to the system prompt, what if the coding agents were told to use their full capabilities to search the internet and solve the problem independently? (I can run this in the background)}
}
