\meetingheader
    {Feb 24, 2026}
    {baseline-runs}
    {\meetinglinks{}}
    {\begin{itemize}
        \item DataAgent (gpt-oss-120b) first runs: tool neutrality, prompt explicitness, failure modes
        \item One-shot baseline evaluation: Claude Code (Opus 4.6) vs OpenAI Codex (gpt-5.3, xhigh)
        \item Clean Docker sandbox, no access to curation agent codebase
    \end{itemize}}
    {Dockerfiles hardened to remove codebase leakage after first unfair run.}

%% ============================================================
%% SECTION 1: DATA AGENT EXPERIMENTS
%% ============================================================

\section{DataAgent Experiments (gpt-oss-120b)}

Feiyang and Hanze provided the initial tool implementations and the FineVision5 (LLaVA) datasets to test on.
While running the curation agent against these tools, several failure modes and design questions surfaced
that will inform multi-agent system design.

\subsection{Setup}

The curation agent uses a single-agent reason--act--observe loop (see Feb 20 notes) backed by
\texttt{gpt-oss-120b} via an OpenAI-compatible API.
The agent is given a set of typed Python tools (profiling, filtering, mix computation)
and a composable system prompt assembled from the task YAML's \texttt{prompt\_style} field.

\subsection{Observation 1: Tool Neutrality}

If the tool descriptions are not carefully neutralized, the model becomes biased toward calling
a particular tool immediately after another.
For example, when \texttt{profile\_datasets} returns quality scores, the model reliably calls
\texttt{compute\_mix\_ratio} next, \emph{even when intermediate assessment or filtering would be
more appropriate}.
The model treats tool descriptions as an implicit pipeline ordering rather than a menu of capabilities.

\textbf{What we want:} The model should reason about what the tools could \emph{achieve} given the
current state, not follow a rigid sequence implied by tool ordering or naming.
This motivates research into tool presentation strategies (randomized ordering, neutral naming,
capability-based descriptions rather than workflow-based descriptions).

\subsection{Observation 2: System Prompt Explicitness}

The system prompt must be quite explicit for \texttt{gpt-oss-120b}.
Without step-by-step workflow instructions, the model frequently:
\begin{itemize}
    \item Skips the \texttt{compute\_mix\_ratio} tool entirely and invents its own allocation in text
    \item Terminates after profiling without performing any quality assessment
    \item Calls tools with hallucinated arguments that don't match the schema
\end{itemize}

\noindent The \texttt{preselection} prompt style (which prescribes six numbered steps:
Profile $\rightarrow$ Image Filter $\rightarrow$ Assess $\rightarrow$ Compute Mix $\rightarrow$ Mix $\rightarrow$ Submit) was required to get consistent tool usage.
More open prompt styles (\texttt{task\_goal}, \texttt{open}) produced unreliable results.

\textbf{Implication:} For weaker models, the system prompt functions less as guidance and more as
a hard constraint.
The DataAgent's composable prompt system (6 information regimes from \texttt{full} to \texttt{minimal})
is designed to test exactly this axis: how much structure does the model need before it can
make autonomous decisions?

\subsection{Observation 3: Domain-Aware Tool Parameters}

The tools encode domain knowledge that the model would otherwise lack.
\texttt{vlm\_filter} has hardcoded thresholds (\texttt{min\_side=200px}, \texttt{max\_aspect=3:1})
derived from VLM training best practices.
\texttt{compute\_mix\_ratio} implements a documented formula (\texttt{quality * sqrt(effective\_size)})
rather than leaving allocation to LLM intuition.
The \texttt{SessionContext} singleton constrains tool arguments to valid dataset names, preventing
the model from calling tools on nonexistent datasets.

These constraints are precisely what the baselines lack (see Section~\ref{sec:baselines}).

\subsection{Open Questions}

\begin{itemize}
    \item How to measure the ``tool neutrality'' axis: can we quantify ordering bias?
    \item Does prompt explicitness interact with model scale? (120b vs 405b vs frontier)
    \item Can we use the orchestrator's ensemble mechanism to average over prompt styles?
\end{itemize}

%% ============================================================
%% SECTION 2: CODING AGENT BASELINES
%% ============================================================

\section{Coding Agent Baselines}
\label{sec:baselines}

\subsection{Experimental Setup}

Docker containers isolate each coding agent from helios4.
The containers mount the FineVision5 datasets \textbf{read-only} and provide an
empty workspace with no access to our curation agent codebase.
Pre-installed packages: \texttt{datasets}, \texttt{pyarrow}, \texttt{numpy}, \texttt{Pillow}, \texttt{uv}.
All agents received the same one-shot guided prompt and had to write their own pipeline from scratch.

Claude Code was limited to 50 agentic turns via \texttt{-{}-max-turns 50}.
Codex was run with \texttt{-{}-full-auto} at \texttt{xhigh} reasoning effort
to provide a fair comparison with Claude's adaptive thinking (high).

\begin{verbatim}
# Claude
claude -p "$(cat task_prompt.txt)" \
  --dangerously-skip-permissions \
  --max-turns 50 \
  --output-format stream-json \
  2>&1 | tee run.jsonl

# Codex (xhigh)
codex exec "$(cat task_prompt.txt)" \
  --full-auto -m gpt-5.3-codex \
  -c model_reasoning_effort=xhigh \
  --json 2>&1 | tee run.jsonl
\end{verbatim}

\noindent Full JSONL logs are in \texttt{2026-02-24/logs/}.

\subsection{Runtime and Resource Usage}

\begin{table}[H]
\centering
\caption{Baseline agent runtime comparison}
\label{tab:baseline-runtime}
\begin{tabular}{lcc}
\toprule
                        & \textbf{Claude Code} & \textbf{OpenAI Codex} \\
\midrule
Model                   & claude-opus-4-6     & gpt-5.3-codex \\
Reasoning               & adaptive (high)     & xhigh \\
Wall time               & 44.9 min            & 11.0 min \\
API turns               & 51                  & 1 \\
Shell commands          & $\sim$40            & --- \\
Cost                    & \$2.16              & --- \\
Exit code               & 0                   & 0 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Dataset Mix}

\begin{table}[H]
\centering
\caption{Final 100k sample allocations by agent}
\label{tab:baseline-mix}
\begin{tabular}{lrrrr}
\toprule
\textbf{Dataset} & \multicolumn{2}{c}{\textbf{Claude}} & \multicolumn{2}{c}{\textbf{Codex (xhigh)}} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5}
                        & Samples & Ratio & Samples & Ratio \\
\midrule
LLaVA\_Instruct\_150K  & 30,350  & 30.4\% & 25,855  & 25.9\% \\
CoSyn\_400k\_chart      & 28,770  & 28.8\% & 25,525  & 25.5\% \\
ocrvqa                  & 22,989  & 23.0\% & 25,254  & 25.3\% \\
vqav2                   & 17,891  & 17.9\% & 23,366  & 23.4\% \\
captcha                 & 0       & 0\%    & 0       & 0\% \\
\midrule
\textbf{Total}          & \textbf{100,000} & & \textbf{100,000} & \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Image Filter Statistics}

All agents applied the same filter thresholds (\texttt{min\_side}$\geq$200px, aspect ratio$\leq$3:1).
All 113k captcha images were rejected (150$\times$40px).

\begin{table}[H]
\centering
\caption{Image filter pass rates (consistent across all agents)}
\label{tab:baseline-filter}
\begin{tabular}{lrrl}
\toprule
\textbf{Dataset} & \textbf{Total} & \textbf{Valid} & \textbf{Rejected} \\
\midrule
LLaVA\_Instruct\_150K & 157,710 & 157,292 & 367 small, 51 aspect \\
vqav2                  &  82,772 &  82,557 & 186 small, 29 aspect \\
ocrvqa                 & 165,746 & 163,078 & 2,668 small \\
captcha                & 113,062 &       0 & 113,062 small \\
CoSyn\_400k\_chart     & 116,814 & 116,143 & 671 aspect \\
\bottomrule
\end{tabular}
\end{table}

%% ============================================================
%% SECTION 3: LOG ANALYSIS
%% ============================================================

\section{Log Analysis}

\subsection{Exploration Tax}

Both coding agents spent significant early turns on pure environmental discovery (filesystem
layout, package availability, Python binary location, schema introspection) before producing
any data insight.

\begin{table}[H]
\centering
\caption{Exploration tax: turns spent on environment discovery before first data insight.
Claude references are to \texttt{claude-opus-4-6.txt};
Codex references are to \texttt{codex-gpt-5.3.txt} (medium run, same boilerplate errors at xhigh).}
\label{tab:exploration-tax}
\begin{tabular}{p{5cm} l l}
\toprule
\textbf{Event} & \textbf{Claude} & \textbf{Codex} \\
\midrule
First \texttt{ls} to discover data layout
    & L18 & L6 \\
\texttt{pip list} / package check
    & L29 & --- \\
\texttt{python: command not found}
    & L80 & L117 \\
Permission denied (\texttt{EACCES})
    & L53 & --- \\
\texttt{AttributeError}: images is a list
    & L94--96 & --- (handled defensively) \\
Read-only filesystem error
    & --- & L215 \\
\midrule
\textbf{Turns before first useful profile}
    & $\sim$10 & $\sim$8 \\
\bottomrule
\end{tabular}
\end{table}

\noindent Both agents independently hit \texttt{python: command not found}
(Claude L80, Codex L117) and had to discover that the Docker image only provides
\texttt{python3}.
Claude additionally crashed on \texttt{images} being \texttt{List(Image)} not a single \texttt{Image}
(L94: ``\texttt{AttributeError: 'list' object has no attribute 'size'}''),
requiring a script rewrite.
Codex hit a read-only filesystem error when \texttt{datasets} tried to write a cache file into the
mounted data directory (L215: ``\texttt{OSError: Read-only file system}'').

\textbf{DataAgent comparison:} The curation agent's \texttt{SessionContext} already knows the dataset
names, the \texttt{DatasetStore} knows the schema (\texttt{images} is \texttt{List(Image)}),
and \texttt{profile\_datasets} handles column types correctly on the first call.
Zero turns are spent on environmental discovery.

\subsection{No Principled Differentiation}

Neither agent attempted to research how dataset composition affects VLM benchmark performance.
Both had unrestricted internet access (Claude has \texttt{WebSearch}; Codex runs in a network-enabled
container), yet \textbf{neither agent performed a single web search} to inform its allocation strategy.

Instead, both improvised quality-weighted formulas from scratch:

\begin{table}[H]
\centering
\caption{Improvised quality formulas by agent (log citations)}
\label{tab:quality-formulas}
\begin{tabular}{p{2.5cm} p{5.5cm} l}
\toprule
\textbf{Agent} & \textbf{Formula} & \textbf{Log ref} \\
\midrule
Claude
    & $0.30 \cdot \text{relevance} + 0.30 \cdot \text{img\_corr} + 0.25 \cdot \text{vis\_dep} + 0.15 \cdot \text{formatting}$,\newline
      then per-dataset multipliers: LLaVA $\times$1.2, CoSyn $\times$1.3, ocrvqa $\times$1.1, vqav2 $\times$0.8
    & L863 \\
\addlinespace
Codex (xhigh)
    & $0.45 \cdot \text{retention} + 0.55 \cdot \text{relevance\_prior}$, quality-weighted allocation (near-uniform result)
    & run.jsonl \\
\bottomrule
\end{tabular}
\end{table}

\noindent Both formulas were invented ad hoc with no empirical grounding.
Claude's benchmark multipliers (``CoSyn $\times$1.3 for MathVista coverage'') are pure LLM intuition;
there is no evidence in the logs that it consulted any literature or data to justify these weights.
Codex at \texttt{xhigh} reasoning effort converged to a near-uniform
25/25/25/23 split, while Claude was moderately more opinionated (30/29/23/18).
The two agents agree on captcha exclusion but diverge on everything else, with no principled
basis for either allocation.

\textbf{DataAgent comparison:} The curation agent's tools encode domain-aware defaults.
\texttt{compute\_mix\_ratio} implements a documented formula (\texttt{quality $\times$ sqrt(effective\_size)})
that the orchestrator can override via \texttt{task\_overrides} for controlled ablation.
The agent's job is strategic reasoning about \emph{which} quality signals matter, not
reinventing the allocation formula each run.

\subsection{Reproducibility and Ensembling}

Each baseline run produces \textbf{one recipe from one agent}.
Even with only two runs, the divergence is substantial:

\begin{table}[H]
\centering
\caption{Per-dataset allocation divergence between Claude and Codex xhigh}
\label{tab:allocation-variance}
\begin{tabular}{lrrr}
\toprule
\textbf{Dataset} & \textbf{Claude \%} & \textbf{Codex \%} & \textbf{$\Delta$} \\
\midrule
LLaVA\_Instruct\_150K  & 30.4 & 25.9 & 4.5pp \\
CoSyn\_400k\_chart      & 28.8 & 25.5 & 3.3pp \\
ocrvqa                  & 23.0 & 25.3 & 2.3pp \\
vqav2                   & 17.9 & 23.4 & 5.5pp \\
\bottomrule
\end{tabular}
\end{table}

\noindent The vqav2 allocation differs by 5.5 percentage points (5,500 samples)
with no principled basis for choosing one allocation over the other.

\textbf{DataAgent comparison:} The orchestrator can run $N$ agents in parallel (e.g., 5 agents $\times$
10 iterations $\times$ multiple sample sizes), extract a \texttt{DataRecipe} from each, and compute
consensus statistics via \texttt{compute\_stats()}.
This ensemble approach absorbs the variance that makes any single baseline run unreliable.

%% ============================================================
%% SECTION 4: DISCUSSION
%% ============================================================

\section{Discussion}

\subsection{Strategy Comparison}

\begin{table}[H]
\centering
\caption{Agent strategy comparison}
\label{tab:baseline-strategies}
\begin{tabular}{p{2.8cm} p{5.5cm} p{5.5cm}}
\toprule
                        & \textbf{Claude (Opus 4.6)} & \textbf{Codex (gpt-5.3)} \\
\midrule
Top allocation
    & LLaVA 30.4\%, benchmark-relevance multipliers (chart reasoning for MathVista/MMMU)
    & Near-uniform split, retention/relevance blend \\
\addlinespace
Quality scoring
    & Rating columns with per-benchmark multipliers (L863)
    & $0.45 \cdot \text{retention} + 0.55 \cdot \text{relevance\_prior}$ (run.jsonl) \\
\addlinespace
Sample selection
    & Top-$k$ by composite quality score per dataset
    & Random sampling with strict per-row image validation \\
\addlinespace
Web research
    & None (0 web searches)
    & None (0 web searches) \\
\addlinespace
Output columns
    & \texttt{images}, \texttt{texts} only
    & All metadata preserved including \texttt{source} \\
\addlinespace
Error recovery
    & Sequential iteration avoids most sandbox issues
    & 3 self-corrected failures (L215: read-only FS, semaphore, shuffle) \\
\addlinespace
Image parsing
    & \multicolumn{2}{c}{Both independently discovered raw Arrow byte parsing ($\sim$1000$\times$ faster than PIL)} \\
\bottomrule
\end{tabular}
\end{table}

\noindent \textit{Note:} An earlier run with \texttt{COPY .\ /workspace} allowed Claude
to read all tool source code. Fixed by removing the copy; results above are from the clean run.

\subsection{Why These Results Support the DataAgent Approach}

The baselines reveal three structural weaknesses that the DataAgent is designed to address:

\begin{enumerate}
    \item \textbf{Exploration tax.}
    Coding agents spend $\sim$8--10 turns on filesystem discovery, schema introspection,
    and environment debugging before producing any data insight.
    The DataAgent eliminates this entirely via \texttt{SessionContext} and \texttt{DatasetStore},
    which pre-encode dataset locations, schemas, and loading strategies.

    \item \textbf{No principled differentiation.}
    Despite having web search capabilities, neither agent researched how dataset composition
    affects VLM benchmark performance.
    Both invented quality formulas from scratch with no empirical grounding.
    The DataAgent's tools encode domain-aware parameters (filter thresholds from VLM best practices,
    documented mixing formulas) so the LLM's reasoning is spent on strategy, not formula invention.

    \item \textbf{Single-shot variance.}
    Allocations vary by up to 5.5pp across runs with no basis for choosing one over another.
    The DataAgent's orchestrator ensemble (parallel agents with \texttt{compute\_stats()})
    produces consensus recipes that absorb this variance.
\end{enumerate}

\keyfindings{
    \item \textbf{DataAgent (gpt-oss-120b):} Tool neutrality and prompt explicitness are critical.
        Without explicit workflow steps, the model skips \texttt{compute\_mix\_ratio} entirely.
        Tool ordering biases the model's pipeline decisions.
    \item \textbf{Baselines completed the task} but spent 8--10 turns on boilerplate
        (permission errors, \texttt{python} not found, schema crashes) that the DataAgent avoids.
    \item \textbf{No agent performed web research} despite having access. All quality formulas
        are improvised LLM intuition with no empirical grounding.
    \item \textbf{Codex xhigh converged to near-uniform} (25/25/25/23) while Claude
        was moderately more opinionated (30/29/23/18); the two agents diverge by up to 5.5pp
        per dataset, motivating the ensemble approach in the orchestrator.
}

\meetingtasks{
    \item \todo{Fine-tune Qwen2.5-VL-7B on both mixes and compare benchmark scores}
    \item \todo{Add \texttt{source} column requirement to task prompt}
    \item \todo{Run DataAgent (gpt-oss-120b) on same task for 3-way comparison}
    \item \todo{Investigate tool neutrality: randomize tool ordering across runs}
    \item \todo{Run orchestrator scaling study (5 agents $\times$ 10 iterations) to quantify ensemble benefit}
}
