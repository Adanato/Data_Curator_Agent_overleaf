\meetingheader
    {Feb 24, 2026}
    {baseline-runs}
    {\meetinglinks{}}
    {\begin{itemize}
        \item DataAgent (gpt-oss-120b) first runs: tool neutrality, prompt explicitness, failure modes
        \item One-shot baseline evaluation: Claude Code (Opus 4.6) vs OpenAI Codex (gpt-5.3, xhigh)
        \item Clean Docker sandbox, no access to curation agent codebase
    \end{itemize}}
    {Dockerfiles hardened to remove codebase leakage after first unfair run.}

%% ============================================================
%% SECTION 1: DATA AGENT EXPERIMENTS
%% ============================================================

\newpage
\section{DataAgent Experiments (gpt-oss-120b)}

Single-agent reason--act--observe loop backed by \texttt{gpt-oss-120b},
with typed Python tools and a composable system prompt from the task YAML's \texttt{prompt\_style} field.
Three failure modes surfaced:

\begin{enumerate}
    \item \textbf{Tool neutrality.}
        The model treats tool descriptions as an implicit pipeline ordering.
        After \texttt{profile\_datasets} returns scores, it always calls \texttt{compute\_mix\_ratio} next,
        skipping filtering. Tool presentation needs randomized ordering or capability-based descriptions.

    \item \textbf{Prompt explicitness.}
        Without step-by-step instructions, \texttt{gpt-oss-120b} skips tools, hallucinates arguments,
        or terminates early.
        The \texttt{preselection} prompt (six steps:
        Profile $\rightarrow$ Image Filter $\rightarrow$ Assess $\rightarrow$ Compute Mix $\rightarrow$ Mix $\rightarrow$ Submit)
        was required for consistent behavior.
        For weaker models, prompts act as hard constraints, not guidance.

    \item \textbf{Domain-aware tool parameters.}
        \texttt{vlm\_filter} encodes VLM best-practice thresholds (200px min side, 3:1 max aspect).
        \texttt{compute\_mix\_ratio} uses \texttt{quality * sqrt(effective\_size)}.
        \texttt{SessionContext} constrains arguments to valid dataset names.
        These are precisely what the baselines lack (Section~\ref{sec:baselines}).
\end{enumerate}

%% ============================================================
%% SECTION 2: CODING AGENT BASELINES
%% ============================================================

\newpage
\section{Coding Agent Baselines}
\label{sec:baselines}

\subsection{Experimental Setup}

Docker containers mount FineVision5 \textbf{read-only} with no access to our curation agent codebase.
Pre-installed: \texttt{datasets}, \texttt{pyarrow}, \texttt{numpy}, \texttt{Pillow}, \texttt{uv}.
Same one-shot guided prompt; each agent wrote its own pipeline from scratch.
Claude: 50 max turns, adaptive thinking (high).
Codex: \texttt{-{}-full-auto}, \texttt{xhigh} reasoning effort.

\begin{verbatim}
# Claude
claude -p "$(cat task_prompt.txt)" \
  --dangerously-skip-permissions \
  --max-turns 50 \
  --output-format stream-json \
  2>&1 | tee run.jsonl

# Codex (xhigh)
codex exec "$(cat task_prompt.txt)" \
  --full-auto -m gpt-5.3-codex \
  -c model_reasoning_effort=xhigh \
  --json 2>&1 | tee run.jsonl
\end{verbatim}

\noindent Full JSONL logs are in \texttt{2026-02-24/logs/}.

\subsection{Runtime and Resource Usage}

\begin{table}[H]
\centering
\caption{Baseline agent runtime comparison}
\label{tab:baseline-runtime}
\begin{tabular}{lcc}
\toprule
                        & \textbf{Claude Code} & \textbf{OpenAI Codex} \\
\midrule
Model                   & claude-opus-4-6     & gpt-5.3-codex \\
Reasoning               & adaptive (high)     & xhigh \\
Wall time               & 44.9 min            & 11.0 min \\
API turns               & 51                  & 1 \\
Shell commands          & $\sim$40            & --- \\
Cost                    & \$2.16              & --- \\
Exit code               & 0                   & 0 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Dataset Mix}

\begin{table}[H]
\centering
\caption{Final 100k sample allocations by agent}
\label{tab:baseline-mix}
\begin{tabular}{lrrrr}
\toprule
\textbf{Dataset} & \multicolumn{2}{c}{\textbf{Claude}} & \multicolumn{2}{c}{\textbf{Codex (xhigh)}} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5}
                        & Samples & Ratio & Samples & Ratio \\
\midrule
LLaVA\_Instruct\_150K  & 30,350  & 30.4\% & 25,855  & 25.9\% \\
CoSyn\_400k\_chart      & 28,770  & 28.8\% & 25,525  & 25.5\% \\
ocrvqa                  & 22,989  & 23.0\% & 25,254  & 25.3\% \\
vqav2                   & 17,891  & 17.9\% & 23,366  & 23.4\% \\
captcha                 & 0       & 0\%    & 0       & 0\% \\
\midrule
\textbf{Total}          & \textbf{100,000} & & \textbf{100,000} & \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Image Filter Statistics}

Same thresholds across all agents ($\geq$200px min side, $\leq$3:1 aspect). All 113k captcha images rejected (150$\times$40px).

\begin{table}[H]
\centering
\caption{Image filter pass rates (consistent across all agents)}
\label{tab:baseline-filter}
\begin{tabular}{lrrl}
\toprule
\textbf{Dataset} & \textbf{Total} & \textbf{Valid} & \textbf{Rejected} \\
\midrule
LLaVA\_Instruct\_150K & 157,710 & 157,292 & 367 small, 51 aspect \\
vqav2                  &  82,772 &  82,557 & 186 small, 29 aspect \\
ocrvqa                 & 165,746 & 163,078 & 2,668 small \\
captcha                & 113,062 &       0 & 113,062 small \\
CoSyn\_400k\_chart     & 116,814 & 116,143 & 671 aspect \\
\bottomrule
\end{tabular}
\end{table}

%% ============================================================
%% SECTION 3: LOG ANALYSIS
%% ============================================================

\newpage
\section{Log Analysis}

Three patterns emerged from the JSONL logs:

\begin{enumerate}
    \item \textbf{Exploration tax.} Both agents spent 8--10 turns on environment discovery
        (\texttt{python} not found, schema crashes, permission errors, read-only FS)
        before producing any data insight. The DataAgent's \texttt{SessionContext} eliminates this entirely.
    \item \textbf{No principled differentiation.} Despite having web access, neither agent
        performed a single search to inform its allocation. Both invented ad-hoc quality formulas:
        Claude used rating-column weights with benchmark multipliers (L863);
        Codex used $0.45 \cdot \text{retention} + 0.55 \cdot \text{relevance\_prior}$ (run.jsonl).
    \item \textbf{Single-shot variance.} Allocations diverge by up to 5.5pp per dataset
        (vqav2: 17.9\% vs 23.4\%) with no basis for choosing one over the other.
        The DataAgent's orchestrator ensemble absorbs this via \texttt{compute\_stats()}.
\end{enumerate}

%% ============================================================
%% SECTION 4: DISCUSSION
%% ============================================================

\newpage
\section{Discussion}

\subsection{Strategy Comparison}

\begin{table}[H]
\centering
\caption{Agent strategy comparison}
\label{tab:baseline-strategies}
\begin{tabular}{p{2.8cm} p{5.5cm} p{5.5cm}}
\toprule
                        & \textbf{Claude (Opus 4.6)} & \textbf{Codex (gpt-5.3)} \\
\midrule
Top allocation
    & LLaVA 30.4\%, benchmark-relevance multipliers (chart reasoning for MathVista/MMMU)
    & Near-uniform split, retention/relevance blend \\
\addlinespace
Quality scoring
    & Rating columns with per-benchmark multipliers (L863)
    & $0.45 \cdot \text{retention} + 0.55 \cdot \text{relevance\_prior}$ (run.jsonl) \\
\addlinespace
Sample selection
    & Top-$k$ by composite quality score per dataset
    & Random sampling with strict per-row image validation \\
\addlinespace
Web research
    & None (0 web searches)
    & None (0 web searches) \\
\addlinespace
Output columns
    & \texttt{images}, \texttt{texts} only
    & All metadata preserved including \texttt{source} \\
\addlinespace
Error recovery
    & Sequential iteration avoids most sandbox issues
    & 3 self-corrected failures (L215: read-only FS, semaphore, shuffle) \\
\addlinespace
Image parsing
    & \multicolumn{2}{c}{Both independently discovered raw Arrow byte parsing ($\sim$1000$\times$ faster than PIL)} \\
\bottomrule
\end{tabular}
\end{table}

\noindent \textit{Note:} An earlier run with \texttt{COPY .\ /workspace} leaked curation agent source code to Claude. Fixed by removing the copy; results above are from the clean run.

\keyfindings{
    \item \textbf{DataAgent (gpt-oss-120b):} Tool neutrality and prompt explicitness are critical.
        Without explicit workflow steps, the model skips \texttt{compute\_mix\_ratio} entirely.
        Tool ordering biases the model's pipeline decisions.
    \item \textbf{Baselines completed the task} but spent 8--10 turns on boilerplate
        (permission errors, \texttt{python} not found, schema crashes) that the DataAgent avoids.
    \item \textbf{No agent performed web research} despite having access. All quality formulas
        are improvised LLM intuition with no empirical grounding.
    \item \textbf{Codex xhigh converged to near-uniform} (25/25/25/23) while Claude
        was moderately more opinionated (30/29/23/18); the two agents diverge by up to 5.5pp
        per dataset, motivating the ensemble approach in the orchestrator.
}

\meetingtasks{
    \item \todo{Fine-tune Qwen2.5-VL-7B on both mixes and compare benchmark scores}
    \item \todo{Add \texttt{source} column requirement to task prompt}
    \item \todo{Run DataAgent (gpt-oss-120b) on same task for 3-way comparison}
    \item \todo{Investigate tool neutrality: randomize tool ordering across runs}
    \item \todo{Run orchestrator scaling study (5 agents $\times$ 10 iterations) to quantify ensemble benefit}
}
