@misc{12onyoursidedigitalteamMetroRichmondZoo2024,
  title = {Metro {{Richmond Zoo}} Welcomes 3 Capybaras},
  author = {12 On Your Side Digital Team},
  year = 2024,
  month = may,
  journal = {https://www.12onyourside.com},
  urldate = {2024-07-06},
  abstract = {Metro Richmond Zoo is welcoming three new editions to the family!},
  chapter = {Chesterfield},
  howpublished = {https://www.12onyourside.com/2024/05/22/metro-richmond-zoo-welcomes-3-capybaras/},
  langid = {english},
  file = {/Users/adam/Zotero/storage/I9FPITJR/metro-richmond-zoo-welcomes-3-capybaras.html}
}

@misc{230513300AdaptiveChameleon,
  title = {[2305.13300] {{Adaptive Chameleon}} or {{Stubborn Sloth}}: {{Revealing}} the {{Behavior}} of {{Large Language Models}} in {{Knowledge Conflicts}}},
  urldate = {2025-07-15},
  howpublished = {https://arxiv.org/abs/2305.13300},
  file = {/Users/adam/Zotero/storage/6M4EYM9W/2305.html}
}

@misc{240604093ScalingEvaluating,
  title = {[2406.04093] {{Scaling}} and Evaluating Sparse Autoencoders},
  urldate = {2025-07-15},
  howpublished = {https://arxiv.org/abs/2406.04093},
  file = {/Users/adam/Zotero/storage/T96CREBK/2406.html}
}

@misc{2IYBC3PENFBEBIWAEE4MMIFVSMjpg1300x863,
  title = {{{2IYBC3PENFBEBIWAEE4MMIFVSM}}.Jpg (1300\texttimes 863)},
  urldate = {2024-07-06},
  howpublished = {https://gray-wwbt-prod.cdn.arcpublishing.com/resizer/v2/2IYBC3PENFBEBIWAEE4MMIFVSM.jpg?auth=ae8cb1e0591efd894ce948c652de43441830a99e19b06d842abc815b98fa68f4\&width=1300\&height=863\&smart=true},
  file = {/Users/adam/Zotero/storage/DLVBURVI/2IYBC3PENFBEBIWAEE4MMIFVSM.html}
}

@misc{71xwaNrgn7L_AC_UF350350_QL80_jpg318x350,
  title = {{{71xwaNrgn7L}}.\_{{AC}}\_{{UF350}},350\_{{QL80}}\_.Jpg (318\texttimes 350)},
  urldate = {2024-10-27},
  howpublished = {https://m.media-amazon.com/images/I/71xwaNrgn7L.\_AC\_UF350,350\_QL80\_.jpg},
  file = {/Users/adam/Zotero/storage/A9TXHUZQ/71xwaNrgn7L._AC_UF350,350_QL80_.html}
}

@article{aalipourMinimumDataSet2020,
  title = {A Minimum Data Set of User Profile or Electronic Health Record for Chemical Warfare Victims' Recommender System},
  author = {Aalipour, E and Ghazisaeedi, M and Moghadam, {\relax MRS} and Shahmoradi, L and Mousavi, B and Beigy, H},
  year = 2020,
  month = jun,
  journal = {JOURNAL OF FAMILY MEDICINE AND PRIMARY CARE},
  volume = {9},
  number = {6},
  pages = {2995--3004},
  issn = {2249-4863},
  doi = {10.4103/jfmpc.jfmpc_261_20},
  abstract = {Background: There are many people who are suffering from a variety of physical and mental illnesses due to the chemical attacks. There are various technologies such as recommender systems that can identify the main concerns related to health and make efforts to address them. To design and develop a recommender system, preparation of data source of this system should be considered. The aim of this study was to determine the minimum data set for user profile or user's electronic health record in chemical warfare victims' recommender system. Methods: This applied descriptive, cross-sectional study which was conducted in 2017. A questionnaire was developed by the authors from the data elements that were collected using the data extraction form from the studied sources. Content validity of the questionnaire was confirmed by using the experts. Test-retest method was used to determine the reliability of the questionnaire. The reliability of the questionnaire with Cronbach's alpha coefficient was confirmed as 84\%. The questionnaire were submitted for related experts based on Delphi method by email or in person. Data resulting from the Delphi technique with descriptive statistics methods in SPSS software were analyzed. Results: Forty-seven nonclinical data elements and 181 clinical data elements were classified. Conclusion: Determining minimum data set of user profile or electronic health record in the recommender system for chemical warfare victims helps the health authorities to implement the recommender system which demonstrates chemical warfare victims' needs.},
  langid = {english},
  keywords = {CARE,Chemical warfare victim,DISEASES,GAS,INFORMATION,minimum data set,QUALITY,recommender system,STRATEGIES,SUPPORT}
}

@article{aalipourMinimumDataSet2020a,
  title = {A Minimum Data Set of User Profile or Electronic Health Record for Chemical Warfare Victims' Recommender System},
  author = {Aalipour, E and Ghazisaeedi, M and Moghadam, {\relax MRS} and Shahmoradi, L and Mousavi, B and Beigy, H},
  year = 2020,
  month = jun,
  journal = {JOURNAL OF FAMILY MEDICINE AND PRIMARY CARE},
  volume = {9},
  number = {6},
  pages = {2995--3004},
  issn = {2249-4863},
  doi = {10.4103/jfmpc.jfmpc_261_20},
  abstract = {Background: There are many people who are suffering from a variety of physical and mental illnesses due to the chemical attacks. There are various technologies such as recommender systems that can identify the main concerns related to health and make efforts to address them. To design and develop a recommender system, preparation of data source of this system should be considered. The aim of this study was to determine the minimum data set for user profile or user's electronic health record in chemical warfare victims' recommender system. Methods: This applied descriptive, cross-sectional study which was conducted in 2017. A questionnaire was developed by the authors from the data elements that were collected using the data extraction form from the studied sources. Content validity of the questionnaire was confirmed by using the experts. Test-retest method was used to determine the reliability of the questionnaire. The reliability of the questionnaire with Cronbach's alpha coefficient was confirmed as 84\%. The questionnaire were submitted for related experts based on Delphi method by email or in person. Data resulting from the Delphi technique with descriptive statistics methods in SPSS software were analyzed. Results: Forty-seven nonclinical data elements and 181 clinical data elements were classified. Conclusion: Determining minimum data set of user profile or electronic health record in the recommender system for chemical warfare victims helps the health authorities to implement the recommender system which demonstrates chemical warfare victims' needs.},
  langid = {english},
  keywords = {CARE,Chemical warfare victim,DISEASES,GAS,INFORMATION,minimum data set,QUALITY,recommender system,STRATEGIES,SUPPORT}
}

@misc{AbstractBackgroundImages,
  title = {Abstract {{Background Images}} - {{Free Download}} on {{Freepik}}},
  journal = {Freepik},
  urldate = {2024-12-11},
  abstract = {Find \& Download Free Graphic Resources for Abstract Background Vectors, Stock Photos \& PSD files. \ding{51} Free for commercial use \ding{51} High Quality Images \#freepik},
  howpublished = {https://www.freepik.com/free-photos-vectors/abstract-background},
  langid = {english},
  file = {/Users/adam/Zotero/storage/CPFWM9DI/abstract-background.html}
}

@misc{AceticAcidVinegar,
  title = {Acetic {{Acid}} in {{Vinegar By Direct Titration Chemistry Tutorial}}},
  urldate = {2022-07-29},
  howpublished = {https://www.ausetute.com.au/titratevinegar.html},
  file = {/Users/adam/Zotero/storage/6EZ69NWU/titratevinegar.html}
}

@misc{aditibOceanEngineeringEducation2019,
  title = {Ocean {{Engineering}}: {{Education}}, {{Colleges}}, {{Jobs And Salary}}},
  shorttitle = {Ocean {{Engineering}}},
  author = {Aditib},
  year = 2019,
  month = sep,
  journal = {Marine Insight},
  urldate = {2022-07-16},
  abstract = {Ocean engineering is a field of study that deals with marine structures and ocean study. Find out more about the jobs of ocean engineers, education requirements, salary, and future prospects.},
  howpublished = {https://www.marineinsight.com/careers-2/what-is-ocean-engineering/},
  langid = {american},
  file = {/Users/adam/Zotero/storage/MEWXF6NW/what-is-ocean-engineering.html}
}

@misc{AkitaAkitaJapan,
  title = {Akita ({{Akita}} , {{Japan}}) - {{Population Statistics}}, {{Charts}}, {{Map}}, {{Location}}, {{Weather}} and {{Web Information}}},
  urldate = {2023-04-08},
  howpublished = {https://www.citypopulation.de/en/japan/akita/\_/05201\_\_akita/},
  file = {/Users/adam/Zotero/storage/HIRXAM6K/05201__akita.html}
}

@misc{AkitaTownPresented2018,
  title = {Akita Town Presented as a Microcosm for Future Population Decline},
  year = 2018,
  month = apr,
  journal = {Japan Today},
  urldate = {2023-04-09},
  abstract = {Kakunodate, a town of around 30,000 in the southern part of Akita Prefecture incorporated into Semboku City, has been nicknamed \&quot;The little Kyoto of the Michinoku region.\&quot; Visitors come from all over Japan to see its spectacular cherry blossoms -- extending for two kilometers along the banks of the Hinokinai\dots},
  howpublished = {https://japantoday.com/category/features/kuchikomi/akita-town-presented-as-a-microcosm-for-future-population-decline},
  langid = {english},
  file = {/Users/adam/Zotero/storage/AMVR6J22/akita-town-presented-as-a-microcosm-for-future-population-decline.html}
}

@misc{AmazonTrustedAI,
  title = {Amazon {{Trusted AI Challenge}}},
  journal = {Amazon Science},
  urldate = {2024-11-30},
  abstract = {The Amazon Trusted AI Challenge is a global university competition to drive secure innovation in generative AI (GenAI) technology, which focuses on responsible AI and large language model (LLM) coding security.},
  howpublished = {https://www.amazon.science/trusted-ai-challenge},
  langid = {english},
  file = {/Users/adam/Zotero/storage/XCYW23XW/trusted-ai-challenge.html}
}

@misc{AmazonWebServices,
  title = {Amazon {{Web Services Sign-In}}},
  urldate = {2025-01-05},
  howpublished = {https://us-east-2.signin.aws.amazon.com/oauth?response\_type=code\&client\_id=arn\%3Aaws\%3Asignin\%3A\%3A\%3Aconsole\%2Fcanvas\&redirect\_uri=https\%3A\%2F\%2Fconsole.aws.amazon.com\%2Fconsole\%2Fhome\%3FhashArgs\%3D\%2523\%26isauthcode\%3Dtrue\%26nc2\%3Dh\_ct\%26src\%3Dheader-signin\%26state\%3DhashArgsFromTB\_us-east-2\_22c4ac68dcf03517\&forceMobileLayout=0\&forceMobileApp=0\&code\_challenge=6aqTlaSOIkpyDs8gwUMsh2seLuF3htsjTp-LSVv54Zg\&code\_challenge\_method=SHA-256},
  file = {/Users/adam/Zotero/storage/67KWDXVM/oauth.html}
}

@misc{AmbientSoundNoise,
  title = {Ambient {{Sound}} and {{Noise Canceling Features}} of {{Your Headphones}} \textbar{} {{Sony USA}}},
  urldate = {2024-10-27},
  howpublished = {https://www.sony.com/electronics/support/articles/00204886},
  langid = {english}
}

@misc{AncillaryAuthoringTemplate,
  title = {Ancillary {{Authoring Template Information}}},
  urldate = {2024-09-29},
  abstract = {Submitting Articles to ACM Journals},
  howpublished = {https://www.acm.org/xpages/publications/authors/submissions},
  langid = {english},
  file = {/Users/adam/Zotero/storage/GXZSNKXR/submissions.html}
}

@misc{AOE3224Ocean,
  title = {{{AOE}} 3224 - {{Ocean Structures}} at {{Virginia Polytechnic Institute}} and {{State University}} \textbar{} {{Coursicle Virginia Tech}}},
  urldate = {2022-07-14},
  howpublished = {https://www.coursicle.com/vt/courses/AOE/3224/},
  file = {/Users/adam/Zotero/storage/ZZK7E8AK/3224.html}
}

@misc{ArabicWorldwideDistribution,
  title = {Arabic - {{Worldwide}} Distribution},
  journal = {Worlddata.info},
  urldate = {2023-04-28},
  abstract = {International distribution of mother tongue Arabic including regional allocations. Mainly in in Egypt.},
  howpublished = {https://www.worlddata.info/languages/arabic.php},
  langid = {english},
  file = {/Users/adam/Zotero/storage/G4N9MZM2/arabic.html}
}

@misc{arbaRecognizedBreeds2021,
  title = {Recognized {{Breeds}}},
  author = {ARBA},
  year = 2021,
  month = jul,
  journal = {ARBA},
  urldate = {2024-07-16},
  langid = {american},
  file = {/Users/adam/Zotero/storage/ZJEJZSHR/recognized-breeds.html}
}

@misc{ArduinoReferenceArduino,
  title = {Arduino {{Reference}} - {{Arduino Reference}}},
  urldate = {2022-10-31},
  howpublished = {https://www.arduino.cc/reference/en/},
  file = {/Users/adam/Zotero/storage/UUVUYAMR/en.html}
}

@misc{arjunEnglishItalianFlag2022,
  title = {English:  {{Italian}} Flag Map},
  shorttitle = {English},
  author = {{Arjun}},
  year = 2022,
  month = jan,
  urldate = {2023-04-19},
  file = {/Users/adam/Zotero/storage/WLNUE9ER/FileFlag-map_of_Italy.html}
}

@misc{ArtClubVirginia,
  title = {Art {{Club}} at {{Virginia Tech}} - {{GobblerConnect}}},
  urldate = {2022-07-16},
  howpublished = {https://gobblerconnect.vt.edu/organization/artclubatvt},
  file = {/Users/adam/Zotero/storage/IEQ954G2/artclubatvt.html}
}

@misc{asanaHowWriteProject,
  title = {How to {{Write}} a {{Project Proposal}} [2024] {$\bullet$} {{Asana}}},
  author = {Asana},
  journal = {Asana},
  urldate = {2024-09-29},
  abstract = {A project proposal outlines everything stakeholders should know about a project. In this guide, we'll teach you how to write one so you can succeed at work.},
  howpublished = {https://asana.com/resources/project-proposal},
  langid = {english},
  file = {/Users/adam/Zotero/storage/P67XGI5B/project-proposal.html}
}

@unpublished{ashuachREVSUnlearningSensitive2024,
  title = {{{REVS}}: {{Unlearning Sensitive Information}} in {{Language Models}} via {{Rank Editing}} in the {{Vocabulary Space}}},
  author = {Ashuach, Tomer and Tutek, Martin and Belinkov, Yonatan},
  year = 2024,
  month = jun,
  journal = {arXiv [cs.CL]},
  abstract = {Large language models (LLMs) risk inadvertently memorizing and divulging sensitive or personally identifiable information (PII) seen in training data, causing privacy concerns. Current approaches to address this issue involve costly dataset scrubbing, or model filtering through unlearning and model editing, which can be bypassed through extraction attacks. We propose REVS, a novel model editing method for unlearning sensitive information from LLMs. REVS identifies and modifies a small subset of neurons relevant for each piece of sensitive information. By projecting these neurons to the vocabulary space (unembedding), we pinpoint the components driving its generation. We then compute a model edit based on the pseudo-inverse of the unembedding matrix, and apply it to de-promote generation of the targeted sensitive data. To adequately evaluate our method on truly sensitive information, we curate two datasets: an email dataset inherently memorized by GPT-J, and a synthetic social security number dataset that we tune the model to memorize. Compared to other state-of-the-art model editing methods, REVS demonstrates superior performance in both eliminating sensitive information and robustness to extraction attacks, while retaining integrity of the underlying model. The code and a demo notebook are available at https://technion-cs-nlp.github.io/REVS.},
  isbn = {2406.09325}
}

@article{assmannJapansShrinkingRegions2014,
  title = {Japan's {{Shrinking Regions}} in the 21st {{Century}}: {{Contemporary Responses}} to {{Depopulation}} and {{Socioeconomic Decline}}. {{Peter Matanle}} and {{Anthony Rausch}} with {{The Shrinking Regions Group}}. {{Amherst}}, {{N}}.{{Y}}.: {{Cambria Press}}, 2011. {{In}}: {{The Journal}} of {{Asian Studies}}, Volume 73, Issue 04, Pp. 1135-1137.},
  author = {Assmann, Stephanie},
  year = 2014,
  month = jan,
  journal = {The Journal of Asian Studies},
  volume = {73},
  pages = {1135--1137}
}

@misc{atlassianHowWriteEffective2022,
  title = {How to Write an Effective Project Proposal},
  author = {Atlassian},
  year = 2022,
  month = jun,
  journal = {Work Life by Atlassian},
  urldate = {2024-09-29},
  abstract = {Learn the parts of a project proposal, tips and tools to organize research and ideas, and how to write a persuasive project proposal for any project.},
  howpublished = {https://www.atlassian.com/blog/project-management/how-to-write-project-proposal},
  langid = {american},
  file = {/Users/adam/Zotero/storage/S8E9RHHQ/how-to-write-project-proposal.html}
}

@misc{AudioEngineerNBCUniversal,
  title = {Audio Engineer at {{NBCUniversal}} - {{Tarta}}.Ai},
  urldate = {2022-07-30},
  howpublished = {https://tarta.ai/j/5JxTOYIBRZB4gUQRugD80722-audio-engineer-in-stamford-connecticut-at-nbcuniversal?utm\_campaign=google\_jobs\_apply\&utm\_source=google\_jobs\_apply\&utm\_medium=organic},
  file = {/Users/adam/Zotero/storage/2MNANYP5/5JxTOYIBRZB4gUQRugD80722-audio-engineer-in-stamford-connecticut-at-nbcuniversal.html}
}

@misc{aUndergraduate,
  title = {Undergraduate},
  author = {A, Texas and of Engineering, M. University College and Tamu, 3127 and Station, College and {Tx 77843-3127}},
  urldate = {2022-07-14},
  howpublished = {https://engineering.tamu.edu/ocean/academics/degrees/undergraduate/index.html},
  langid = {english},
  file = {/Users/adam/Zotero/storage/HYI36HT5/index.html}
}

@misc{AustraliaCultureCustoms,
  title = {Australia - {{Culture}}, {{Customs}} and {{Etiquette}}},
  urldate = {2023-04-19},
  howpublished = {https://www.commisceo-global.com/resources/country-guides/australia-guide},
  file = {/Users/adam/Zotero/storage/WB2UTBUL/australia-guide.html}
}

@misc{AustraliaHistoryBritannica,
  title = {Australia - {{History}} \textbar{} {{Britannica}}},
  urldate = {2023-04-19},
  abstract = {This article discusses the history of Australia from the arrival of European explorers in the 16th century to the present. For a more detailed discussion of Aboriginal culture, see Australian Aboriginal peoples. Prior to documented history, travelers from Asia may have reached Australia. China's control of South Asian waters could have extended to a landing in Australia in the early 15th century. Likewise, Muslim voyagers who visited and settled in Southeast Asia came within 300 miles (480 km) of Australia, and adventure, wind, or current might have carried some individuals the extra distance. Both Arab and Chinese documents tell of},
  howpublished = {https://www.britannica.com/place/Australia/History},
  langid = {english},
  file = {/Users/adam/Zotero/storage/WG44IDGM/History.html}
}

@misc{author1WhyAllAnimals2022,
  title = {Why Do All Animals Love Capybaras?},
  author = {{author1}, author1},
  year = 2022,
  month = jan,
  journal = {Animal-Club.co.uk},
  urldate = {2024-07-08},
  abstract = {These large herbivores often live in groups of 10-20, making them highly sociable and causing other animals to, well, LOVE capybaras. And who can blame them?},
  howpublished = {https://animal-club.co.uk/why-do-all-animals-love-capybaras/},
  langid = {british},
  file = {/Users/adam/Zotero/storage/XIFFBBG2/why-do-all-animals-love-capybaras.html}
}

@misc{AvigilonPresenceDetector2019,
  title = {Avigilon {{Presence Detector}} >> {{Avigilon}}},
  year = 2019,
  month = sep,
  urldate = {2022-09-25},
  abstract = {Discreet design and complete coverage},
  howpublished = {https://www.avigilon.com/products/cameras-sensors/apd},
  langid = {english},
  file = {/Users/adam/Zotero/storage/QMFV3PU5/apd.html}
}

@misc{BachelorsDegreeMaterials,
  title = {Bachelor's {{Degree}} in {{Materials Science}} and {{Engineering}} \textbar{} {{Degrees}} \& {{Programs}}},
  journal = {University of Nevada, Reno},
  urldate = {2022-07-14},
  abstract = {The bachelor's degree program in materials science and engineering at the University of Nevada, Reno teaches students to understand and apply advanced science and engineering principles to different materials such as ceramics, glasses or metals in order to develop new or better products.},
  howpublished = {https://www.unr.edu/degrees/majors/materials-science},
  langid = {american},
  file = {/Users/adam/Zotero/storage/TKHU5965/materials-science.html}
}

@article{badmanPerceptionsSocialRigidity2022,
  title = {Perceptions of Social Rigidity Predict Loneliness across the {{Japanese}} Population},
  author = {Badman, Ryan P. and Nordstr{\"o}m, Robert and Ueda, Michiko and Akaishi, Rei},
  year = 2022,
  month = sep,
  journal = {Scientific Reports},
  volume = {12},
  number = {1},
  pages = {16073},
  publisher = {Nature Publishing Group},
  issn = {2045-2322},
  doi = {10.1038/s41598-022-20561-5},
  urldate = {2023-04-08},
  abstract = {Loneliness is associated with mental and physical health problems and elevated suicide risk, and is increasingly widespread in modern societies. However, identifying the primary factors underlying loneliness remains a major public health challenge. Historically, loneliness was thought to result from a lack of high-quality social connections, but broader cultural factors (e.g. social norms) are increasingly recognized to also influence loneliness. Here, we used a large-scale survey (N\,=\,4977) to assess to what degree the loneliness epidemic in Japan is associated with traditional measures of social isolation (number of close friends), cultural factors (perceptions of social rigidity, as measured by relational mobility), and socioeconomic factors (e.g. income). We confirmed that a lack of close friends is a dominant factor underlying loneliness in Japan. We also found that perceptions of the social rigidity in one's environment was a major correlate of loneliness. Subjects who perceived lower levels of rigidity in their social environments felt significantly less lonely than those who perceived higher levels of social rigidity, though the association was weak in low income males. Thus, Japanese society and other high social rigidity cultures may need to reflect on the possibility that inflexible traditional norms of socialization are exacerbating loneliness.},
  copyright = {2022 The Author(s)},
  langid = {english},
  keywords = {Human behaviour,Risk factors},
  file = {/Users/adam/Zotero/storage/9CUVA4UE/Badman et al. - 2022 - Perceptions of social rigidity predict loneliness .pdf}
}

@unpublished{barbulescuEachTextualSequence2024,
  title = {To {{Each}} ({{Textual Sequence}}) {{Its Own}}: {{Improving Memorized-Data Unlearning}} in {{Large Language Models}}},
  author = {Barbulescu, George-Octavian and Triantafillou, Peter},
  year = 2024,
  month = may,
  journal = {arXiv [cs.LG]},
  abstract = {LLMs have been found to memorize training textual sequences and regurgitate verbatim said sequences during text generation time. This fact is known to be the cause of privacy and related (e.g., copyright) problems. Unlearning in LLMs then takes the form of devising new algorithms that will properly deal with these side-effects of memorized data, while not hurting the model's utility. We offer a fresh perspective towards this goal, namely, that each textual sequence to be forgotten should be treated differently when being unlearned based on its degree of memorization within the LLM. We contribute a new metric for measuring unlearning quality, an adversarial attack showing that SOTA algorithms lacking this perspective fail for privacy, and two new unlearning methods based on Gradient Ascent and Task Arithmetic, respectively. A comprehensive performance evaluation across an extensive suite of NLP tasks then mapped the solution space, identifying the best solutions under different scales in model capacities and forget set sizes and quantified the gains of the new approaches.},
  isbn = {2405.03097}
}

@article{baxterCommercializationDyeSensitized2012,
  title = {Commercialization of Dye Sensitized Solar Cells: {{Present}} Status and Future Research Needs to Improve Efficiency, Stability, and Manufacturing},
  author = {Baxter, Jason B.},
  year = 2012,
  month = mar,
  journal = {Journal of Vacuum Science \& Technology A},
  volume = {30},
  number = {2},
  pages = {020801},
  publisher = {American Vacuum Society},
  issn = {0734-2101},
  doi = {10.1116/1.3676433},
  urldate = {2022-08-03}
}

@misc{BeerLambertLaw2013,
  title = {The {{Beer-Lambert Law}}},
  year = 2013,
  month = oct,
  journal = {Chemistry LibreTexts},
  urldate = {2022-08-05},
  abstract = {The Beer-Lambert law relates the attenuation of light to the properties of the material through which the light is traveling. This page takes a brief look at the Beer-Lambert Law and explains the use \dots},
  howpublished = {https://chem.libretexts.org/Bookshelves/Physical\_and\_Theoretical\_Chemistry\_Textbook\_Maps/Supplemental\_Modules\_(Physical\_and\_Theoretical\_Chemistry)/Spectroscopy/Electronic\_Spectroscopy/Electronic\_Spectroscopy\_Basics/The\_Beer-Lambert\_Law},
  langid = {english},
  file = {/Users/adam/Zotero/storage/K83UZ6PB/The_Beer-Lambert_Law.html}
}

@article{bhattPurpleLlamaCYBERSECEVAL,
  title = {Purple {{Llama CYBERSECEVAL}}: {{A Secure Coding Benchmark}} for {{Language Models}}},
  author = {Bhatt, Manish and Chennabasappa, Sahana and Nikolaidis, Cyrus and Wan, Shengye and Evtimov, Ivan and Gabi, Dominik and Song, Daniel and Ahmad, Faizan and Aschermann, Cornelius and Fontana, Lorenzo and Giri, Ravi Prakash and Kapil, Dhaval and Kozyrakis, Yiannis and LeBlanc, David and Milazzo, James and Straumann, Aleksandar and Synnaeve, Gabriel and Vontimitta, Varun and Whitman, Spencer and Saxe, Joshua},
  langid = {english},
  file = {/Users/adam/Zotero/storage/RG39AVNB/Bhatt et al. - Purple Llama CYBERSECEVAL A Secure Coding Benchma.pdf}
}

@misc{BlackWhiteImage,
  title = {A Black and White Image of a Computer Keyboard Photo -- {{Free Grey Image}} on {{Unsplash}}},
  urldate = {2024-12-05},
  howpublished = {https://unsplash.com/photos/a-black-and-white-image-of-a-computer-keyboard-BMn5Z-MGv5c},
  file = {/Users/adam/Zotero/storage/9PGF234R/a-black-and-white-image-of-a-computer-keyboard-BMn5Z-MGv5c.html}
}

@misc{brandfonbrenerLosstoLossPredictionScaling2024,
  title = {Loss-to-{{Loss Prediction}}: {{Scaling Laws}} for {{All Datasets}}},
  shorttitle = {Loss-to-{{Loss Prediction}}},
  author = {Brandfonbrener, David and Anand, Nikhil and Vyas, Nikhil and Malach, Eran and Kakade, Sham},
  year = 2024,
  month = nov,
  number = {arXiv:2411.12925},
  eprint = {2411.12925},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2411.12925},
  urldate = {2025-08-09},
  abstract = {While scaling laws provide a reliable methodology for predicting train loss across compute scales for a single data distribution, less is known about how these predictions should change as we change the distribution. In this paper, we derive a strategy for predicting one loss from another and apply it to predict across different pre-training datasets and from pre-training data to downstream task data. Our predictions extrapolate well even at 20x the largest FLOP budget used to fit the curves. More precisely, we find that there are simple shifted power law relationships between (1) the train losses of two models trained on two separate datasets when the models are paired by training compute (train-to-train), (2) the train loss and the test loss on any downstream distribution for a single model (train-to-test), and (3) the test losses of two models trained on two separate train datasets (test-to-test). The results hold up for pre-training datasets that differ substantially (some are entirely code and others have no code at all) and across a variety of downstream tasks. Finally, we find that in some settings these shifted power law relationships can yield more accurate predictions than extrapolating single-dataset scaling laws.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/adam/Zotero/storage/MD6EPPYI/Brandfonbrener et al. - 2024 - Loss-to-Loss Prediction Scaling Laws for All Data.pdf;/Users/adam/Zotero/storage/N74695XS/2411.html}
}

@misc{BrazilLanguageCulture,
  title = {Brazil - {{Language}}, {{Culture}}, {{Customs}} and {{Etiquette}}},
  urldate = {2023-04-19},
  howpublished = {https://www.commisceo-global.com/resources/country-guides/brazil-guide\#c1},
  file = {/Users/adam/Zotero/storage/4ISYSAKL/brazil-guide.html}
}

@misc{brutyoPhotoRichardBrutyo2019,
  title = {Photo by {{Richard Brutyo}} on {{Unsplash}}},
  author = {Brutyo, Richard},
  year = 2019,
  month = mar,
  urldate = {2024-07-17},
  abstract = {Happy Women's Day! -- Download this photo by Richard Brutyo on Unsplash},
  howpublished = {https://unsplash.com/photos/yellow-labrador-retriever-biting-yellow-tulip-flower-Sg3XwuEpybU},
  langid = {american},
  file = {/Users/adam/Zotero/storage/KJJRIPWM/yellow-labrador-retriever-biting-yellow-tulip-flower-Sg3XwuEpybU.html}
}

@misc{bunniesandsunshineHappyBunniesBinky2013,
  title = {Happy Bunnies Binky and a Bunny Rolls Over.},
  author = {{BunniesandSunshine}},
  year = 2013,
  month = dec,
  urldate = {2024-07-16},
  abstract = {Holland lop bunnies Simon and River are binkying (a bunny dance of joy) and Simon rolls himself over.  You can follow their antics at:  http://bunnies-and-sunshine.blogspot....}
}

@misc{BusinessEtiquetteBuilding,
  title = {Business Etiquette and Building Relationships in {{Qatar}} \textbar{} {{Aetna International}}},
  urldate = {2023-04-19},
  howpublished = {https://www.aetnainternational.com/en/individuals/destination-guides/guide-to-moving-to-qatar/business-etiquette-relationships.html},
  file = {/Users/adam/Zotero/storage/SIFKY3HH/business-etiquette-relationships.html}
}

@misc{CafeCapybaCapybaras,
  title = {Cafe {{Capyba Capybaras}}},
  author = {, Cafe Capyba},
  urldate = {2024-07-08},
  howpublished = {https://lh3.googleusercontent.com/p/AF1QipMq4I4C7l0tOFZPI\_-WYKEVAxKUba3-vOWwTvdj=s680-w680-h510},
  file = {/Users/adam/Zotero/storage/RA7R7AR2/AF1QipMq4I4C7l0tOFZPI_-WYKEVAxKUba3-vOWwTvdj=s680-w680-h510.html}
}

@misc{CafeCapybaMenu,
  title = {Cafe {{Capyba}} Menu},
  author = {, Cafe Capyba},
  urldate = {2024-07-08},
  howpublished = {https://lh3.googleusercontent.com/p/AF1QipOiAwia1cUsGlN-\_-IOTqOEz\_\_EN0g3BMjomR9G=s680-w680-h510},
  file = {/Users/adam/Zotero/storage/NF8VA9VS/AF1QipOiAwia1cUsGlN-_-IOTqOEz__EN0g3BMjomR9G=s680-w680-h510.html}
}

@misc{CafeCapybaraGoogle,
  title = {Cafe Capybara - {{Google Search}}},
  urldate = {2024-07-08},
  howpublished = {https://www.google.com/search?q=cafe+capybara\&rlz=1C5CHFA\_enUS1085US1085\&oq=Cafe+capyba\&gs\_lcrp=EgZjaHJvbWUqDQgAEAAY4wIYsQMYgAQyDQgAEAAY4wIYsQMYgAQyEAgBEC4YrwEYxwEYsQMYgAQyBggCEEUYOTIHCAMQABiABDIHCAQQABiABDIGCAUQRRg8MgYIBhBFGD0yBggHEEUYPNIBCDIxNTlqMGo3qAIAsAIA\&sourceid=chrome\&ie=UTF-8\#lpg=cid:CgIgARICEAE\%3D,ik:CAoSLEFGMVFpcE9pQXdpYTFjVXNHbE4tXy1JT1RxT0V6X19FTjBnM0JNam9tUjlH},
  file = {/Users/adam/Zotero/storage/Y6MUMVCV/search.html}
}

@misc{CakeSliceFree,
  title = {Cake {{Slice}} Free Icon Designed by Monkik},
  journal = {Flaticon},
  urldate = {2024-11-26},
  abstract = {Free vector icon. Download thousands of free icons of  in SVG, PSD, PNG, EPS format or as ICON FONT \#flaticon \#icon \#cake \#dessert \#food},
  howpublished = {https://www.flaticon.com/free-icon/cake-slice\_992754?term=cake\&page=1\&position=7\&origin=search\&related\_id=992754},
  langid = {english},
  file = {/Users/adam/Zotero/storage/GQB3I868/cake-slice_992754.html}
}

@misc{Capybara,
  title = {Capybara},
  author = {, Rainforest Alliance},
  journal = {Rainforest Alliance},
  urldate = {2024-07-08},
  howpublished = {https://www.rainforest-alliance.org/species/capybara/},
  langid = {american},
  file = {/Users/adam/Zotero/storage/74USTDR3/capybara.html}
}

@misc{CapybaraSanDiego,
  title = {Capybara \textbar{} {{San Diego Zoo Animals}} \& {{Plants}}},
  urldate = {2024-07-06},
  howpublished = {https://animals.sandiegozoo.org/animals/capybara},
  file = {/Users/adam/Zotero/storage/CH7UJFGZ/capybara.html}
}

@misc{CapybaraYouTube,
  title = {Capybara - {{YouTube}}},
  urldate = {2024-07-06},
  howpublished = {https://www.youtube.com/watch?v=WbjyfvaH7Ko\&t=1s},
  file = {/Users/adam/Zotero/storage/62JKJJF9/watch.html}
}

@misc{casalesPhotoGuillermoCasales2020,
  title = {Photo by {{Guillermo Casales}} on {{Unsplash}}},
  author = {Casales, Guillermo},
  year = 2020,
  month = aug,
  urldate = {2024-07-17},
  abstract = {Download this photo by Guillermo Casales on Unsplash},
  howpublished = {https://unsplash.com/photos/white-rabbit-on-green-grass-during-daytime-LQfcolSv2M0},
  langid = {american},
  file = {/Users/adam/Zotero/storage/AVG6CY4C/white-rabbit-on-green-grass-during-daytime-LQfcolSv2M0.html}
}

@misc{CaseStudyAgefriendly,
  title = {Case {{Study}}: {{The Age-friendly Programme}} in {{Akita City}}},
  shorttitle = {Case {{Study}}},
  journal = {Age-Friendly World},
  urldate = {2023-04-08},
  abstract = {In 2011 Akita was one of the first cities in Japan to join the WHO Global Network for Age-friendly Cities and Communities (GNAFCC). Most of Akita's age-friendly policies are guided by an approach focused on building the community and strengthening the social fabric of the city (1). One of the main aims of the age-friendly programme has been to create a society where older individuals are no longer perceived as people who need support but as people who support society. The programme recognises that older people want to use their experience, knowledge, and skills to be more active citizens. Akita City's three key priorities for its age-friendly programme have been (2): \ding{108} to involve the residents of Akita City in a leading role;\ding{108} to ensure co-operation between private enterprises, administrative organisations and citizens;\ding{108} to encourage cooperation between all the relevant departments in the City Government. One of the main factors in the success of the programme is the collaboration between the different parties (private enterprises, and administrative organisations and citizens) and recognition of the importance of each party being able to prioritise its own initiatives. Akita City is the capital of Akita Prefecture, located in the north-eastern part of Japan. In June 2018 its population was 310,407 people (2). Japan remains an ethnically homogeneous country and the number of foreign residents living in Akita City is currently around 1,222 (less than 0.5\%), the majority of whom are from China, South Korea, and the Philippines (2). The most striking demographic feature is the city's predicted population decline and rapidly ageing profile. The proportion of the population aged 65 and over is currently around 29\% (2). By 2040 the city's total population is predicted to fall to 244,726, of whom 44\% are predicted to be aged 65 and over. By that point,...},
  file = {/Users/adam/Zotero/storage/ZUMXJDGC/akita-city.html}
}

@misc{cenBridgingTrainingInferenceGap2024,
  title = {Bridging the {{Training-Inference Gap}} in {{LLMs}} by {{Leveraging Self-Generated Tokens}}},
  author = {Cen, Zhepeng and Liu, Yao and Zeng, Siliang and Chaudhar, Pratik and Rangwala, Huzefa and Karypis, George and Fakoor, Rasool},
  year = 2024,
  month = oct,
  number = {arXiv:2410.14655},
  eprint = {2410.14655},
  publisher = {arXiv},
  urldate = {2024-10-23},
  abstract = {Language models are often trained to maximize the likelihood of the next token given past tokens in the training dataset. However, during inference time, they are utilized differently, generating text sequentially and auto-regressively by using previously generated tokens as input to predict the next one. Marginal differences in predictions at each step can cascade over successive steps, resulting in different distributions from what the models were trained for and potentially leading to unpredictable behavior. This paper proposes two simple approaches based on model own generation to address this discrepancy between the training and inference time. Our first approach is Batch-Scheduled Sampling, where, during training, we stochastically choose between the ground-truth token from the dataset and the model's own generated token as input to predict the next token. This is done in an offline manner, modifying the context window by interleaving ground-truth tokens with those generated by the model. Our second approach is Reference-Answer-based Correction, where we explicitly incorporate a self-correction capability into the model during training. This enables the model to effectively self-correct the gaps between the generated sequences and the ground truth data without relying on an external oracle model. By incorporating our proposed strategies during training, we have observed an overall improvement in performance compared to baseline methods, as demonstrated by our extensive experiments using summarization, general question-answering, and math question-answering tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/adam/Zotero/storage/LK8PL9VB/Cen et al. - 2024 - Bridging the Training-Inference Gap in LLMs by Lev.pdf;/Users/adam/Zotero/storage/2A7M2XVA/2410.html}
}

@misc{CenterOceanEngineering,
  title = {Center for {{Ocean Engineering}} >> {{Research}}},
  urldate = {2022-07-14},
  langid = {american},
  file = {/Users/adam/Zotero/storage/8UBS2WUZ/research.html}
}

@unpublished{changLocalizationMethodsActually2023,
  title = {Do {{Localization Methods Actually Localize Memorized Data}} in {{LLMs}}? {{A Tale}} of {{Two Benchmarks}}},
  author = {Chang, Ting-Yun and Thomason, Jesse and Jia, Robin},
  year = 2023,
  month = nov,
  journal = {arXiv [cs.CL]},
  abstract = {The concept of localization in LLMs is often mentioned in prior work; however, methods for localization have never been systematically and directly evaluated. We propose two complementary benchmarks that evaluate the ability of localization methods to pinpoint LLM components responsible for memorized data. In our INJ benchmark, we actively inject a piece of new information into a small subset of LLM weights, enabling us to directly evaluate whether localization methods can identify these "ground truth" weights. In our DEL benchmark, we evaluate localization by measuring how much dropping out identified neurons deletes a memorized pretrained sequence. Despite their different perspectives, our two benchmarks yield consistent rankings of five localization methods. Methods adapted from network pruning perform well on both benchmarks, and all evaluated methods show promising localization ability. On the other hand, even successful methods identify neurons that are not specific to a single memorized sequence.},
  isbn = {2311.09060}
}

@misc{ChatGPT,
  title = {{{ChatGPT}}},
  urldate = {2025-08-15},
  abstract = {A conversational AI system that listens, learns, and challenges},
  howpublished = {https://chatgpt.com},
  langid = {american},
  file = {/Users/adam/Zotero/storage/7CC4UCBG/689e983f-a924-8326-a583-80087d483951.html}
}

@misc{chenAceReasonNemotronAdvancingMath2025,
  title = {{{AceReason-Nemotron}}: {{Advancing Math}} and {{Code Reasoning}} through {{Reinforcement Learning}}},
  shorttitle = {{{AceReason-Nemotron}}},
  author = {Chen, Yang and Yang, Zhuolin and Liu, Zihan and Lee, Chankyu and Xu, Peng and Shoeybi, Mohammad and Catanzaro, Bryan and Ping, Wei},
  year = 2025,
  month = jun,
  number = {arXiv:2505.16400},
  eprint = {2505.16400},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2505.16400},
  urldate = {2025-06-30},
  abstract = {Despite recent progress in large-scale reinforcement learning (RL) for reasoning, the training recipe for building high-performing reasoning models remains elusive. Key implementation details of frontier models, such as DeepSeek-R1, including data curation strategies and RL training recipe, are often omitted. Moreover, recent research indicates distillation remains more effective than RL for smaller models. In this work, we demonstrate that large-scale RL can significantly enhance the reasoning capabilities of strong, small- and mid-sized models, achieving results that surpass those of state-of-the-art distillation-based models. We systematically study the RL training process through extensive ablations and propose a simple yet effective approach: first training on math-only prompts, then on code-only prompts. Notably, we find that math-only RL not only significantly enhances the performance of strong distilled models on math benchmarks (e.g., +14.6\% / +17.2\% on AIME 2025 for the 7B / 14B models), but also code reasoning tasks (e.g., +6.8\% / +5.8\% on LiveCodeBench for the 7B / 14B models). In addition, extended code-only RL iterations further improve performance on code benchmarks with minimal or no degradation in math results. We develop a robust data curation pipeline to collect challenging prompts with high-quality, verifiable answers and test cases to enable verification-based RL across both domains. Finally, we identify key experimental insights, including curriculum learning with progressively increasing response lengths and the stabilizing effect of on-policy parameter updates. We find that RL not only elicits the foundational reasoning capabilities acquired during pretraining and supervised fine-tuning (e.g., distillation), but also pushes the limits of the model's reasoning ability, enabling it to solve problems that were previously unsolvable.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/adam/Zotero/storage/HINPPKWS/Chen et al. - 2025 - AceReason-Nemotron Advancing Math and Code Reason.pdf;/Users/adam/Zotero/storage/H4WUDDHG/2505.html}
}

@article{chenFastModelDebias2023,
  title = {Fast Model Debias with Machine Unlearning},
  author = {Chen, Ruizhe and Yang, Jianfei and Xiong, Huimin and Bai, Jianhong and Hu, Tianxiang and Hao, Jinxiang and Feng, Yang and Zhou, Joey Tianyi and Wu, Jian and Liu, Zuo-Qiang},
  year = 2023,
  month = oct,
  journal = {Adv. Neural Inf. Process. Syst.},
  volume = {abs/2310.12560},
  issn = {1049-5258},
  doi = {10.48550/arXiv.2310.12560},
  abstract = {Recent discoveries have revealed that deep neural networks might behave in a biased manner in many real-world scenarios. For instance, deep networks trained on a large-scale face recognition dataset CelebA tend to predict blonde hair for females and black hair for males. Such biases not only jeopardize the robustness of models but also perpetuate and amplify social biases, which is especially concerning for automated decision-making processes in healthcare, recruitment, etc., as they could exacerbate unfair economic and social inequalities among different groups. Existing debiasing methods suffer from high costs in bias labeling or model re-training, while also exhibiting a deficiency in terms of elucidating the origins of biases within the model. To this respect, we propose a fast model debiasing framework (FMD) which offers an efficient approach to identify, evaluate and remove biases inherent in trained models. The FMD identifies biased attributes through an explicit counterfactual concept and quantifies the influence of data samples with influence functions. Moreover, we design a machine unlearning-based strategy to efficiently and effectively remove the bias in a trained model with a small counterfactual dataset. Experiments on the Colored MNIST, CelebA, and Adult Income datasets along with experiments with large language models demonstrate that our method achieves superior or competing accuracies compared with state-of-the-art methods while attaining significantly fewer biases and requiring much less debiasing cost. Notably, our method requires only a small external dataset and updating a minimal amount of model parameters, without the requirement of access to training data that may be too large or unavailable in practice.}
}

@misc{chengAgentR1TrainingPowerful2025,
  title = {Agent-{{R1}}: {{Training Powerful LLM Agents}} with {{End-to-End Reinforcement Learning}}},
  shorttitle = {Agent-{{R1}}},
  author = {Cheng, Mingyue and Ouyang, Jie and Yu, Shuo and Yan, Ruiran and Luo, Yucong and Liu, Zirui and Wang, Daoyu and Liu, Qi and Chen, Enhong},
  year = 2025,
  month = nov,
  journal = {arXiv.org},
  urldate = {2026-02-21},
  abstract = {Large Language Models (LLMs) are increasingly being explored for building Agents capable of active environmental interaction (e.g., via tool use) to solve complex problems. Reinforcement Learning (RL) is considered a key technology with significant potential for training such Agents; however, the effective application of RL to LLM Agents is still in its nascent stages and faces considerable challenges. Currently, this emerging field lacks in-depth exploration into RL approaches specifically tailored for the LLM Agent context, alongside a scarcity of flexible and easily extensible training frameworks designed for this purpose. To help advance this area, this paper first revisits and clarifies Reinforcement Learning methodologies for LLM Agents by systematically extending the Markov Decision Process (MDP) framework to comprehensively define the key components of an LLM Agent. Secondly, we introduce Agent-R1, a modular, flexible, and user-friendly training framework for RL-based LLM Agents, designed for straightforward adaptation across diverse task scenarios and interactive environments. We conducted experiments on Multihop QA benchmark tasks, providing initial validation for the effectiveness of our proposed methods and framework.},
  howpublished = {https://arxiv.org/abs/2511.14460v1},
  langid = {english},
  file = {/Users/adam/Zotero/storage/VVSEQ3Y3/Cheng et al. - 2025 - Agent-R1 Training Powerful LLM Agents with End-to-End Reinforcement Learning.pdf}
}

@unpublished{chenMachineUnlearningLarge2024,
  title = {Machine {{Unlearning}} in {{Large Language Models}}},
  author = {Chen, Kongyang and Wang, Zixin and Mi, Bing and Liu, Waixi and Wang, Shaowei and Ren, Xiaojun and Shen, Jiaxing},
  year = 2024,
  month = feb,
  journal = {arXiv [cs.CR]},
  abstract = {Recently, large language models (LLMs) have emerged as a notable field, attracting significant attention for its ability to automatically generate intelligent contents for various application domains. However, LLMs still suffer from significant security and privacy issues. For example, LLMs might expose user privacy from hacking attacks or targeted prompts. To address this problem, this paper introduces a novel machine unlearning framework into LLMs. Our objectives are to make LLMs not produce harmful, hallucinatory, or privacy-compromising responses, while retaining their standard output capabilities. To accomplish this, we use an evaluative model to pinpoint dialogues needing unlearning. We also establish a distance loss to function as the model's negative loss, diverting it from previous undesirable outputs. Furthermore, we determine the expected output's cluster mean to formulate a positive loss, directing the model's outputs toward preferable outcomes without compromising its reasoning abilities and performance. Experimental results show that our approach effectively meets unlearning objectives without substantially compromising model performance.},
  isbn = {2404.16841}
}

@unpublished{chenMoFOMomentumFilteredOptimizer2024,
  title = {{{MoFO}}: {{Momentum-Filtered Optimizer}} for {{Mitigating Forgetting}} in {{LLM Fine-Tuning}}},
  author = {Chen, Yupeng and Wang, Senmiao and Lin, Zhihang and Qin, Zeyu and Zhang, Yushun and Ding, Tian and Sun, Ruoyu},
  year = 2024,
  month = jul,
  journal = {arXiv [cs.LG]},
  abstract = {Recently, large language models (LLMs) have demonstrated remarkable capabilities in a wide range of tasks. Typically, an LLM is pre-trained on large corpora and subsequently fine-tuned on task-specific datasets. However, during fine-tuning, LLMs may forget the knowledge acquired in the pre-training stage, leading to a decline in general capabilities. To address this issue, we propose a new fine-tuning algorithm termed Momentum-Filtered Optimizer (MoFO). The key idea of MoFO is to iteratively select and update the model parameters with the largest momentum magnitudes. Compared to full-parameter training, MoFO achieves similar fine-tuning performance while keeping parameters closer to the pre-trained model, thereby mitigating knowledge forgetting. Unlike most existing methods for forgetting mitigation, MoFO combines the following two advantages. First, MoFO does not require access to pre-training data. This makes MoFO particularly suitable for fine-tuning scenarios where pre-training data is unavailable, such as fine-tuning checkpoint-only open-source LLMs. Second, MoFO does not alter the original loss function. This could avoid impairing the model performance on the fine-tuning tasks. We validate MoFO through rigorous convergence analysis and extensive experiments, demonstrating its superiority over existing methods in mitigating forgetting and enhancing fine-tuning performance.},
  isbn = {2407.20999}
}

@misc{chenSkillitDataDrivenSkills2023,
  title = {Skill-It! {{A Data-Driven Skills Framework}} for {{Understanding}} and {{Training Language Models}}},
  author = {Chen, Mayee F. and Roberts, Nicholas and Bhatia, Kush and Wang, Jue and Zhang, Ce and Sala, Frederic and R{\'e}, Christopher},
  year = 2023,
  month = jul,
  number = {arXiv:2307.14430},
  eprint = {2307.14430},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2307.14430},
  urldate = {2025-08-14},
  abstract = {The quality of training data impacts the performance of pre-trained large language models (LMs). Given a fixed budget of tokens, we study how to best select data that leads to good downstream model performance across tasks. We develop a new framework based on a simple hypothesis: just as humans acquire interdependent skills in a deliberate order, language models also follow a natural order when learning a set of skills from their training data. If such an order exists, it can be utilized for improved understanding of LMs and for data-efficient training. Using this intuition, our framework formalizes the notion of a skill and of an ordered set of skills in terms of the associated data. First, using both synthetic and real data, we demonstrate that these ordered skill sets exist, and that their existence enables more advanced skills to be learned with less data when we train on their prerequisite skills. Second, using our proposed framework, we introduce an online data sampling algorithm, Skill-It, over mixtures of skills for both continual pre-training and fine-tuning regimes, where the objective is to efficiently learn multiple skills in the former and an individual skill in the latter. On the LEGO synthetic in the continual pre-training setting, Skill-It obtains 36.5 points higher accuracy than random sampling. On the Natural Instructions dataset in the fine-tuning setting, Skill-It reduces the validation loss on the target skill by 13.6\% versus training on data associated with the target skill itself. We apply our skills framework on the recent RedPajama dataset to continually pre-train a 3B-parameter LM, achieving higher accuracy on the LM Evaluation Harness with 1B tokens than the baseline approach of sampling uniformly over data sources with 3B tokens.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/adam/Zotero/storage/Y69LIDZB/Chen et al. - 2023 - Skill-it! A Data-Driven Skills Framework for Under.pdf}
}

@unpublished{chenUnlearnWhatYou2023,
  title = {Unlearn {{What You Want}} to {{Forget}}: {{Efficient Unlearning}} for {{LLMs}}},
  author = {Chen, Jiaao and Yang, Diyi},
  year = 2023,
  month = oct,
  journal = {arXiv [cs.CL]},
  abstract = {Large language models (LLMs) have achieved significant progress from pre-training on and memorizing a wide range of textual data, however, this process might suffer from privacy issues and violations of data protection regulations. As a result, the ability to easily remove data related to individual users from such models while not deteriorating their predictive quality after the removal becomes increasingly important. To address these issues, in this work, we propose an efficient unlearning framework that could efficiently update LLMs without having to retrain the whole model after data removals, by introducing lightweight unlearning layers learned with a selective teacher-student objective into the transformers. In addition, we introduce a fusion mechanism to effectively combine different unlearning layers that learns to forget different sets of data to handle a sequence of forgetting operations. Experiments on classification and generation tasks demonstrate the effectiveness of our proposed methods compared to the state-of-the-art baselines.},
  isbn = {2310.20150}
}

@unpublished{chenWhenLLMMeets2024,
  title = {When {{LLM Meets DRL}}: {{Advancing Jailbreaking Efficiency}} via {{DRL-guided Search}}},
  author = {Chen, Xuan and Nie, Yuzhou and Guo, Wenbo and Zhang, Xiangyu},
  year = 2024,
  month = jun,
  journal = {arXiv [cs.CR]},
  abstract = {Recent studies developed jailbreaking attacks, which construct jailbreaking prompts to ``fool'' LLMs into responding to harmful questions. Early-stage jailbreaking attacks require access to model internals or significant human efforts. More advanced attacks utilize genetic algorithms for automatic and black-box attacks. However, the random nature of genetic algorithms significantly limits the effectiveness of these attacks. In this paper, we propose RLbreaker, a black-box jailbreaking attack driven by deep reinforcement learning (DRL). We model jailbreaking as a search problem and design an RL agent to guide the search, which is more effective and has less randomness than stochastic search, such as genetic algorithms. Specifically, we design a customized DRL system for the jailbreaking problem, including a novel reward function and a customized proximal policy optimization (PPO) algorithm. Through extensive experiments, we demonstrate that RLbreaker is much more effective than existing jailbreaking attacks against six state-of-the-art (SOTA) LLMs. We also show that RLbreaker is robust against three SOTA defenses and its trained agents can transfer across different LLMs. We further validate the key design choices of RLbreaker via a comprehensive ablation study.},
  isbn = {2406.08705}
}

@unpublished{choiSNAPUnlearningSelective2024,
  title = {{{SNAP}}: {{Unlearning Selective Knowledge}} in {{Large Language Models}} with {{Negative Instructions}}},
  author = {Choi, Minseok and Rim, Daniel and Lee, Dohyun and Choo, Jaegul},
  year = 2024,
  month = jun,
  journal = {arXiv [cs.CL]},
  abstract = {Instruction-following large language models (LLMs), such as ChatGPT, have become increasingly popular with the general audience, many of whom are incorporating them into their daily routines. However, these LLMs inadvertently disclose personal or copyrighted information, which calls for a machine unlearning method to remove selective knowledge. Previous attempts sought to forget the link between the target information and its associated entities, but it rather led to generating undesirable responses about the target, compromising the end-user experience. In this work, we propose SNAP, an innovative framework designed to selectively unlearn information by 1) training an LLM with negative instructions to generate obliterated responses, 2) augmenting hard positives to retain the original LLM performance, and 3) applying the novel Wasserstein regularization to ensure adequate deviation from the initial weights of the LLM. We evaluate our framework on various NLP benchmarks and demonstrate that our approach retains the original LLM capabilities, while successfully unlearning the specified information.},
  isbn = {2406.12329}
}

@misc{CircuitDesignPhotoresistor,
  title = {Circuit Design {{Photoresistor}} ({{Analog Input}})},
  journal = {Tinkercad},
  urldate = {2022-10-31},
  abstract = {Tinkercad is a free, easy-to-use app for 3D design, electronics, and coding.},
  howpublished = {https://www.tinkercad.com/things/gspKYWZjICi},
  file = {/Users/adam/Zotero/storage/B5Y4JMG7/editel.html}
}

@misc{CircuitDesignPIR,
  title = {Circuit Design {{PIR Motion Sensor}} ({{Digital Input}})},
  journal = {Tinkercad},
  urldate = {2022-10-31},
  abstract = {Tinkercad is a free, easy-to-use app for 3D design, electronics, and coding.},
  howpublished = {https://www.tinkercad.com/things/i9RJXoI8mjS},
  file = {/Users/adam/Zotero/storage/P64U3HBP/editel.html}
}

@misc{CodeEthicsNational,
  title = {Code of {{Ethics}} \textbar{} {{National Society}} of {{Professional Engineers}}},
  urldate = {2022-10-24},
  howpublished = {https://www.nspe.org/resources/ethics/code-ethics},
  file = {/Users/adam/Zotero/storage/9IFVT738/code-ethics.html}
}

@misc{CodeGenData,
  title = {Code {{Gen Data}}},
  journal = {Google Docs},
  urldate = {2024-11-09},
  abstract = {Organize the doc to identify the useful datasets spanning through the three aspects (safe \textbraceleft capability testing\textbraceright, vulnerable, malware) List out a table summarizing the data to download Set up the pipeline to incorporate the list of datasets to evaluate  Model to evaluate: DeepSeek CodeLlama  Safe...},
  howpublished = {https://docs.google.com/document/u/2/d/1AuiBbJuQyNwYX8ibxySUVv8aa2w1i-pVh\_uZMMEYccI/edit?ouid=103030888654436358758\&usp=docs\_home\&ths=true\&usp=embed\_facebook},
  langid = {english},
  file = {/Users/adam/Zotero/storage/26URQLLY/edit.html}
}

@misc{CommonJobTitles,
  title = {Common Job Titles for {{MSE}} - {{Google Search}}},
  urldate = {2022-07-16},
  howpublished = {https://www.google.com/search?q=common+job+titles+for+MSE\&oq=common+job+titles+for+MSE\&aqs=chrome..69i57j33i160l4.17629j0j4\&sourceid=chrome\&ie=UTF-8}
}

@misc{CreativeNewBusinesses2023,
  title = {Creative {{New Businesses Aim}} to {{Revitalize Rural Akita}} >> {{Japan}} 2 {{Earth}}},
  year = 2023,
  month = mar,
  journal = {Japan 2 Earth >> Sparking a transition to the future},
  urldate = {2023-04-17},
  abstract = {Entrepreneurs came together at AKITA RISE to share plans for guesthouses, farming blue corn, and mother-run cooperatives aiming to revitalize rural communities.},
  langid = {american},
  file = {/Users/adam/Zotero/storage/QJ66JNY5/2510.html}
}

@misc{cuiProcessReinforcementImplicit2025,
  title = {Process {{Reinforcement}} through {{Implicit Rewards}}},
  author = {Cui, Ganqu and Yuan, Lifan and Wang, Zefan and Wang, Hanbin and Zhang, Yuchen and Chen, Jiacheng and Li, Wendi and He, Bingxiang and Fan, Yuchen and Yu, Tianyu and Xu, Qixin and Chen, Weize and Yuan, Jiarui and Chen, Huayu and Zhang, Kaiyan and Lv, Xingtai and Wang, Shuo and Yao, Yuan and Han, Xu and Peng, Hao and Cheng, Yu and Liu, Zhiyuan and Sun, Maosong and Zhou, Bowen and Ding, Ning},
  year = 2025,
  month = feb,
  journal = {arXiv.org},
  urldate = {2026-02-21},
  abstract = {Dense process rewards have proven a more effective alternative to the sparse outcome-level rewards in the inference-time scaling of large language models (LLMs), particularly in tasks requiring complex multi-step reasoning. While dense rewards also offer an appealing choice for the reinforcement learning (RL) of LLMs since their fine-grained rewards have the potential to address some inherent issues of outcome rewards, such as training efficiency and credit assignment, this potential remains largely unrealized. This can be primarily attributed to the challenges of training process reward models (PRMs) online, where collecting high-quality process labels is prohibitively expensive, making them particularly vulnerable to reward hacking. To address these challenges, we propose PRIME (Process Reinforcement through IMplicit rEwards), which enables online PRM updates using only policy rollouts and outcome labels through implict process rewards. PRIME combines well with various advantage functions and forgoes the dedicated reward model training phrase that existing approaches require, substantially reducing the development overhead. We demonstrate PRIME's effectiveness on competitional math and coding. Starting from Qwen2.5-Math-7B-Base, PRIME achieves a 15.1\% average improvement across several key reasoning benchmarks over the SFT model. Notably, our resulting model, Eurus-2-7B-PRIME, surpasses Qwen2.5-Math-7B-Instruct on seven reasoning benchmarks with 10\% of its training data.},
  howpublished = {https://arxiv.org/abs/2502.01456v2},
  langid = {english},
  file = {/Users/adam/Zotero/storage/I6LWFQE4/Cui et al. - 2025 - Process Reinforcement through Implicit Rewards.pdf}
}

@misc{cutesywootsybunniesBunnyFlops2014,
  title = {Bunny {{Flops}}},
  author = {{CutesyWootsyBunnies}},
  year = 2014,
  month = may,
  urldate = {2024-07-16},
  abstract = {When your bunny does a flop it's a sign that your bunny is utterly blissed out and content. It can appear surprising, even alarming to a human when a bunny throws itself into a flop but it is a wonderful thing. The bunny is happy beyond words. All it can do is flop! Video and Music 'Bunny Flops' by Rosaleen Donnan (with the help of some loops). I took the footage in 2010 and 2011 on a little Sony Webbie HD video cam I used back then. www.rosaleendonnan.com}
}

@misc{CyberKillChains,
  title = {Cyber {{Kill Chains Explained}}: {{Phases}}, {{Pros}}/{{Cons}} \& {{Security Tactics}}},
  shorttitle = {Cyber {{Kill Chains Explained}}},
  journal = {Splunk},
  urldate = {2024-03-12},
  abstract = {A cyber kill chain framework can help organizations to better understand and combat attacks. Learn about the evolution and applications of the cyber kill chain.},
  howpublished = {https://www.splunk.com/en\_us/blog/learn/cyber-kill-chains.html},
  langid = {english},
  file = {/Users/adam/Zotero/storage/LNFSEUMG/cyber-kill-chains.html}
}

@misc{CyberSecurityClub,
  title = {Cyber {{Security Club}} at {{Virginia Tech}} - {{GobblerConnect}}},
  urldate = {2022-07-16},
  howpublished = {https://gobblerconnect.vt.edu/organization/csecvt},
  file = {/Users/adam/Zotero/storage/27PCLVW2/csecvt.html}
}

@misc{CyberSecurityCluba,
  title = {Cyber {{Security Club}} at {{Virginia Tech}} - {{GobblerConnect}}},
  urldate = {2022-07-16},
  howpublished = {https://gobblerconnect.vt.edu/organization/csecvt}
}

@misc{CyberSecurityClubb,
  title = {Cyber {{Security Club}} at {{Virginia Tech}} - {{GobblerConnect}}},
  urldate = {2022-07-16},
  howpublished = {https://gobblerconnect.vt.edu/organization/csecvt}
}

@misc{CyberSecurityClubc,
  title = {Cyber {{Security Club}} at {{Virginia Tech}} - {{GobblerConnect}}},
  urldate = {2022-07-16},
  howpublished = {https://gobblerconnect.vt.edu/organization/csecvt}
}

@misc{dantasPhotoJaimeDantas2019,
  title = {Photo by {{Jaime Dantas}} on {{Unsplash}}},
  author = {Dantas, Jaime},
  year = 2019,
  month = may,
  urldate = {2024-07-06},
  abstract = {Download this photo by Du\v san veverkolog on Unsplash},
  howpublished = {https://unsplash.com/photos/brown-rodent-in-body-of-water-in-front-of-rock-yObnHvuwkiY},
  langid = {american},
  file = {/Users/adam/Zotero/storage/VM98VD2Q/brown-rodent-in-body-of-water-in-front-of-rock-yObnHvuwkiY.html}
}

@misc{dantasPhotoJaimeDantas2020,
  title = {Photo by {{Jaime Dantas}} on {{Unsplash}}},
  author = {Dantas, Jaime},
  year = 2020,
  month = nov,
  urldate = {2024-07-06},
  abstract = {Capybara in a protected area of the Reservoir of Guarapiranga -- Download this photo by Jaime Dantas on Unsplash},
  howpublished = {https://unsplash.com/photos/brown-rodent-on-green-grass-during-daytime-WROg3LroIIU},
  langid = {american},
  file = {/Users/adam/Zotero/storage/EZ6KK4GD/brown-rodent-on-green-grass-during-daytime-WROg3LroIIU.html}
}

@misc{dantasPhotoJaimeDantas2020a,
  title = {Photo by {{Jaime Dantas}} on {{Unsplash}}},
  author = {Dantas, Jaime},
  year = 2020,
  month = nov,
  urldate = {2024-07-06},
  abstract = {Capybara in a protected area of the Reservoir of Guarapiranga -- Download this photo by Jaime Dantas on Unsplash},
  howpublished = {https://unsplash.com/photos/brown-lion-and-lioness-on-green-grass-field-during-daytime-Fpvr7thkAf0},
  langid = {american},
  file = {/Users/adam/Zotero/storage/N3T28AUJ/brown-lion-and-lioness-on-green-grass-field-during-daytime-Fpvr7thkAf0.html}
}

@article{davidsonDetectionChemicalWarfare2020,
  title = {Detection of {{Chemical Warfare Agents}} by {{Colorimetric Sensor Arrays}}},
  author = {Davidson, {\relax CE} and Dixon, {\relax MM} and Williams, {\relax BR} and Kilper, {\relax GK} and Lim, {\relax SH} and Martino, {\relax RA} and Rhodes, P and Hulet, {\relax MS} and Miles, {\relax RW} and Samuels, {\relax AC} and Emanuel, {\relax PA} and Miklos, {\relax AE}},
  year = 2020,
  month = apr,
  journal = {ACS SENSORS},
  volume = {5},
  number = {4},
  pages = {1102--1109},
  issn = {2379-3694},
  doi = {10.1021/acssensors.0c00042},
  abstract = {We report the successful use of colorimetric arrays to identify chemical warfare agents (CWAs). Methods were developed to interpret and analyze a 73-indicator array with an entirely automated workflow. Using a cross-validated first-nearest-neighbor algorithm for assessing detection and identification performances on 632 exposures, at 30 min postexposure we report, on average, 78\% correct chemical identification, 86\% correct class-level identification, and 96\% correct red light/green light (agent versus non-agent) detection. Of 174 total independent agent test exposures, 164 were correctly identified from a 30 min exposure in the red light/green light context, yielding a 94\% correct identification of CWAs. Of 149 independent non-agent exposures, 139 were correctly identified at 30 min in the red light/ green light context, yielding a 7\% false alarm rate. We find that this is a promising approach for the development of a miniaturized, field-portable analytical equipment suitable for soldiers and first responders.},
  langid = {english},
  keywords = {chemical identification,chemical warfare agent,colorimetric sensor array,defense,field-portable,IDENTIFICATION}
}

@article{davidsonDetectionChemicalWarfare2020a,
  title = {Detection of {{Chemical Warfare Agents}} by {{Colorimetric Sensor Arrays}}},
  author = {Davidson, {\relax CE} and Dixon, {\relax MM} and Williams, {\relax BR} and Kilper, {\relax GK} and Lim, {\relax SH} and Martino, {\relax RA} and Rhodes, P and Hulet, {\relax MS} and Miles, {\relax RW} and Samuels, {\relax AC} and Emanuel, {\relax PA} and Miklos, {\relax AE}},
  year = 2020,
  month = apr,
  journal = {ACS SENSORS},
  volume = {5},
  number = {4},
  pages = {1102--1109},
  issn = {2379-3694},
  doi = {10.1021/acssensors.0c00042},
  abstract = {We report the successful use of colorimetric arrays to identify chemical warfare agents (CWAs). Methods were developed to interpret and analyze a 73-indicator array with an entirely automated workflow. Using a cross-validated first-nearest-neighbor algorithm for assessing detection and identification performances on 632 exposures, at 30 min postexposure we report, on average, 78\% correct chemical identification, 86\% correct class-level identification, and 96\% correct red light/green light (agent versus non-agent) detection. Of 174 total independent agent test exposures, 164 were correctly identified from a 30 min exposure in the red light/green light context, yielding a 94\% correct identification of CWAs. Of 149 independent non-agent exposures, 139 were correctly identified at 30 min in the red light/ green light context, yielding a 7\% false alarm rate. We find that this is a promising approach for the development of a miniaturized, field-portable analytical equipment suitable for soldiers and first responders.},
  langid = {english},
  keywords = {chemical identification,chemical warfare agent,colorimetric sensor array,defense,field-portable,IDENTIFICATION}
}

@misc{DeepSeekR1DeepSeek_R1pdfMain,
  title = {{{DeepSeek-R1}}/{{DeepSeek}}\_{{R1}}.Pdf at Main {$\cdot$} Deepseek-Ai/{{DeepSeek-R1}}},
  urldate = {2025-01-31},
  howpublished = {https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek\_R1.pdf},
  file = {/Users/adam/Zotero/storage/5HBFXIDS/DeepSeek_R1.html}
}

@misc{DefinitionPERSPECTIVE2023,
  title = {Definition of {{PERSPECTIVE}}},
  year = 2023,
  month = may,
  urldate = {2023-05-02},
  abstract = {a mental view or prospect; a visible scene; especially : one giving a distinctive impression of distance : vista; the interrelation in which a subject or its parts are mentally viewed; also : point of view\dots{} See the full definition},
  howpublished = {https://www.merriam-webster.com/dictionary/perspective},
  langid = {english},
  file = {/Users/adam/Zotero/storage/38WGCNEY/perspective.html}
}

@misc{DefinitionXENOPHOBIA2023,
  title = {Definition of {{XENOPHOBIA}}},
  year = 2023,
  month = apr,
  urldate = {2023-05-02},
  abstract = {fear and hatred of strangers or foreigners or of anything that is strange or foreign\dots{} See the full definition},
  howpublished = {https://www.merriam-webster.com/dictionary/xenophobia},
  langid = {english},
  file = {/Users/adam/Zotero/storage/SM9PNIQI/xenophobia.html}
}

@misc{departmentofjobsOwningRabbitAgriculture2024,
  type = {Text},
  title = {Owning a Rabbit - {{Agriculture}}},
  author = {{Department of Jobs}, Precincts {and} Regions},
  year = 2024,
  month = mar,
  journal = {Agriculture Victoria},
  publisher = {{Department of Jobs, Precincts and Regions}},
  urldate = {2024-07-16},
  abstract = {It is important that you understand all the requirements for caring for a rabbit before you buy one. Rabbits need  housing, exercise, socialisation and a specific diet for good welfare.},
  copyright = {https://agriculture.vic.gov.au/copyright},
  howpublished = {https://agriculture.vic.gov.au/livestock-and-animals/animal-welfare-victoria/other-pets/rabbits/owning-a-rabbit},
  langid = {australian},
  annotation = {Last Modified: 2024-03-05},
  file = {/Users/adam/Zotero/storage/AF3SWHHJ/owning-a-rabbit.html}
}

@article{deptJapanSelectedIssues2018,
  title = {Japan: {{Selected Issues}}},
  shorttitle = {Japan},
  author = {Dept, International Monetary Fund Asia {and} Pacific},
  year = 2018,
  month = nov,
  journal = {IMF Staff Country Reports},
  volume = {2018},
  number = {334},
  publisher = {International Monetary Fund},
  doi = {10.5089/9781484386804.002.A003},
  urldate = {2023-04-08},
  abstract = {Selected Issues},
  chapter = {IMF Staff Country Reports},
  isbn = {9781484386804},
  langid = {english},
  file = {/Users/adam/Zotero/storage/LQ84828X/Dept - 2018 - Japan Selected Issues.pdf}
}

@misc{deskQatarNationalDay2022,
  title = {Qatar {{National Day}} 2021 {{Celebrations}}},
  author = {Desk, Marhaba},
  year = 2022,
  month = dec,
  journal = {Marhaba Qatar},
  urldate = {2023-04-19},
  abstract = {Qatar National Day is celebrated every year on 18 December to commemorate the country's unification in 1878. Various traditional activities are observed all over the country.},
  langid = {british},
  file = {/Users/adam/Zotero/storage/FDH7XF3N/qatar-national-day-celebrations.html}
}

@misc{diannecarolineDidYouKnow2024,
  title = {Did You Know? {{Rabbits}} Are Eco-Friendly Pets!},
  shorttitle = {Did You Know?},
  author = {DianneCaroline},
  year = 2024,
  month = feb,
  journal = {House Rabbit Society},
  urldate = {2024-07-16},
  abstract = {According to Forbes, 66\% of households in the U.S. have a pet, as of 2024. That's almost 90 million households! There are so many reasons why it's great to have a pet---there's nothing like having a loving companion to spend time with. Having a pet is also associated with lower stress, decreased blood pressure, and},
  langid = {american},
  file = {/Users/adam/Zotero/storage/WE5AKEG7/did-you-know-rabbits-are-eco-friendly-pets.html}
}

@misc{DoingBusinessQatar,
  title = {Doing {{Business}} in {{Qatar}} \textbar{} {{Expat Arrivals}}},
  urldate = {2023-04-28},
  howpublished = {https://www.expatarrivals.com/middle-east/qatar/doing-business-qatar},
  file = {/Users/adam/Zotero/storage/27QPDU7G/doing-business-qatar.html}
}

@misc{DoingBusinessQatara,
  title = {Doing {{Business}} in {{Qatar}} \textbar{} {{Expat Arrivals}}},
  urldate = {2023-04-19},
  howpublished = {https://www.expatarrivals.com/middle-east/qatar/doing-business-qatar},
  file = {/Users/adam/Zotero/storage/RWAY62UB/doing-business-qatar.html}
}

@misc{dontoliverDonToliverParty2020,
  title = {Don {{Toliver}} - {{After Party}} [{{Official Music Video}}]},
  author = {{Don Toliver}},
  year = 2020,
  month = mar,
  urldate = {2024-07-08},
  abstract = {Don Toliver - After Party 'HARDSTONE PSYCHO' OUT NOW!  STREAM/DL: https://DonToliver.lnk.to/HARDSTONEPS...  Stream/Download the new album 'Heaven or Hell': https://DonToliver.lnk.to/HeavenOrHellID  Official Store: https://shop.dontolivermusic.com/ LIFE OF A DON - the new album out now: https://DonToliver.lnk.to/LifeofaDon Subscribe to Don's YouTube channel: https://dontoliver.lnk.to/YTSubscribe Follow Don Toliver ~~/~dontoliver~~ ~~/~dontolivermusic~~ ~~/~dontoliver~~ ~~/~dontoliver~~ ~~/~dontoliver~~ \#DonToliver \#AfterParty \#HeavenOrHell}
}

@misc{dossantos11BrazilBusiness2019,
  title = {11 {{Brazil Business Culture Differences}} [{{As Told}} by a {{Brazilian}}]},
  author = {Dossantos, Rebecca},
  year = 2019,
  month = jul,
  journal = {Colibri Content},
  urldate = {2023-04-26},
  abstract = {Here's a tell-all on business etiquette in Brazil as told by a Brazilian! From how to greet and what to wear, this is your ultimate guide!},
  langid = {american},
  file = {/Users/adam/Zotero/storage/YSENV6S3/brazil-business-culture-differences.html}
}

@misc{dossantos11BrazilBusiness2019a,
  title = {11 {{Brazil Business Culture Differences}} [{{As Told}} by a {{Brazilian}}]},
  author = {Dossantos, Rebecca},
  year = 2019,
  month = jul,
  journal = {Colibri Content},
  urldate = {2023-04-26},
  abstract = {Here's a tell-all on business etiquette in Brazil as told by a Brazilian! From how to greet and what to wear, this is your ultimate guide!},
  langid = {american},
  file = {/Users/adam/Zotero/storage/ZSZYU5ZM/brazil-business-culture-differences.html}
}

@misc{DownloadAbstractTemplate,
  title = {Download {{Abstract}} Template Background White and Bright Blue Squares Overlapping with Halftone and Texture. for Free},
  journal = {Vecteezy},
  urldate = {2024-12-05},
  abstract = {Download the Abstract template background white and bright blue squares overlapping with halftone and texture. 4243021 royalty-free Vector from Vecteezy for your project and explore over a million other vectors, icons and clipart graphics!},
  howpublished = {https://www.vecteezy.com/vector-art/4243021-abstract-template-background-white-and-bright-blue-squares-overlapping-with-halftone-and-texture},
  langid = {english},
  file = {/Users/adam/Zotero/storage/8ADNUJDH/4243021-abstract-template-background-white-and-bright-blue-squares-overlapping-with-halftone-an.html}
}

@misc{DownloadBrazilCountry,
  title = {Download Brazil Country Map with Flag for Free},
  journal = {Vecteezy},
  urldate = {2023-04-19},
  abstract = {Download the brazil country map with flag 2843036 royalty-free Vector from Vecteezy for your project and explore over a million other vectors, icons and clipart graphics!},
  howpublished = {https://www.vecteezy.com/vector-art/2843036-brazil-country-map-with-flag},
  langid = {english},
  file = {/Users/adam/Zotero/storage/P5CPDDEC/2843036-brazil-country-map-with-flag.html}
}

@incollection{driverHistoryUtilitarianism2022,
  title = {The {{History}} of {{Utilitarianism}}},
  booktitle = {The {{Stanford Encyclopedia}} of {{Philosophy}}},
  author = {Driver, Julia},
  editor = {Zalta, Edward N. and Nodelman, Uri},
  year = 2022,
  edition = {Winter 2022},
  publisher = {Metaphysics Research Lab, Stanford University},
  urldate = {2024-09-14},
  abstract = {Utilitarianism is one of the most powerful and persuasive approachesto normative ethics in the history of philosophy. Though notfully articulated until the 19th century, proto-utilitarianpositions can be discerned throughout the history of ethicaltheory.},
  keywords = {Bentham Jeremy,consequentialism,hedonism,Hume David,Mill John Stuart,Moore George Edward,Scottish Philosophy: in the 18th Century,Shaftesbury Lord [Anthony Ashley Cooper 3rd Earl of],Sidgwick Henry,well-being},
  file = {/Users/adam/Zotero/storage/XXTUF9AS/utilitarianism-history.html}
}

@misc{E1_Adanato,
  title = {E1\_{{Adanato}}},
  journal = {Google Docs},
  urldate = {2022-07-16},
  abstract = {E1. Planning to be a Hokie Engineer Part 1 Summer 2022 Self Exploration 	The most interesting type of work for me is creating and solving problems. It lets me go into deep focused work which is really relaxing.  I like feeling intelligent when I'm done with a hard problem.  	When I was 14 years o...},
  howpublished = {https://docs.google.com/document/d/1FTno--o0sfA1y8oUAVLaF6y2k91HewokNicZNwE9W\_4/edit?ouid=103030888654436358758\&usp=docs\_home\&ths=true\&usp=embed\_facebook},
  langid = {english},
  file = {/Users/adam/Zotero/storage/4F6FRFEA/edit.html}
}

@misc{ec-councilCyberKillChain2022,
  title = {The {{Cyber Kill Chain}}: {{The Seven Steps}} of a {{Cyberattack}}},
  shorttitle = {The {{Cyber Kill Chain}}},
  author = {{EC-Council}},
  year = 2022,
  month = jan,
  journal = {Cybersecurity Exchange},
  urldate = {2024-03-13},
  abstract = {Learn what the Cyber Kill Chain is and how understanding steps of Cyber Kill Chain can help you prevent cybersecurity breaches in your organization.},
  langid = {american},
  file = {/Users/adam/Zotero/storage/9BCCNMAF/cyber-kill-chain-seven-steps-cyberattack.html}
}

@misc{ECE3504Principles,
  title = {{{ECE}} 3504 - {{Principles}} of {{Computer Architecture}} ({{3C}})},
  urldate = {2022-07-16},
  howpublished = {https://ece.vt.edu/content/ece\_vt\_edu/en/undergrad/courses/3504.html},
  langid = {english},
  file = {/Users/adam/Zotero/storage/S42CVCF8/3504.html}
}

@misc{EducationSystemQatar,
  title = {The Education System in {{Qatar}}},
  journal = {Expatica Qatar},
  urldate = {2023-04-19},
  abstract = {We explain the education system in Qatar, including local and international schools, and special educational needs options for expats.},
  howpublished = {https://www.expatica.com/qa/education/children-education/education-system-in-qatar-73441/},
  langid = {american},
  file = {/Users/adam/Zotero/storage/F3GAFE8Z/education-system-in-qatar-73441.html}
}

@misc{eganationOkPullHop2020,
  title = {Ok {{I}} Pull up Hop out at the after Party You and All Your Friends (Capybara)},
  author = {{EGA NATION}},
  year = 2020,
  month = sep,
  urldate = {2024-07-08},
  abstract = {subscribe :D capybara capybara capybara capybara capybara capybara capybara capybara capybara capybara capybara capybara capybara capybara capybara capybara capybara capybara capybara capybara}
}

@misc{eignerDeterminantsLLMassistedDecisionMaking2024,
  title = {Determinants of {{LLM-assisted Decision-Making}}},
  author = {Eigner, Eva and H{\"a}ndler, Thorsten},
  year = 2024,
  month = feb,
  number = {arXiv:2402.17385},
  eprint = {2402.17385},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-07-21},
  abstract = {Decision-making is a fundamental capability in everyday life. Large Language Models (LLMs) provide multifaceted support in enhancing human decision-making processes. However, understanding the influencing factors of LLM-assisted decision-making is crucial for enabling individuals to utilize LLM-provided advantages and minimize associated risks in order to make more informed and better decisions. This study presents the results of a comprehensive literature analysis, providing a structural overview and detailed analysis of determinants impacting decision-making with LLM support. In particular, we explore the effects of technological aspects of LLMs, including transparency and prompt engineering, psychological factors such as emotions and decision-making styles, as well as decisionspecific determinants such as task difficulty and accountability. In addition, the impact of the determinants on the decision-making process is illustrated via multiple application scenarios. Drawing from our analysis, we develop a dependency framework that systematizes possible interactions in terms of reciprocal interdependencies between these determinants. Our research reveals that, due to the multifaceted interactions with various determinants, factors such as trust in or reliance on LLMs, the user's mental model, and the characteristics of information processing are identified as significant aspects influencing LLM-assisted decision-making processes. Our findings can be seen as crucial for improving decision quality in human-AI collaboration, empowering both users and organizations, and designing more effective LLM interfaces. Additionally, our work provides a foundation for future empirical investigations on the determinants of decision-making assisted by LLMs.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Human-Computer Interaction},
  file = {/Users/adam/Zotero/storage/3QHR4DQX/Eigner and Hndler - 2024 - Determinants of LLM-assisted Decision-Making.pdf}
}

@unpublished{eldanWhosHarryPotter2023,
  title = {Who's {{Harry Potter}}? {{Approximate Unlearning}} in {{LLMs}}},
  author = {Eldan, Ronen and Russinovich, Mark},
  year = 2023,
  month = oct,
  journal = {arXiv [cs.CL]},
  abstract = {Large language models (LLMs) are trained on massive internet corpora that often contain copyrighted content. This poses legal and ethical challenges for the developers and users of these models, as well as the original authors and publishers. In this paper, we propose a novel technique for unlearning a subset of the training data from a LLM, without having to retrain it from scratch. We evaluate our technique on the task of unlearning the Harry Potter books from the Llama2-7b model (a generative language model recently open-sourced by Meta). While the model took over 184K GPU-hours to pretrain, we show that in about 1 GPU hour of finetuning, we effectively erase the model's ability to generate or recall Harry Potter-related content, while its performance on common benchmarks (such as Winogrande, Hellaswag, arc, boolq and piqa) remains almost unaffected. We make our fine-tuned model publicly available on HuggingFace for community evaluation. To the best of our knowledge, this is the first paper to present an effective technique for unlearning in generative language models. Our technique consists of three main components: First, we use a reinforced model that is further trained on the target data to identify the tokens that are most related to the unlearning target, by comparing its logits with those of a baseline model. Second, we replace idiosyncratic expressions in the target data with generic counterparts, and leverage the model's own predictions to generate alternative labels for every token. These labels aim to approximate the next-token predictions of a model that has not been trained on the target data. Third, we finetune the model on these alternative labels, which effectively erases the original text from the model's memory whenever it is prompted with its context.},
  isbn = {2310.02238}
}

@misc{elliottMeetGiantRodent2017,
  title = {Meet the Giant Rodent Who Gets on with {{EVERYONE}} in the Animal Kingdom},
  author = {Elliott, Annabel Fenwick},
  year = 2017,
  month = jul,
  journal = {Mail Online},
  urldate = {2024-07-06},
  abstract = {The capybara, native to South America, is the largest rodent in the world and a highly social creature which has been observed bonding with everything from kittens and ducklings to monkeys.},
  chapter = {Good News},
  howpublished = {http://www.dailymail.co.uk/\textasciitilde/article-4663910/index.html},
  file = {/Users/adam/Zotero/storage/JRMI4GAR/The-capybara-friendliest-animal-world.html}
}

@misc{elliottMeetGiantRodent2017a,
  title = {Meet the Giant Rodent Who Gets on with {{EVERYONE}} in the Animal Kingdom},
  author = {Elliott, Annabel Fenwick},
  year = 2017,
  month = jul,
  journal = {Mail Online},
  urldate = {2024-07-08},
  abstract = {The capybara, native to South America, is the largest rodent in the world and a highly social creature which has been observed bonding with everything from kittens and ducklings to monkeys.},
  chapter = {Good News},
  howpublished = {http://www.dailymail.co.uk/\textasciitilde/article-4663910/index.html},
  file = {/Users/adam/Zotero/storage/YEUQ6QCD/The-capybara-friendliest-animal-world.html}
}

@misc{EnglishSpotRabbit,
  title = {The {{English Spot Rabbit}} - {{Top Facts}} \& {{Guide}}},
  urldate = {2024-07-17},
  abstract = {The English Spot Rabbit has an average life expectancy of between 5 to 9 years, although they can live longer when cared for properly},
  langid = {american},
  file = {/Users/adam/Zotero/storage/7TG7AK2X/english-spot-rabbit.html}
}

@misc{EntertainmentAudioEngineer,
  title = {Entertainment {{Audio Engineer Job}} in {{Blacksburg}}},
  journal = {Jobilize},
  urldate = {2022-07-30},
  abstract = {Overview: Join the Park's Technical Operations team and help create some of this year's brand-new shows, attractions, and events. Do you have previous experience and are looking...},
  howpublished = {https://www.jobilize.com/job/us-va-blacksburg-entertainment-audio-engineer-cedar-fair-hiring-now},
  file = {/Users/adam/Zotero/storage/8CKLYCA9/us-va-blacksburg-entertainment-audio-engineer-cedar-fair-hiring-now.html}
}

@misc{EventManagerJob,
  title = {Event {{Manager Job Description}}},
  journal = {Betterteam},
  urldate = {2022-07-29},
  abstract = {Learn about the key requirements, duties, responsibilities, and skills that should be in an event manager job description.},
  howpublished = {https://www.betterteam.com/event-manager-job-description},
  langid = {american},
  file = {/Users/adam/Zotero/storage/PBSUGHVC/event-manager-job-description.html}
}

@misc{everillSugarModernCapitalisms2023,
  title = {Sugar as {{Modern Capitalism}}'s {{Original Sin}}},
  author = {Everill, Bronwen},
  year = 2023,
  month = may,
  journal = {Foreign Policy},
  urldate = {2023-08-25},
  abstract = {A new book shows its history as anything but sweet.},
  langid = {american},
  file = {/Users/adam/Zotero/storage/G8NSPX8A/sugar-capitalism-slave-trade-labor-exploitation.html}
}

@misc{ExploreGobblerConnect,
  title = {Explore - {{GobblerConnect}}},
  urldate = {2022-07-30},
  howpublished = {https://gobblerconnect.vt.edu/},
  file = {/Users/adam/Zotero/storage/Y96S4TPQ/gobblerconnect.vt.edu.html}
}

@misc{fanSimplicityPrevailsRethinking2024,
  title = {Simplicity {{Prevails}}: {{Rethinking Negative Preference Optimization}} for {{LLM Unlearning}}},
  shorttitle = {Simplicity {{Prevails}}},
  author = {Fan, Chongyu and Liu, Jiancheng and Lin, Licong and Jia, Jinghan and Zhang, Ruiqi and Mei, Song and Liu, Sijia},
  year = 2024,
  month = oct,
  number = {arXiv:2410.07163},
  eprint = {2410.07163},
  publisher = {arXiv},
  urldate = {2024-10-15},
  abstract = {In this work, we address the problem of large language model (LLM) unlearning, aiming to remove unwanted data influences and associated model capabilities (e.g., copyrighted data or harmful content generation) while preserving essential model utilities, without the need for retraining from scratch. Despite the growing need for LLM unlearning, a principled optimization framework remains lacking. To this end, we revisit the state-of-the-art approach, negative preference optimization (NPO), and identify the issue of reference model bias, which could undermine NPO's effectiveness, particularly when unlearning forget data of varying difficulty. Given that, we propose a simple yet effective unlearning optimization framework, called SimNPO, showing that 'simplicity' in removing the reliance on a reference model (through the lens of simple preference optimization) benefits unlearning. We also provide deeper insights into SimNPO's advantages, supported by analysis using mixtures of Markov chains. Furthermore, we present extensive experiments validating SimNPO's superiority over existing unlearning baselines in benchmarks like TOFU and MUSE, and robustness against relearning attacks. Codes are available at https://github.com/OPTML-Group/Unlearn-Simple.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/adam/Zotero/storage/28INXJRZ/Fan et al. - 2024 - Simplicity Prevails Rethinking Negative Preferenc.pdf;/Users/adam/Zotero/storage/IIWC53TB/2410.html}
}

@misc{fanSimplicityPrevailsRethinking2024a,
  title = {Simplicity {{Prevails}}: {{Rethinking Negative Preference Optimization}} for {{LLM Unlearning}}},
  shorttitle = {Simplicity {{Prevails}}},
  author = {Fan, Chongyu and Liu, Jiancheng and Lin, Licong and Jia, Jinghan and Zhang, Ruiqi and Mei, Song and Liu, Sijia},
  year = 2024,
  month = oct,
  number = {arXiv:2410.07163},
  eprint = {2410.07163},
  publisher = {arXiv},
  urldate = {2024-10-21},
  abstract = {In this work, we address the problem of large language model (LLM) unlearning, aiming to remove unwanted data influences and associated model capabilities (e.g., copyrighted data or harmful content generation) while preserving essential model utilities, without the need for retraining from scratch. Despite the growing need for LLM unlearning, a principled optimization framework remains lacking. To this end, we revisit the state-of-the-art approach, negative preference optimization (NPO), and identify the issue of reference model bias, which could undermine NPO's effectiveness, particularly when unlearning forget data of varying difficulty. Given that, we propose a simple yet effective unlearning optimization framework, called SimNPO, showing that 'simplicity' in removing the reliance on a reference model (through the lens of simple preference optimization) benefits unlearning. We also provide deeper insights into SimNPO's advantages, supported by analysis using mixtures of Markov chains. Furthermore, we present extensive experiments validating SimNPO's superiority over existing unlearning baselines in benchmarks like TOFU and MUSE, and robustness against relearning attacks. Codes are available at https://github.com/OPTML-Group/Unlearn-Simple.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/adam/Zotero/storage/EXHJLCJM/Fan et al. - 2024 - Simplicity Prevails Rethinking Negative Preferenc.pdf;/Users/adam/Zotero/storage/8QMMT85N/2410.html}
}

@misc{FAQsPETRAInformation,
  title = {{{FAQs}} \textbar{} {{PETRA}}: {{Information}} on the {{Use}}, {{Benefits}} \& {{Safety}} of {{PET Plastic}}.},
  urldate = {2022-07-24},
  howpublished = {http://www.petresin.org/faq.asp},
  file = {/Users/adam/Zotero/storage/M76NGG5R/faq.html}
}

@article{felsteadAssessingGrowthRemote2017,
  title = {Assessing the Growth of Remote Working and Its Consequences for Effort, Well-Being and Work-Life Balance},
  author = {Felstead, Alan and Henseke, Golo},
  year = 2017,
  month = nov,
  journal = {New Technology, Work and Employment},
  volume = {32},
  number = {3},
  pages = {195--212},
  publisher = {John Wiley \& Sons, Ltd},
  issn = {0268-1072},
  doi = {10.1111/ntwe.12097},
  urldate = {2023-04-08},
  abstract = {This article critically assesses the assumption that more and more work is being detached from place and that this is a ?win-win? for both employers and employees. Based on an analysis of official labour market data, it finds that only one-third of the increase in remote working can be explained by compositional factors such as movement to the knowledge economy, the growth in flexible employment and organisational responses to the changing demographic make-up of the employed labour force. This suggests that the detachment of work from place is a growing trend. This article also shows that while remote working is associated with higher organisational commitment, job satisfaction and job-related well-being, these benefits come at the cost of work intensification and a greater inability to switch off.},
  keywords = {homeworking,job quality,job satisfaction,job-related well-being,remote working,teleworking,work effort,work-life balance}
}

@article{felsteadAssessingGrowthRemote2017a,
  title = {Assessing the Growth of Remote Working and Its Consequences for Effort, Well-Being and Work-Life Balance},
  author = {Felstead, Alan and Henseke, Golo},
  year = 2017,
  month = nov,
  journal = {New Technology, Work and Employment},
  volume = {32},
  number = {3},
  pages = {195--212},
  publisher = {John Wiley \& Sons, Ltd},
  issn = {0268-1072},
  doi = {10.1111/ntwe.12097},
  urldate = {2023-04-08},
  abstract = {This article critically assesses the assumption that more and more work is being detached from place and that this is a ?win-win? for both employers and employees. Based on an analysis of official labour market data, it finds that only one-third of the increase in remote working can be explained by compositional factors such as movement to the knowledge economy, the growth in flexible employment and organisational responses to the changing demographic make-up of the employed labour force. This suggests that the detachment of work from place is a growing trend. This article also shows that while remote working is associated with higher organisational commitment, job satisfaction and job-related well-being, these benefits come at the cost of work intensification and a greater inability to switch off.},
  keywords = {homeworking,job quality,job satisfaction,job-related well-being,remote working,teleworking,work effort,work-life balance}
}

@misc{FileFlagmapQatarsvg,
  title = {File:{{Flag-map}} of {{Qatar}}.Svg - {{Wikipedia}}},
  shorttitle = {File},
  urldate = {2023-04-19},
  howpublished = {https://commons.wikimedia.org/wiki/File:Flag-map\_of\_Qatar.svg},
  langid = {english},
  file = {/Users/adam/Zotero/storage/FJB6QWQG/FileFlag-map_of_Qatar.html}
}

@misc{FlemishGiantRabbit2024,
  title = {Flemish {{Giant Rabbit}}: {{Care}}, {{Temperament}}, \& {{Characteristics}}},
  shorttitle = {Flemish {{Giant Rabbit}}},
  year = 2024,
  month = may,
  urldate = {2024-07-17},
  abstract = {Discover everything you need to know about Flemish Giant rabbits, including their size, temperament, health considerations, and dietary needs.},
  chapter = {Rabbits},
  langid = {british},
  file = {/Users/adam/Zotero/storage/E3S2CE55/flemish-giant-rabbit.html}
}

@misc{ForeignAidExplained,
  title = {Foreign Aid, Explained},
  urldate = {2024-12-07},
  abstract = {Foreign aid accounts for just over 1\% of the US federal budget, but it carries an outsized impact. Here's what every American should know.},
  howpublished = {https://concernusa.org/news/foreign-aid-explained/},
  langid = {english},
  file = {/Users/adam/Zotero/storage/4J77W22M/foreign-aid-explained.html}
}

@misc{forpetloversHowCalmBarking2023,
  title = {How {{To Calm A Barking Dog At Night}}?? {{Super Easy Tricks}}},
  shorttitle = {How {{To Calm A Barking Dog At Night}}?},
  author = {{For Pet Lovers}},
  year = 2023,
  month = jan,
  urldate = {2024-07-18},
  abstract = {---------------------------------------- Watch our Best Reviews of Hot Selling Products Videos: Home Remedies for Dog's Bleeding Anus:~~~{$\bullet~$}Home~Remedies~for~Dog's~Bleeding~Anus...~~7 Best Dog Barking Device:~~~{$\bullet~$}7~Best~Dog~Barking~DeviceLong~Range~...~~ Best Ultrasonic Dog Bark Control Devices :~~~{$\bullet~$}Best~Ultrasonic~Dog~Bark~Control~Devi...~~ Best Dog Control Collars:~~~{$\bullet~$}Best~Dog~Control~Collars~-~Top~Dog~Ba...}
}

@misc{FosteringCollegeStudent,
  title = {Fostering {{College Student Mental Health}} and {{Resilience}}},
  urldate = {2024-10-28},
  abstract = {College student mental health has been the focus of much attention in recent years. Mental health is integral to student success and mental health concerns among college students are an ongoing and systemic problem; not just a consequence of the pandemic.},
  howpublished = {https://www.psychiatry.org:443/news-room/apa-blogs/fostering-college-student-mental-health-and-resili},
  langid = {english}
}

@misc{fotosPhotoAlexasFotos,
  title = {Photo by {{Alexas Fotos}} on {{Pexels}}},
  author = {Fotos, Alexas},
  journal = {Pexels},
  urldate = {2024-07-17},
  abstract = {Download this photo by Alexas Fotos for free on Pexels},
  howpublished = {https://www.pexels.com/photo/gray-rabbit-on-hay-7192754/},
  langid = {american},
  file = {/Users/adam/Zotero/storage/2356TTI8/gray-rabbit-on-hay-7192754.html}
}

@misc{FreeVectorAbstract,
  title = {Free {{Vector}} \textbar{} {{Abstract}} Blue Geometric Shapes Background},
  journal = {Freepik},
  urldate = {2024-12-11},
  abstract = {Download this Free Vector about Abstract blue geometric shapes background, and discover more than 15 Million Professional Graphic Resources on Freepik},
  howpublished = {https://www.freepik.com/free-vector/abstract-blue-geometric-shapes-background\_6166980.htm},
  langid = {english},
  file = {/Users/adam/Zotero/storage/SUE43YPD/abstract-blue-geometric-shapes-background_6166980.html}
}

@misc{fuConstrainedDecodingSecure2024,
  title = {Constrained {{Decoding}} for {{Secure Code Generation}}},
  author = {Fu, Yanjun and Baker, Ethan and Ding, Yu and Chen, Yizheng},
  year = 2024,
  month = jul,
  number = {arXiv:2405.00218},
  eprint = {2405.00218},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2405.00218},
  urldate = {2025-01-10},
  abstract = {Code Large Language Models (Code LLMs) have been increasingly used by developers to boost productivity, but they often generate vulnerable code. Thus, there is an urgent need to ensure that code generated by Code LLMs is correct and secure. Previous research has primarily focused on generating secure code, overlooking the fact that secure code also needs to be correct. This oversight can lead to a false sense of security. Currently, the community lacks a method to measure actual progress in this area, and we need solutions that address both security and correctness of code generation. This paper introduces a new benchmark, CodeGuard+, along with two new metrics, to measure Code LLMs' ability to generate both secure and correct code. Using our new evaluation methods, we show that the state-of-the-art defense technique, prefix tuning, may not be as strong as previously believed, since it generates secure code but sacrifices functional correctness. We also demonstrate that different decoding methods significantly affect the security of Code LLMs. Furthermore, we explore a new defense direction: constrained decoding for secure code generation. We propose new constrained decoding techniques to generate secure code. Our results reveal that constrained decoding is more effective than prefix tuning to improve the security of Code LLMs, without requiring a specialized training dataset. Moreover, our evaluations over eight state-of-the-art Code LLMs show that constrained decoding has strong performance to improve the security of Code LLMs, and our technique outperforms GPT-4.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Cryptography and Security,Computer Science - Machine Learning,Computer Science - Software Engineering},
  file = {/Users/adam/Zotero/storage/4NWAJPXT/Fu et al. - 2024 - Constrained Decoding for Secure Code Generation.pdf;/Users/adam/Zotero/storage/LXN2GDC5/2405.html}
}

@misc{geCapabilitySalienceVector2025,
  title = {Capability {{Salience Vector}}: {{Fine-grained Alignment}} of {{Loss}} and {{Capabilities}} for {{Downstream Task Scaling Law}}},
  shorttitle = {Capability {{Salience Vector}}},
  author = {Ge, Qiming and Xing, Shuhao and Gao, Songyang and Zhou, Yunhua and Zou, Yicheng and Zhang, Songyang and Chen, Zhi and Yan, Hang and Zhang, Qi and Guo, Qipeng and Chen, Kai},
  year = 2025,
  month = jun,
  number = {arXiv:2506.13216},
  eprint = {2506.13216},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2506.13216},
  urldate = {2025-07-08},
  abstract = {Scaling law builds the relationship between training computation and validation loss, enabling researchers to effectively predict the loss trending of models across different levels of computation. However, a gap still remains between validation loss and the model's downstream capabilities, making it untrivial to apply scaling law to direct performance prediction for downstream tasks. The loss typically represents a cumulative penalty for predicted tokens, which are implicitly considered to have equal importance. Nevertheless, our studies have shown evidence that when considering different training data distributions, we cannot directly model the relationship between downstream capability and computation or token loss. To bridge the gap between validation loss and downstream task capabilities, in this work, we introduce Capability Salience Vector, which decomposes the overall loss and assigns different importance weights to tokens to assess a specific meta-capability, aligning the validation loss with downstream task performance in terms of the model's capabilities. Experiments on various popular benchmarks demonstrate that our proposed Capability Salience Vector could significantly improve the predictability of language model performance on downstream tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/adam/Zotero/storage/C55GU4LA/Ge et al. - 2025 - Capability Salience Vector Fine-grained Alignment.pdf;/Users/adam/Zotero/storage/EBNTFY2A/2506.html}
}

@misc{gehringRLEFGroundingCode2024b,
  title = {{{RLEF}}: {{Grounding Code LLMs}} in {{Execution Feedback}} with {{Reinforcement Learning}}},
  shorttitle = {{{RLEF}}},
  author = {Gehring, Jonas and Zheng, Kunhao and Copet, Jade and Mella, Vegard and Cohen, Taco and Synnaeve, Gabriel},
  year = 2024,
  month = oct,
  journal = {arXiv.org},
  urldate = {2024-10-04},
  abstract = {Large language models (LLMs) deployed as agents solve user-specified tasks over multiple steps while keeping the required manual engagement to a minimum. Crucially, such LLMs need to ground their generations in any feedback obtained to reliably achieve desired outcomes. We propose an end-to-end reinforcement learning method for teaching models to leverage execution feedback in the realm of code synthesis, where state-of-the-art LLMs struggle to improve code iteratively compared to independent sampling. We benchmark on competitive programming tasks, where we achieve new start-of-the art results with both small (8B parameters) and large (70B) models while reducing the amount of samples required by an order of magnitude. Our analysis of inference-time behavior demonstrates that our method produces LLMs that effectively leverage automatic feedback over multiple steps.},
  howpublished = {https://arxiv.org/abs/2410.02089v1},
  langid = {english},
  file = {/Users/adam/Zotero/storage/NL9A5LII/Gehring et al. - 2024 - RLEF Grounding Code LLMs in Execution Feedback wi.pdf}
}

@article{gillespieImpactFreeSugar2023,
  title = {The {{Impact}} of {{Free Sugar}} on {{Human Health-A Narrative Review}}.},
  author = {Gillespie, Kerri M. and Kemps, Eva and White, Melanie J. and Bartlett, Selena E.},
  year = 2023,
  month = feb,
  journal = {Nutrients},
  volume = {15},
  number = {4},
  address = {Switzerland},
  issn = {2072-6643},
  doi = {10.3390/nu15040889},
  abstract = {The importance of nutrition in human health has been understood for over a century. However, debate is ongoing regarding the role of added and free sugars  in physiological and neurological health. In this narrative review, we have  addressed several key issues around this debate and the major health conditions  previously associated with sugar. We aim to determine the current evidence  regarding the role of free sugars in human health, specifically obesity,  diabetes, cardiovascular diseases, cognition, and mood. We also present some  predominant theories on mechanisms of action. The findings suggest a negative  effect of excessive added sugar consumption on human health and wellbeing.  Specific class and source of carbohydrate appears to greatly influence the impact  of these macronutrients on health. Further research into individual effects of  carbohydrate forms in diverse populations is needed to understand the complex  relationship between sugar and health.},
  langid = {english},
  pmcid = {PMC9966020},
  pmid = {36839247},
  keywords = {*Diabetes Mellitus,*Sugars,Beverages/analysis,cognition,coronary heart disease,diabetes,Dietary Sucrose,fructose,Fructose/pharmacology,high-fructose corn syrup,Humans,microbiome,neuroinflammation,obesity,Obesity,sugar,sugar-sweetened beverage}
}

@misc{GlobalSugarMarket,
  title = {Global {{Sugar Market Share}}, {{Price Trends}} and {{Forecast}} 2023-2028},
  urldate = {2023-08-25},
  abstract = {The global sugar market size reached~189.0 Million Tons in 2022, and is expected to reach~217.2 Million Tons by 2028, exhibiting a growth rate (CAGR) of 1.64\% during 2023-2028.},
  howpublished = {https://www.imarcgroup.com/sugar-manufacturing-plant},
  langid = {english},
  file = {/Users/adam/Zotero/storage/DIN9GC5N/sugar-manufacturing-plant.html}
}

@misc{GoogleJavaStyle,
  title = {Google {{Java Style Guide}}},
  urldate = {2024-09-29},
  howpublished = {https://google.github.io/styleguide/javaguide.html},
  file = {/Users/adam/Zotero/storage/RVVD5QNI/javaguide.html}
}

@misc{GoogleTrends,
  title = {Google {{Trends}}},
  urldate = {2024-07-06},
  howpublished = {https://trends.google.com/trends/},
  file = {/Users/adam/Zotero/storage/W2P6J3TB/trends.html}
}

@incollection{guoCHAPTER15Flow2005,
  title = {{{CHAPTER}} 15 - {{Flow Assurance}}},
  booktitle = {Offshore {{Pipelines}}},
  author = {Guo, Boyun and Song, Shanhong and Chacko, Jacob and Ghalambor, Ali},
  editor = {Guo, Boyun and Song, Shanhong and Chacko, Jacob and Ghalambor, Ali},
  year = 2005,
  month = jan,
  pages = {169--214},
  publisher = {Gulf Professional Publishing},
  address = {Burlington},
  doi = {10.1016/B978-075067847-6/50072-X},
  isbn = {978-0-7506-7847-6}
}

@misc{guoRedCodeRiskyCode2024,
  title = {{{RedCode}}: {{Risky Code Execution}} and {{Generation Benchmark}} for {{Code Agents}}},
  shorttitle = {{{RedCode}}},
  author = {Guo, Chengquan and Liu, Xun and Xie, Chulin and Zhou, Andy and Zeng, Yi and Lin, Zinan and Song, Dawn and Li, Bo},
  year = 2024,
  month = nov,
  number = {arXiv:2411.07781},
  eprint = {2411.07781},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2411.07781},
  urldate = {2024-11-15},
  abstract = {With the rapidly increasing capabilities and adoption of code agents for AI-assisted coding, safety concerns, such as generating or executing risky code, have become significant barriers to the real-world deployment of these agents. To provide comprehensive and practical evaluations on the safety of code agents, we propose RedCode, a benchmark for risky code execution and generation: (1) RedCode-Exec provides challenging prompts that could lead to risky code execution, aiming to evaluate code agents' ability to recognize and handle unsafe code. We provide a total of 4,050 risky test cases in Python and Bash tasks with diverse input formats including code snippets and natural text. They covers 25 types of critical vulnerabilities spanning 8 domains (e.g., websites, file systems). We provide Docker environments and design corresponding evaluation metrics to assess their execution results. (2) RedCode-Gen provides 160 prompts with function signatures and docstrings as input to assess whether code agents will follow instructions to generate harmful code or software. Our empirical findings, derived from evaluating three agent frameworks based on 19 LLMs, provide insights into code agents' vulnerabilities. For instance, evaluations on RedCode-Exec show that agents are more likely to reject executing risky operations on the operating system, but are less likely to reject executing technically buggy code, indicating high risks. Risky operations described in natural text lead to a lower rejection rate than those in code format. Additionally, evaluations on RedCode-Gen show that more capable base models and agents with stronger overall coding abilities, such as GPT4, tend to produce more sophisticated and effective harmful software. Our findings highlight the need for stringent safety evaluations for diverse code agents. Our dataset and code are available at https://github.com/AI-secure/RedCode.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Software Engineering},
  file = {/Users/adam/Zotero/storage/4L3CYDPN/Guo et al. - 2024 - RedCode Risky Code Execution and Generation Bench.pdf;/Users/adam/Zotero/storage/Z5DVEMK7/2411.html}
}

@unpublished{guSecondOrderInformationMatters2024,
  title = {Second-{{Order Information Matters}}: {{Revisiting Machine Unlearning}} for {{Large Language Models}}},
  author = {Gu, Kang and Rashid, Md Rafi Ur and Sultana, Najrin and Mehnaz, Shagufta},
  year = 2024,
  month = mar,
  journal = {arXiv [cs.LG]},
  abstract = {With the rapid development of Large Language Models (LLMs), we have witnessed intense competition among the major LLM products like ChatGPT, LLaMa, and Gemini. However, various issues (e.g. privacy leakage and copyright violation) of the training corpus still remain underexplored. For example, the Times sued OpenAI and Microsoft for infringing on its copyrights by using millions of its articles for training. From the perspective of LLM practitioners, handling such unintended privacy violations can be challenging. Previous work addressed the ``unlearning" problem of LLMs using gradient information, while they mostly introduced significant overheads like data preprocessing or lacked robustness. In this paper, contrasting with the methods based on first-order information, we revisit the unlearning problem via the perspective of second-order information (Hessian). Our unlearning algorithms, which are inspired by classic Newton update, are not only data-agnostic/model-agnostic but also proven to be robust in terms of utility preservation or privacy guarantee. Through a comprehensive evaluation with four NLP datasets as well as a case study on real-world datasets, our methods consistently show superiority over the first-order methods.},
  isbn = {2403.10557}
}

@unpublished{guTeaMsRLTeachingLLMs2024,
  title = {{{TeaMs-RL}}: {{Teaching LLMs}} to {{Teach Themselves Better Instructions}} via {{Reinforcement Learning}}},
  author = {Gu, Shangding and Knoll, Alois and Jin, Ming},
  year = 2024,
  month = mar,
  journal = {arXiv [cs.CL]},
  abstract = {The development of Large Language Models (LLMs) often confronts challenges stemming from the heavy reliance on human annotators in the reinforcement learning with human feedback (RLHF) framework, or the frequent and costly external queries tied to the self-instruct paradigm. In this work, we pivot to Reinforcement Learning (RL) -- but with a twist. Diverging from the typical RLHF, which refines LLMs following instruction data training, we use RL to directly generate the foundational instruction dataset that alone suffices for fine-tuning. Our method, TeaMs-RL, uses a suite of textual operations and rules, prioritizing the diversification of training datasets. It facilitates the generation of high-quality data without excessive reliance on external advanced models, paving the way for a single fine-tuning step and negating the need for subsequent RLHF stages. Our findings highlight key advantages of our approach: reduced need for human involvement and fewer model queries (only \$5.73\textbackslash\%\$ of WizardLM's total), along with enhanced capabilities of LLMs in crafting and comprehending complex instructions compared to strong baselines, and substantially improved model privacy protection.},
  isbn = {2403.08694}
}

@misc{HalfJapaneseAged2022,
  title = {Half of {{Japanese Aged}} 65 to 69 {{Still}} in {{Employment}}},
  year = 2022,
  month = oct,
  journal = {nippon.com},
  urldate = {2023-04-08},
  abstract = {An increasing number of seniors are remaining in Japan's workforce to compensate for the dwindling numbers of those in the working-age population.},
  howpublished = {https://www.nippon.com/en/japan-data/h01453/},
  langid = {english},
  file = {/Users/adam/Zotero/storage/SH5XEAY3/h01453.html}
}

@misc{hamiltonCoffeeShopWhere2024,
  title = {The Coffee Shop Where Customers Can Pet the Largest Living Rodent},
  author = {Hamilton, Jessica},
  year = 2024,
  month = mar,
  journal = {Mail Online},
  urldate = {2024-07-08},
  abstract = {Cafe Capyba in Tokyo lets customers enjoy a coffee alongside two large capybaras named~Kohaku and Pisuke. The giant rodents roam freely, giving visitors a chance to view them up close.},
  chapter = {Travel},
  howpublished = {https://www.dailymail.co.uk/travel/article-13117175/Tokyo-capybara-cafe-cat-cafe.html},
  file = {/Users/adam/Zotero/storage/SF46IUGB/Tokyo-capybara-cafe-cat-cafe.html}
}

@misc{handshakejulieTop10Jobs2020,
  title = {Top 10 Jobs for {{Computer Science}} Majors},
  author = {{handshakejulie}},
  year = 2020,
  month = apr,
  journal = {Handshake},
  urldate = {2022-07-16},
  abstract = {Thinking about pursuing a career in the growing tech industry? Here are 10 great job options you'll have as a computer science major.},
  howpublished = {https://joinhandshake.com/blog/students/top-10-jobs-for-computer-science-majors/},
  langid = {american},
  file = {/Users/adam/Zotero/storage/LCKJFKHQ/top-10-jobs-for-computer-science-majors.html}
}

@misc{HarvardLibraryFind,
  title = {Harvard {{Library}} - {{Find}} a {{Space}}},
  journal = {Harvard Library},
  urldate = {2022-09-12},
  howpublished = {https://library.harvard.edu/spaces},
  langid = {english},
  file = {/Users/adam/Zotero/storage/MX9ZSJ45/spaces.html}
}

@misc{HarvardLibraryFinda,
  title = {Harvard {{Library}} - {{Find}} a {{Space}}},
  journal = {Harvard Library},
  urldate = {2022-09-12},
  howpublished = {https://library.harvard.edu/spaces},
  langid = {english}
}

@article{hatlebakkDoesAidWork2021,
  title = {Does Aid Work?},
  author = {Hatlebakk, Magnus},
  year = 2021,
  journal = {CMI Report},
  volume = {2021:11},
  urldate = {2024-12-07},
  abstract = {This report attempts to understand why research findings differ on the impacts of foreign aid on in particular economic growth, but as...},
  langid = {english},
  file = {/Users/adam/Zotero/storage/P9BNN7A9/Hatlebakk - 2021 - Does aid work.pdf}
}

@unpublished{hayesInexactUnlearningNeeds2024,
  title = {Inexact {{Unlearning Needs More Careful Evaluations}} to {{Avoid}} a {{False Sense}} of {{Privacy}}},
  author = {Hayes, Jamie and Shumailov, Ilia and Triantafillou, Eleni and Khalifa, Amr and Papernot, Nicolas},
  year = 2024,
  month = mar,
  journal = {arXiv [cs.LG]},
  abstract = {The high cost of model training makes it increasingly desirable to develop techniques for unlearning. These techniques seek to remove the influence of a training example without having to retrain the model from scratch. Intuitively, once a model has unlearned, an adversary that interacts with the model should no longer be able to tell whether the unlearned example was included in the model's training set or not. In the privacy literature, this is known as membership inference. In this work, we discuss adaptations of Membership Inference Attacks (MIAs) to the setting of unlearning (leading to their "U-MIA" counterparts). We propose a categorization of existing U-MIAs into "population U-MIAs", where the same attacker is instantiated for all examples, and "per-example U-MIAs", where a dedicated attacker is instantiated for each example. We show that the latter category, wherein the attacker tailors its membership prediction to each example under attack, is significantly stronger. Indeed, our results show that the commonly used U-MIAs in the unlearning literature overestimate the privacy protection afforded by existing unlearning techniques on both vision and language models. Our investigation reveals a large variance in the vulnerability of different examples to per-example U-MIAs. In fact, several unlearning algorithms lead to a reduced vulnerability for some, but not all, examples that we wish to unlearn, at the expense of increasing it for other examples. Notably, we find that the privacy protection for the remaining training examples may worsen as a consequence of unlearning. We also discuss the fundamental difficulty of equally protecting all examples using existing unlearning schemes, due to the different rates at which examples are unlearned. We demonstrate that naive attempts at tailoring unlearning stopping criteria to different examples fail to alleviate these issues.},
  isbn = {2403.01218}
}

@misc{HedonismInternetEncyclopedia,
  title = {Hedonism \textbar{} {{Internet Encyclopedia}} of {{Philosophy}}},
  urldate = {2024-10-06},
  langid = {american},
  file = {/Users/adam/Zotero/storage/6KYT2V45/hedonism.html}
}

@misc{HeresWhyTaylor2014,
  title = {Here's {{Why Taylor Swift Pulled Her Music From Spotify}}},
  year = 2014,
  month = nov,
  journal = {Time},
  urldate = {2023-04-20},
  abstract = {The 1989 star is telling artists to say no to low-royalty streaming services},
  howpublished = {https://time.com/3554468/why-taylor-swift-spotify/},
  langid = {english},
  file = {/Users/adam/Zotero/storage/TS9G2CBS/why-taylor-swift-spotify.html}
}

@misc{Homepage,
  title = {Homepage},
  urldate = {2022-07-15},
  howpublished = {https://ieee.vt.edu/content/ieee\_vt\_edu/en/index.html},
  langid = {english},
  file = {/Users/adam/Zotero/storage/T59H4MY5/www.ieee.vt.edu.html}
}

@misc{HowCalmBarking,
  title = {How {{To Calm A Barking Dog At Night}}?? {{Super Easy Tricks}} - {{YouTube}}},
  urldate = {2024-07-17},
  howpublished = {https://www.youtube.com/watch?v=MlS-RZ8viww}
}

@misc{HowCyberKill,
  title = {How the Cyber Kill Chain Works - {{Google Search}}},
  urldate = {2024-03-12},
  howpublished = {https://www.google.com/search?q=How+the+cyber+kill+chain+works\&rlz=1C5CHFA\_enUS1085US1085\&oq=How+the+cyber+kill+chain+works\&gs\_lcrp=EgZjaHJvbWUyBggAEEUYOTIICAEQABgWGB4yDQgCEAAYhgMYgAQYigUyDQgDEAAYhgMYgAQYigUyDQgEEAAYhgMYgAQYigXSAQg0MjMyajBqN6gCALACAA\&sourceid=chrome\&ie=UTF-8},
  file = {/Users/adam/Zotero/storage/XJQCZAE3/search.html}
}

@misc{HowLongCats,
  title = {How {{Long Do Cats Live}}? {{Here}}'s {{What}} to {{Expect}}},
  shorttitle = {How {{Long Do Cats Live}}?},
  urldate = {2024-07-17},
  abstract = {How long do cats live? The average cat life expectancy largely depends on their care, nutrition, and environment. Learn more about the life expectancy of cats here.},
  howpublished = {https://www.petmd.com/cat/care/how-long-do-cats-live},
  langid = {english},
  file = {/Users/adam/Zotero/storage/3629ZVVD/how-long-do-cats-live.html}
}

@misc{HowLongDogs,
  title = {How {{Long Do Dogs Live}}? \textbar{} {{PetMD}}},
  urldate = {2024-07-17},
  howpublished = {https://www.petmd.com/dog/care/how-long-do-dogs-live},
  file = {/Users/adam/Zotero/storage/XRA7F885/how-long-do-dogs-live.html}
}

@misc{HowLongGuinea,
  title = {How {{Long Do Guinea Pigs Live}}?},
  urldate = {2024-07-17},
  abstract = {Dr. Lauren Jones discusses the average lifespan of guinea pigs, including tips to help improve your guinea pig's lifespan.},
  howpublished = {https://www.petmd.com/exotic/how-long-do-guinea-pigs-live},
  langid = {english},
  file = {/Users/adam/Zotero/storage/I86ET6T3/how-long-do-guinea-pigs-live.html}
}

@misc{HowLongHamsters,
  title = {How {{Long Do Hamsters Live}}?},
  urldate = {2024-07-17},
  abstract = {Dr. Melissa Witherell discusses hamster life expectancy including tips to help improve your hamster's lifespan.},
  howpublished = {https://www.petmd.com/exotic/general-health/how-long-do-hamsters-live},
  langid = {english},
  file = {/Users/adam/Zotero/storage/3AVDA4VS/how-long-do-hamsters-live.html}
}

@misc{HowLongRabbits,
  title = {How {{Long Do Rabbits Live}}?},
  urldate = {2024-07-18},
  abstract = {Dr. Melissa Witherell discusses the lifespan of rabbits, including tips on how to help your pet rabbit live longer.},
  howpublished = {https://www.petmd.com/rabbit/care/evr\_rb\_how-long-do-rabbits-live},
  langid = {english},
  file = {/Users/adam/Zotero/storage/LSHBLQCG/evr_rb_how-long-do-rabbits-live.html}
}

@misc{HowMakeAI2024,
  title = {How to {{Make AI}} '{{Forget}}' {{All}} the {{Private Data It Shouldn}}'t {{Have}} \textbar{} {{Working Knowledge}}},
  year = 2024,
  month = feb,
  journal = {Harvard Business School},
  urldate = {2024-11-26},
  howpublished = {https://www.library.hbs.edu/working-knowledge/qa-seth-neel-on-machine-unlearning-and-the-right-to-be-forgotten},
  langid = {american},
  file = {/Users/adam/Zotero/storage/KVVQERAW/qa-seth-neel-on-machine-unlearning-and-the-right-to-be-forgotten.html}
}

@misc{HowManyWords,
  title = {How Many Words Is Half a Page - {{Google Search}}},
  urldate = {2022-10-31},
  howpublished = {https://www.google.com/search?q=how+many+words+is+half+a+page\&rlz=1C1CHBF\_enUS938US938\&oq=how+many+words+is+half+a+page\&aqs=chrome..69i57j0i512l9.7650j0j7\&sourceid=chrome\&ie=UTF-8},
  file = {/Users/adam/Zotero/storage/CWYDDL3U/search.html}
}

@misc{HowManyWordsa,
  title = {How Many Words Is Half a Page - {{Google Search}}},
  urldate = {2022-10-31},
  howpublished = {https://www.google.com/search?q=how+many+words+is+half+a+page\&rlz=1C1CHBF\_enUS938US938\&oq=how+many+words+is+half+a+page\&aqs=chrome..69i57j0i512l9.7650j0j7\&sourceid=chrome\&ie=UTF-8},
  file = {/Users/adam/Zotero/storage/ZXG6VHX3/search.html}
}

@misc{HowMuchDoes2023,
  title = {How {{Much Does Spotify Pay Per Stream}} in 2023},
  year = 2023,
  month = jan,
  urldate = {2023-04-20},
  abstract = {Want to know how much Spotify pays artists per stream in 2023? We've got the recent low-down plus a Spotify royalty calculator!},
  howpublished = {https://dittomusic.com/en/blog/how-much-does-spotify-pay-per-stream/},
  langid = {english},
  file = {/Users/adam/Zotero/storage/TLF4DADV/how-much-does-spotify-pay-per-stream.html}
}

@misc{HowMuchForeign,
  title = {How {{Much Foreign Aid Reaches Foreign Governments}}?},
  journal = {Center For Global Development},
  urldate = {2024-12-07},
  abstract = {In the international development community, ``country ownership'' is considered a good thing, while criticisims of foreign aid are based on the idea that this is the problem. But only about a third of assistance is actually managed by those it is intended to help.},
  howpublished = {https://www.cgdev.org/blog/how-much-foreign-aid-reaches-foreign-governments},
  langid = {english},
  file = {/Users/adam/Zotero/storage/N8S3SL7G/how-much-foreign-aid-reaches-foreign-governments.html}
}

@misc{HowNNDSSConducts2022,
  title = {How {{NNDSS Conducts Case Surveillance}} \textbar{} {{CDC}}},
  year = 2022,
  month = jul,
  urldate = {2022-08-01},
  abstract = {Understand the process NNDSS uses to collect national notifiable disease data on behalf of CDC in order to support public health agencies.},
  howpublished = {https://www.cdc.gov/nndss/about/conduct.html},
  langid = {american},
  file = {/Users/adam/Zotero/storage/H6FZVE4S/conduct.html}
}

@unpublished{huangOffsetUnlearningLarge2024,
  title = {Offset {{Unlearning}} for {{Large Language Models}}},
  author = {Huang, James Y and Zhou, Wenxuan and Wang, Fei and Morstatter, Fred and Zhang, Sheng and Poon, Hoifung and Chen, Muhao},
  year = 2024,
  month = apr,
  journal = {arXiv [cs.CL]},
  abstract = {Despite the strong capabilities of Large Language Models (LLMs) to acquire knowledge from their training corpora, the memorization of sensitive information in the corpora such as copyrighted, harmful, and private content has led to ethical and legal concerns. In response to these challenges, unlearning has emerged as a potential remedy for LLMs affected by problematic training data. However, previous unlearning techniques are either not applicable to black-box LLMs due to required access to model internal weights, or violate data protection principles by retaining sensitive data for inference-time correction. We propose \$\textbackslash delta\$-unlearning, an offset unlearning framework for black-box LLMs. Instead of tuning the black-box LLM itself, \$\textbackslash delta\$-unlearning learns the logit offset needed for unlearning by contrasting the logits from a pair of smaller models. Experiments demonstrate that \$\textbackslash delta\$-unlearning can effectively unlearn target data while maintaining similar or even stronger performance on general out-of-forget-scope tasks. \$\textbackslash delta\$-unlearning also effectively incorporates different unlearning algorithms, making our approach a versatile solution to adapting various existing unlearning algorithms to black-box LLMs.},
  isbn = {2404.11045}
}

@unpublished{huangOffsetUnlearningLarge2024a,
  title = {Offset {{Unlearning}} for {{Large Language Models}}},
  author = {Huang, James Y and Zhou, Wenxuan and Wang, Fei and Morstatter, Fred and Zhang, Sheng and Poon, Hoifung and Chen, Muhao},
  year = 2024,
  month = apr,
  journal = {arXiv [cs.CL]},
  abstract = {Despite the strong capabilities of Large Language Models (LLMs) to acquire knowledge from their training corpora, the memorization of sensitive information in the corpora such as copyrighted, harmful, and private content has led to ethical and legal concerns. In response to these challenges, unlearning has emerged as a potential remedy for LLMs affected by problematic training data. However, previous unlearning techniques are either not applicable to black-box LLMs due to required access to model internal weights, or violate data protection principles by retaining sensitive data for inference-time correction. We propose \$\textbackslash delta\$-unlearning, an offset unlearning framework for black-box LLMs. Instead of tuning the black-box LLM itself, \$\textbackslash delta\$-unlearning learns the logit offset needed for unlearning by contrasting the logits from a pair of smaller models. Experiments demonstrate that \$\textbackslash delta\$-unlearning can effectively unlearn target data while maintaining similar or even stronger performance on general out-of-forget-scope tasks. \$\textbackslash delta\$-unlearning also effectively incorporates different unlearning algorithms, making our approach a versatile solution to adapting various existing unlearning algorithms to black-box LLMs.},
  isbn = {2404.11045}
}

@misc{huangTrustLLMTrustworthinessLarge2024,
  title = {{{TrustLLM}}: {{Trustworthiness}} in {{Large Language Models}}},
  shorttitle = {{{TrustLLM}}},
  author = {Huang, Yue and Sun, Lichao and Wang, Haoran and Wu, Siyuan and Zhang, Qihui and Li, Yuan and Gao, Chujie and Huang, Yixin and Lyu, Wenhan and Zhang, Yixuan and Li, Xiner and Liu, Zhengliang and Liu, Yixin and Wang, Yijue and Zhang, Zhikun and Vidgen, Bertie and Kailkhura, Bhavya and Xiong, Caiming and Xiao, Chaowei and Li, Chunyuan and Xing, Eric and Huang, Furong and Liu, Hao and Ji, Heng and Wang, Hongyi and Zhang, Huan and Yao, Huaxiu and Kellis, Manolis and Zitnik, Marinka and Jiang, Meng and Bansal, Mohit and Zou, James and Pei, Jian and Liu, Jian and Gao, Jianfeng and Han, Jiawei and Zhao, Jieyu and Tang, Jiliang and Wang, Jindong and Vanschoren, Joaquin and Mitchell, John and Shu, Kai and Xu, Kaidi and Chang, Kai-Wei and He, Lifang and Huang, Lifu and Backes, Michael and Gong, Neil Zhenqiang and Yu, Philip S. and Chen, Pin-Yu and Gu, Quanquan and Xu, Ran and Ying, Rex and Ji, Shuiwang and Jana, Suman and Chen, Tianlong and Liu, Tianming and Zhou, Tianyi and Wang, William and Li, Xiang and Zhang, Xiangliang and Wang, Xiao and Xie, Xing and Chen, Xun and Wang, Xuyu and Liu, Yan and Ye, Yanfang and Cao, Yinzhi and Chen, Yong and Zhao, Yue},
  year = 2024,
  month = sep,
  number = {arXiv:2401.05561},
  eprint = {2401.05561},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2401.05561},
  urldate = {2024-11-28},
  abstract = {Large language models (LLMs), exemplified by ChatGPT, have gained considerable attention for their excellent natural language processing capabilities. Nonetheless, these LLMs present many challenges, particularly in the realm of trustworthiness. Therefore, ensuring the trustworthiness of LLMs emerges as an important topic. This paper introduces TrustLLM, a comprehensive study of trustworthiness in LLMs, including principles for different dimensions of trustworthiness, established benchmark, evaluation, and analysis of trustworthiness for mainstream LLMs, and discussion of open challenges and future directions. Specifically, we first propose a set of principles for trustworthy LLMs that span eight different dimensions. Based on these principles, we further establish a benchmark across six dimensions including truthfulness, safety, fairness, robustness, privacy, and machine ethics. We then present a study evaluating 16 mainstream LLMs in TrustLLM, consisting of over 30 datasets. Our findings firstly show that in general trustworthiness and utility (i.e., functional effectiveness) are positively related. Secondly, our observations reveal that proprietary LLMs generally outperform most open-source counterparts in terms of trustworthiness, raising concerns about the potential risks of widely accessible open-source LLMs. However, a few open-source LLMs come very close to proprietary ones. Thirdly, it is important to note that some LLMs may be overly calibrated towards exhibiting trustworthiness, to the extent that they compromise their utility by mistakenly treating benign prompts as harmful and consequently not responding. Finally, we emphasize the importance of ensuring transparency not only in the models themselves but also in the technologies that underpin trustworthiness. Knowing the specific trustworthy technologies that have been employed is crucial for analyzing their effectiveness.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/adam/Zotero/storage/8CU2QQDW/Huang et al. - 2024 - TrustLLM Trustworthiness in Large Language Models.pdf;/Users/adam/Zotero/storage/K4GAGXEY/2401.html}
}

@misc{huangUnlearnBurnAdversarial2024,
  title = {Unlearn and {{Burn}}: {{Adversarial Machine Unlearning Requests Destroy Model Accuracy}}},
  shorttitle = {Unlearn and {{Burn}}},
  author = {Huang, Yangsibo and Liu, Daogao and Chua, Lynn and Ghazi, Badih and Kamath, Pritish and Kumar, Ravi and Manurangsi, Pasin and Nasr, Milad and Sinha, Amer and Zhang, Chiyuan},
  year = 2024,
  month = oct,
  number = {arXiv:2410.09591},
  eprint = {2410.09591},
  publisher = {arXiv},
  urldate = {2024-10-21},
  abstract = {Machine unlearning algorithms, designed for selective removal of training data from models, have emerged as a promising approach to growing privacy concerns. In this work, we expose a critical yet underexplored vulnerability in the deployment of unlearning systems: the assumption that the data requested for removal is always part of the original training set. We present a threat model where an attacker can degrade model accuracy by submitting adversarial unlearning requests for data not present in the training set. We propose white-box and black-box attack algorithms and evaluate them through a case study on image classification tasks using the CIFAR-10 and ImageNet datasets, targeting a family of widely used unlearning methods. Our results show extremely poor test accuracy following the attack: 3.6\% on CIFAR-10 and 0.4\% on ImageNet for white-box attacks, and 8.5\% on CIFAR-10 and 1.3\% on ImageNet for black-box attacks. Additionally, we evaluate various verification mechanisms to detect the legitimacy of unlearning requests and reveal the challenges in verification, as most of the mechanisms fail to detect stealthy attacks without severely impairing their ability to process valid requests. These findings underscore the urgent need for research on more robust request verification methods and unlearning protocols, should the deployment of machine unlearning systems become more prevalent in the future.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Cryptography and Security},
  file = {/Users/adam/Zotero/storage/EEGU48FT/Huang et al. - 2024 - Unlearn and Burn Adversarial Machine Unlearning R.pdf;/Users/adam/Zotero/storage/RADNKMSZ/2410.html}
}

@misc{huDeepGenerativeModels2018,
  title = {Deep {{Generative Models}} with {{Learnable Knowledge Constraints}}},
  author = {Hu, Zhiting and Yang, Zichao and Salakhutdinov, Ruslan and Liang, Xiaodan and Qin, Lianhui and Dong, Haoye and Xing, Eric},
  year = 2018,
  month = nov,
  number = {arXiv:1806.09764},
  eprint = {1806.09764},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2024-03-30},
  abstract = {The broad set of deep generative models (DGMs) has achieved remarkable advances. However, it is often difficult to incorporate rich structured domain knowledge with the end-to-end DGMs. Posterior regularization (PR) offers a principled framework to impose structured constraints on probabilistic models, but has limited applicability to the diverse DGMs that can lack a Bayesian formulation or even explicit density evaluation. PR also requires constraints to be fully specified a priori, which is impractical or suboptimal for complex knowledge with learnable uncertain parts. In this paper, we establish mathematical correspondence between PR and reinforcement learning (RL), and, based on the connection, expand PR to learn constraints as the extrinsic reward in RL. The resulting algorithm is model-agnostic to apply to any DGMs, and is flexible to adapt arbitrary constraints with the model jointly. Experiments on human image generation and templated sentence generation show models with learned knowledge constraints by our algorithm greatly improve over base generative models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/adam/Zotero/storage/PS4LTDTY/Hu et al. - 2018 - Deep Generative Models with Learnable Knowledge Co.pdf;/Users/adam/Zotero/storage/7NJABVL2/1806.html}
}

@unpublished{ilharcoEditingModelsTask2022,
  title = {Editing {{Models}} with {{Task Arithmetic}}},
  author = {Ilharco, Gabriel and Ribeiro, Marco Tulio and Wortsman, Mitchell and Gururangan, Suchin and Schmidt, Ludwig and Hajishirzi, Hannaneh and Farhadi, Ali},
  year = 2022,
  month = dec,
  journal = {arXiv [cs.LG]},
  abstract = {Changing how pre-trained models behave -- e.g., improving their performance on a downstream task or mitigating biases learned during pre-training -- is a common practice when developing machine learning systems. In this work, we propose a new paradigm for steering the behavior of neural networks, centered around \textbackslash textit\textbraceleft task vectors\textbraceright. A task vector specifies a direction in the weight space of a pre-trained model, such that movement in that direction improves performance on the task. We build task vectors by subtracting the weights of a pre-trained model from the weights of the same model after fine-tuning on a task. We show that these task vectors can be modified and combined together through arithmetic operations such as negation and addition, and the behavior of the resulting model is steered accordingly. Negating a task vector decreases performance on the target task, with little change in model behavior on control tasks. Moreover, adding task vectors together can improve performance on multiple tasks at once. Finally, when tasks are linked by an analogy relationship of the form ``A is to B as C is to D", combining task vectors from three of the tasks can improve performance on the fourth, even when no data from the fourth task is used for training. Overall, our experiments with several models, modalities and tasks show that task arithmetic is a simple, efficient and effective way of editing models.},
  isbn = {2212.04089}
}

@misc{ImmediateMeasuresNeeded,
  title = {Immediate {{Measures Needed}} to {{Tackle Population Decline}} (2015-04-14)},
  journal = {Keidanren},
  urldate = {2023-04-08},
  howpublished = {http://www.keidanren.or.jp/en/policy/2015/037.html},
  langid = {english},
  file = {/Users/adam/Zotero/storage/MMQNFJ4J/037.html}
}

@misc{ImpactFreeSugar,
  title = {The {{Impact}} of {{Free Sugar}} on {{Human Health}}---{{A Narrative Review}} - {{PMC}}},
  urldate = {2023-08-25},
  howpublished = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9966020/},
  file = {/Users/adam/Zotero/storage/7KLV8IDJ/PMC9966020.html}
}

@misc{IndexFeldwinnDarby,
  title = {Index of /Feldwinn/Darby/{{DemoLibrary}}/{{DemoPDFs}}},
  urldate = {2022-07-24},
  howpublished = {https://people.chem.ucsb.edu/feldwinn/darby/DemoLibrary/DemoPDFs/},
  file = {/Users/adam/Zotero/storage/4S8BQV5A/DemoPDFs.html}
}

@misc{IndexFeldwinnDarbya,
  title = {Index of /Feldwinn/Darby/{{DemoLibrary}}/{{DemoPDFs}}},
  urldate = {2022-07-24},
  howpublished = {https://people.chem.ucsb.edu/feldwinn/darby/DemoLibrary/DemoPDFs/},
  file = {/Users/adam/Zotero/storage/3T4R5WBR/DemoPDFs.html}
}

@misc{IndividualStudyRooms2015,
  title = {Individual {{Study Rooms}}},
  year = 2015,
  month = sep,
  journal = {William \& Mary Libraries},
  urldate = {2022-09-12},
  abstract = {Reserve an individual study room~ The library has individual study rooms on the second and third floors available for reservation. See the current floor plans for locations.},
  howpublished = {https://libraries.wm.edu/policies/individual-study-rooms},
  langid = {english},
  file = {/Users/adam/Zotero/storage/CATW6YE2/individual-study-rooms.html}
}

@misc{IndustrialSugarMarket,
  title = {Industrial {{Sugar Market Size}}, {{Share}} \& {{Growth}} \textbar{} {{Forecast}} [2029]},
  urldate = {2023-08-25},
  howpublished = {https://www.fortunebusinessinsights.com/industrial-sugar-market-102462},
  file = {/Users/adam/Zotero/storage/X97ZU6DY/industrial-sugar-market-102462.html}
}

@misc{InfectiousDiseasesBranch,
  title = {Infectious {{Diseases Branch}}},
  urldate = {2022-07-31},
  howpublished = {https://www.cdph.ca.gov/Programs/CID/DCDC/Pages/IDB.aspx},
  file = {/Users/adam/Zotero/storage/VA6KPUJL/IDB.html}
}

@misc{InstituteElectricalElectronics,
  title = {The {{Institute}} of {{Electrical}} and {{Electronics Engineers}} - {{GobblerConnect}}},
  urldate = {2022-07-15},
  howpublished = {https://gobblerconnect.vt.edu/organization/ieee},
  file = {/Users/adam/Zotero/storage/EQ434YZF/ieee.html}
}

@misc{InternationalSchoolsEducation,
  title = {International {{Schools}} and the {{Education System}} in {{Italy}}},
  journal = {InterNations},
  urldate = {2023-04-19},
  abstract = {Read the InterNations guide on the education system and international schools in Italy. An overview to help you choose the best school for your kids.},
  howpublished = {https://www.internations.org/italy-expats/guide/education},
  langid = {english},
  file = {/Users/adam/Zotero/storage/GI76395M/education.html}
}

@misc{ItalyCountriesOffice,
  title = {Italy - {{Countries}} - {{Office}} of the {{Historian}}},
  urldate = {2023-04-19},
  howpublished = {https://history.state.gov/countries/italy},
  file = {/Users/adam/Zotero/storage/LKB9MFB9/italy.html}
}

@misc{ItalyGovernment,
  title = {Italy: {{Government}}},
  shorttitle = {Italy},
  urldate = {2023-04-19},
  abstract = {Italy Government},
  howpublished = {https://globaledge.msu.edu/countries/italy/government},
  langid = {american},
  file = {/Users/adam/Zotero/storage/GV8WKZ86/government.html}
}

@misc{ItalyLanguageCulture,
  title = {Italy - {{Language}}, {{Culture}}, {{Customs}} and {{Etiquette}}},
  urldate = {2023-04-19},
  howpublished = {https://www.commisceo-global.com/resources/country-guides/italy-guide},
  file = {/Users/adam/Zotero/storage/FBYIIZ2A/italy-guide.html}
}

@article{iwasakiShrinkageRegionalCities2021,
  title = {Shrinkage of Regional Cities in {{Japan}}: {{Analysis}} of Changes in Densely Inhabited Districts},
  shorttitle = {Shrinkage of Regional Cities in {{Japan}}},
  author = {Iwasaki, Yukiya},
  year = 2021,
  month = jun,
  journal = {Cities},
  volume = {113},
  pages = {103168},
  issn = {0264-2751},
  doi = {10.1016/j.cities.2021.103168},
  urldate = {2023-04-17},
  abstract = {City populations are shrinking all over the world, and this subject has gained attention in recent years. In Japan, policies for addressing population decline in regional areas have focused on prefectural capitals. This study analyzes the relationships between the densely inhabited districts (DIDs) of central cities (prefectural capitals) and the DIDs of their surrounding areas from 1960 to 2015, in regional areas of Japan. Population changes between a central city's DID and its surrounding DIDs were classified into four categories according to the developmental processes involved. The results suggest that the demographic development process of central city and surrounding DIDs is related to the concentration of population and industry in the central city. In addition, the population decline of central city DIDs is caused by a combination of factors such as a decrease in the population of young people, outflow of business establishments from the central city, and social decline resulting from the outflow of residents and formation of residential areas outside central city DIDs.},
  langid = {english},
  keywords = {Central city,Densely inhabited districts (DID),Regional area,Urban shrinkage},
  file = {/Users/adam/Zotero/storage/SRV9BEG4/Iwasaki - 2021 - Shrinkage of regional cities in Japan Analysis of.pdf;/Users/adam/Zotero/storage/QQ229I4U/S0264275121000664.html}
}

@article{jackIssueJapansAging,
  title = {The {{Issue}} of {{Japan}}'s {{Aging Population}}},
  author = {Jack, Dallin},
  langid = {english},
  file = {/Users/adam/Zotero/storage/8F5EAZYN/Jack - The Issue of Japan's Aging Population.pdf}
}

@misc{jacksongalaxyStopConstantMeow2021,
  title = {Stop {{The Constant Meow}}: 6 {{Reasons Why Your Cat Over-Vocalizes}}},
  shorttitle = {Stop {{The Constant Meow}}},
  author = {{Jackson Galaxy}},
  year = 2021,
  month = mar,
  urldate = {2024-07-17},
  abstract = {Do you want to Stop The Constant Cat Meow? Meowing is a specific vocalization from cats to humans.  In fact, cat's rarely meow at each other past the time that they're small kittens. But what happens when your cat is meowing at you constantly. What does it mean? And can you make it stop? Watch this video to find out! Check out our online store! https://shop.jacksongalaxy.com/ And our Amazon store! https://www.amazon.com/stores/Jackson... Related Videos Cat Litter Rant - You're Doing it Wrong ~~~{$\bullet~$}YOU'RE~DOING~CAT~LITTER~WRONG~\&~Here'...~~ What does Separation Anxiety look like ~~~{$\bullet~$}What~does~Separation~Anxiety~look~lik...~~ Your cat's meowing might also mean he/she needs you to grab something... from our store! https://shop.jacksongalaxy.com/?utm\_s... -------------------------------------------------------------------------------------------  Don't forget to subscribe and ring that bell!  Join this channel to get access to perks:     ~~~/~@jacksongalaxy~~     Calling all pet parents!  Share your video moments with me...and maybe the world!  Upload your clips here  https://www.jacksongalaxy.com/submit  My Favorite Cat Products (naturally!)  http://jacksongalaxy.com   Follow Me Here!  INSTAGRAM  ~~/~thecatdaddy~~ FACEBOOK  https://www.fb.com/JacksonGalaxy TWITTER  ~~/~jacksongalaxy~~ TIKTOK  ~~/~jacksongalaxy~~ REDDIT  ~~/~jackson-g.~~.   The Jackson Galaxy Project  We make a difference for at-risk animals and the people who care for them. Join us!   http://www.thejacksongalaxyproject.org/ \#JacksonGalaxy \#Cat \#Advice}
}

@unpublished{jangKnowledgeUnlearningMitigating2022,
  title = {Knowledge {{Unlearning}} for {{Mitigating Privacy Risks}} in {{Language Models}}},
  author = {Jang, Joel and Yoon, Dongkeun and Yang, Sohee and Cha, Sungmin and Lee, Moontae and Logeswaran, Lajanugen and Seo, Minjoon},
  year = 2022,
  month = oct,
  journal = {arXiv [cs.CL]},
  abstract = {Pretrained Language Models (LMs) memorize a vast amount of knowledge during initial pretraining, including information that may violate the privacy of personal lives and identities. Previous work addressing privacy issues for language models has mostly focused on data preprocessing and differential privacy methods, both requiring re-training the underlying LM. We propose knowledge unlearning as an alternative method to reduce privacy risks for LMs post hoc. We show that simply performing gradient ascent on target token sequences is effective at forgetting them with little to no degradation of general language modeling performances for larger LMs; it sometimes even substantially improves the underlying LM with just a few iterations. We also find that sequential unlearning is better than trying to unlearn all the data at once and that unlearning is highly dependent on which kind of data (domain) is forgotten. By showing comparisons with a previous data preprocessing method and a decoding method known to mitigate privacy risks for LMs, we show that unlearning can give a stronger empirical privacy guarantee in scenarios where the data vulnerable to extraction attacks are known a priori while being much more efficient and robust. We release the code and dataset needed to replicate our results at https://github.com/joeljang/knowledge-unlearning.},
  isbn = {2210.01504}
}

@misc{JapanEconomicResearch,
  title = {{Japan Economic Research Institute \textbar{} }},
  urldate = {2023-04-08},
  langid = {japanese},
  file = {/Users/adam/Zotero/storage/LX34QQBE/en.html}
}

@misc{JAPANGERIATRICSSOCIETY,
  title = {{{THE JAPAN GERIATRICS SOCIETY}}},
  urldate = {2023-04-08},
  howpublished = {https://www.jpn-geriat-soc.or.jp/en/},
  file = {/Users/adam/Zotero/storage/NYQ5S64E/en.html}
}

@misc{JapanRuralPopulation,
  title = {Japan {{Rural Population}} 1960-2023},
  urldate = {2023-04-08},
  abstract = {Rural population refers to people living in rural areas as defined by national statistical offices. It is calculated as the difference between total population and urban population. Aggregation of urban and rural population may not add up to total population because of different country coverages.},
  howpublished = {https://www.macrotrends.net/countries/JPN/japan/rural-population},
  file = {/Users/adam/Zotero/storage/WECFTHRL/rural-population.html}
}

@article{jehnDynamicNatureConflict2001,
  title = {The {{Dynamic Nature}} of {{Conflict}}: {{A Longitudinal Study}} of {{Intragroup Conflict}} and {{Group Performance}}.},
  author = {Jehn, Karen A. and Mannix, Elizabeth A.},
  year = 2001,
  journal = {Academy of Management Journal},
  volume = {44},
  pages = {238--251},
  publisher = {Academy of Management},
  address = {US},
  issn = {1948-0989(Electronic),0001-4273(Print)},
  doi = {10.2307/3069453},
  abstract = {In a longitudinal study, we found that higher group performance was associated with a particular pattern of conflict. Teams performing well were characterized by low but increasing levels of process conflict, low levels of relationship conflict, with a rise near project deadlines, and moderate levels of task conflict at the midpoint of group interaction. The members of teams with this ideal conflict profile had similar pre-established value systems, high levels of trust and respect, and open discussion norms around conflict during the middle stages of their interaction. (PsycINFO Database Record (c) 2019 APA, all rights reserved)},
  keywords = {*Conflict,*Group Dynamics,*Group Performance,*Work Teams,Business and Industrial Personnel}
}

@article{jehnDynamicNatureConflict2001a,
  title = {The {{Dynamic Nature}} of {{Conflict}}: {{A Longitudinal Study}} of {{Intragroup Conflict}} and {{Group Performance}}.},
  author = {Jehn, Karen A. and Mannix, Elizabeth A.},
  year = 2001,
  journal = {Academy of Management Journal},
  volume = {44},
  pages = {238--251},
  publisher = {Academy of Management},
  address = {US},
  issn = {1948-0989(Electronic),0001-4273(Print)},
  doi = {10.2307/3069453},
  abstract = {In a longitudinal study, we found that higher group performance was associated with a particular pattern of conflict. Teams performing well were characterized by low but increasing levels of process conflict, low levels of relationship conflict, with a rise near project deadlines, and moderate levels of task conflict at the midpoint of group interaction. The members of teams with this ideal conflict profile had similar pre-established value systems, high levels of trust and respect, and open discussion norms around conflict during the middle stages of their interaction. (PsycINFO Database Record (c) 2019 APA, all rights reserved)},
  keywords = {*Conflict,*Group Dynamics,*Group Performance,*Work Teams,Business and Industrial Personnel}
}

@article{jehnDynamicNatureConflict2001b,
  title = {The {{Dynamic Nature}} of {{Conflict}}: {{A Longitudinal Study}} of {{Intragroup Conflict}} and {{Group Performance}}.},
  author = {Jehn, Karen A. and Mannix, Elizabeth A.},
  year = 2001,
  journal = {Academy of Management Journal},
  volume = {44},
  pages = {238--251},
  publisher = {Academy of Management},
  address = {US},
  issn = {1948-0989(Electronic),0001-4273(Print)},
  doi = {10.2307/3069453},
  abstract = {In a longitudinal study, we found that higher group performance was associated with a particular pattern of conflict. Teams performing well were characterized by low but increasing levels of process conflict, low levels of relationship conflict, with a rise near project deadlines, and moderate levels of task conflict at the midpoint of group interaction. The members of teams with this ideal conflict profile had similar pre-established value systems, high levels of trust and respect, and open discussion norms around conflict during the middle stages of their interaction. (PsycINFO Database Record (c) 2019 APA, all rights reserved)},
  keywords = {*Conflict,*Group Dynamics,*Group Performance,*Work Teams,Business and Industrial Personnel}
}

@misc{jiangAdaptiveDataOptimization2024,
  title = {Adaptive {{Data Optimization}}: {{Dynamic Sample Selection}} with {{Scaling Laws}}},
  shorttitle = {Adaptive {{Data Optimization}}},
  author = {Jiang, Yiding and Zhou, Allan and Feng, Zhili and Malladi, Sadhika and Kolter, J. Zico},
  year = 2024,
  month = oct,
  number = {arXiv:2410.11820},
  eprint = {2410.11820},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2410.11820},
  urldate = {2025-07-08},
  abstract = {The composition of pretraining data is a key determinant of foundation models' performance, but there is no standard guideline for allocating a limited computational budget across different data sources. Most current approaches either rely on extensive experiments with smaller models or dynamic data adjustments that also require proxy models, both of which significantly increase the workflow complexity and computational overhead. In this paper, we introduce Adaptive Data Optimization (ADO), an algorithm that optimizes data distributions in an online fashion, concurrent with model training. Unlike existing techniques, ADO does not require external knowledge, proxy models, or modifications to the model update. Instead, ADO uses per-domain scaling laws to estimate the learning potential of each domain during training and adjusts the data mixture accordingly, making it more scalable and easier to integrate. Experiments demonstrate that ADO can achieve comparable or better performance than prior methods while maintaining computational efficiency across different computation scales, offering a practical solution for dynamically adjusting data distribution without sacrificing flexibility or increasing costs. Beyond its practical benefits, ADO also provides a new perspective on data collection strategies via scaling laws.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/adam/Zotero/storage/KKL2GZIX/Jiang et al. - 2024 - Adaptive Data Optimization Dynamic Sample Selecti.pdf;/Users/adam/Zotero/storage/59VZRR3W/2410.html}
}

@misc{jiangWildTeamingScaleIntheWild2024,
  title = {{{WildTeaming}} at {{Scale}}: {{From In-the-Wild Jailbreaks}} to ({{Adversarially}}) {{Safer Language Models}}},
  shorttitle = {{{WildTeaming}} at {{Scale}}},
  author = {Jiang, Liwei and Rao, Kavel and Han, Seungju and Ettinger, Allyson and Brahman, Faeze and Kumar, Sachin and Mireshghallah, Niloofar and Lu, Ximing and Sap, Maarten and Choi, Yejin and Dziri, Nouha},
  year = 2024,
  month = jun,
  number = {arXiv:2406.18510},
  eprint = {2406.18510},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2406.18510},
  urldate = {2025-06-30},
  abstract = {We introduce WildTeaming, an automatic LLM safety red-teaming framework that mines in-the-wild user-chatbot interactions to discover 5.7K unique clusters of novel jailbreak tactics, and then composes multiple tactics for systematic exploration of novel jailbreaks. Compared to prior work that performed red-teaming via recruited human workers, gradient-based optimization, or iterative revision with LLMs, our work investigates jailbreaks from chatbot users who were not specifically instructed to break the system. WildTeaming reveals previously unidentified vulnerabilities of frontier LLMs, resulting in up to 4.6x more diverse and successful adversarial attacks compared to state-of-the-art jailbreak methods. While many datasets exist for jailbreak evaluation, very few open-source datasets exist for jailbreak training, as safety training data has been closed even when model weights are open. With WildTeaming we create WildJailbreak, a large-scale open-source synthetic safety dataset with 262K vanilla (direct request) and adversarial (complex jailbreak) prompt-response pairs. To mitigate exaggerated safety behaviors, WildJailbreak provides two contrastive types of queries: 1) harmful queries (vanilla \& adversarial) and 2) benign queries that resemble harmful queries in form but contain no harm. As WildJailbreak considerably upgrades the quality and scale of existing safety resources, it uniquely enables us to examine the scaling effects of data and the interplay of data properties and model capabilities during safety training. Through extensive experiments, we identify the training properties that enable an ideal balance of safety behaviors: appropriate safeguarding without over-refusal, effective handling of vanilla and adversarial queries, and minimal, if any, decrease in general capabilities. All components of WildJailbeak contribute to achieving balanced safety behaviors of models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/adam/Zotero/storage/8PTRDEDZ/Jiang et al. - 2024 - WildTeaming at Scale From In-the-Wild Jailbreaks .pdf;/Users/adam/Zotero/storage/62J538BH/2406.html}
}

@unpublished{jiaSOULUnlockingPower2024,
  title = {{{SOUL}}: {{Unlocking}} the {{Power}} of {{Second-Order Optimization}} for {{LLM Unlearning}}},
  author = {Jia, Jinghan and Zhang, Yihua and Zhang, Yimeng and Liu, Jiancheng and Runwal, Bharat and Diffenderfer, James and Kailkhura, Bhavya and Liu, Sijia},
  year = 2024,
  month = apr,
  journal = {arXiv [cs.LG]},
  abstract = {Large Language Models (LLMs) have highlighted the necessity of effective unlearning mechanisms to comply with data regulations and ethical AI practices. LLM unlearning aims at removing undesired data influences and associated model capabilities without compromising utility beyond the scope of unlearning. While interest in studying LLM unlearning is growing, the impact of the optimizer choice for LLM unlearning remains unexplored. In this work, we shed light on the significance of optimizer selection in LLM unlearning for the first time, establishing a clear connection between second-order optimization and influence unlearning (a classical approach using influence functions to update the model for data influence removal). This insight propels us to develop a second-order optimization-based LLM unlearning framework, termed Second-Order UnLearning (SOUL), which extends the static, one-shot model update using influence unlearning to a dynamic, iterative unlearning process. Our extensive experiments show that SOUL consistently outperforms conventional first-order methods across various unlearning tasks, models, and metrics, indicating that second-order optimization offers an effective and broadly applicable solution for LLM unlearning. Codes are available at https://github.com/OPTML-Group/SOUL.},
  isbn = {2404.18239}
}

@misc{jinDiscoveringHierarchicalLatent2025,
  title = {Discovering {{Hierarchical Latent Capabilities}} of {{Language Models}} via {{Causal Representation Learning}}},
  author = {Jin, Jikai and Syrgkanis, Vasilis and Kakade, Sham and Zhang, Hanlin},
  year = 2025,
  month = jun,
  number = {arXiv:2506.10378},
  eprint = {2506.10378},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2506.10378},
  urldate = {2025-08-11},
  abstract = {Faithful evaluation of language model capabilities is crucial for deriving actionable insights that can inform model development. However, rigorous causal evaluations in this domain face significant methodological challenges, including complex confounding effects and prohibitive computational costs associated with extensive retraining. To tackle these challenges, we propose a causal representation learning framework wherein observed benchmark performance is modeled as a linear transformation of a few latent capability factors. Crucially, these latent factors are identified as causally interrelated after appropriately controlling for the base model as a common confounder. Applying this approach to a comprehensive dataset encompassing over 1500 models evaluated across six benchmarks from the Open LLM Leaderboard, we identify a concise three-node linear causal structure that reliably explains the observed performance variations. Further interpretation of this causal structure provides substantial scientific insights beyond simple numerical rankings: specifically, we reveal a clear causal direction starting from general problem-solving capabilities, advancing through instruction-following proficiency, and culminating in mathematical reasoning ability. Our results underscore the essential role of carefully controlling base model variations during evaluation, a step critical to accurately uncovering the underlying causal relationships among latent model capabilities.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/adam/Zotero/storage/HVEPF3UJ/Jin et al. - 2025 - Discovering Hierarchical Latent Capabilities of La.pdf;/Users/adam/Zotero/storage/LPBMHF4D/2506.html}
}

@misc{jinRWKUBenchmarkingRealWorld2024,
  title = {{{RWKU}}: {{Benchmarking Real-World Knowledge Unlearning}} for {{Large Language Models}}},
  shorttitle = {{{RWKU}}},
  author = {Jin, Zhuoran and Cao, Pengfei and Wang, Chenhao and He, Zhitao and Yuan, Hongbang and Li, Jiachun and Chen, Yubo and Liu, Kang and Zhao, Jun},
  year = 2024,
  month = jun,
  number = {arXiv:2406.10890},
  eprint = {2406.10890},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-10-11},
  abstract = {Large language models (LLMs) inevitably memorize sensitive, copyrighted, and harmful knowledge from the training corpus; therefore, it is crucial to erase this knowledge from the models. Machine unlearning is a promising solution for efficiently removing specific knowledge by post hoc modifying models. In this paper, we propose a Real-World Knowledge Unlearning benchmark ( RWKU) for LLM unlearning. RWKU is designed based on the following three key factors: (1) For the task setting, we consider a more practical and challenging unlearning setting, where neither the forget corpus nor the retain corpus is accessible. (2) For the knowledge source, we choose 200 real-world famous people as the unlearning targets and show that such popular knowledge is widely present in various LLMs. (3) For the evaluation framework, we design the forget set and the retain set to evaluate the model's capabilities across various real-world applications. Regarding the forget set, we provide four membership inference attack (MIA) methods and nine kinds of adversarial attack probes to rigorously test unlearning efficacy. Regarding the retain set, we assess locality and utility in terms of neighbor perturbation, general ability, reasoning ability, truthfulness, factuality, and fluency. We conduct extensive experiments across two unlearning scenarios, two models and six baseline methods and obtain some meaningful findings. We release our benchmark and code publicly at http://rwku-bench.github.io for future work.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/adam/Zotero/storage/P9KQN6ZG/Jin et al. - 2024 - RWKU Benchmarking Real-World Knowledge Unlearning.pdf}
}

@incollection{jollimoreImpartiality2023,
  title = {Impartiality},
  booktitle = {The {{Stanford Encyclopedia}} of {{Philosophy}}},
  author = {Jollimore, Troy},
  editor = {Zalta, Edward N. and Nodelman, Uri},
  year = 2023,
  edition = {Winter 2023},
  publisher = {Metaphysics Research Lab, Stanford University},
  urldate = {2024-10-06},
  abstract = {Impartiality is sometimes treated by philosophers as if it wereequivalent to moral impartiality. Or, at the very least, theformer word is often used, without the qualifying adjective`moral', even when it is the particularly moral conceptthat is intended. This is misleading, since impartiality in itsbroadest sense is best understood as a formal notion, while moralimpartiality in particular is a substantive concept -- and oneconcerning which there is considerable dispute.},
  keywords = {animals moral status of,bias implicit,Chinese Philosophy: ethics,Chinese Philosophy: Mohism,consequentialism,egalitarianism,equality,ethics: deontological,friendship,Godwin William,justice: global,justice: international distributive,justification political: public,Kant Immanuel: moral philosophy,Mill John Stuart,obligations: special,original position},
  file = {/Users/adam/Zotero/storage/2VLIVDNN/impartiality.html}
}

@unpublished{kadheSplitUnlearnMerge2024,
  title = {Split, {{Unlearn}}, {{Merge}}: {{Leveraging Data Attributes}} for {{More Effective Unlearning}} in {{LLMs}}},
  author = {Kadhe, Swanand Ravindra and Ahmed, Farhan and Wei, Dennis and Baracaldo, Nathalie and Padhi, Inkit},
  year = 2024,
  month = jun,
  journal = {arXiv [cs.LG]},
  abstract = {Large language models (LLMs) have shown to pose social and ethical risks such as generating toxic language or facilitating malicious use of hazardous knowledge. Machine unlearning is a promising approach to improve LLM safety by directly removing harmful behaviors and knowledge. In this paper, we propose "SPlit, UNlearn, MerGE" (SPUNGE), a framework that can be used with any unlearning method to amplify its effectiveness. SPUNGE leverages data attributes during unlearning by splitting unlearning data into subsets based on specific attribute values, unlearning each subset separately, and merging the unlearned models. We empirically demonstrate that SPUNGE significantly improves the performance of two recent unlearning methods on state-of-the-art LLMs while maintaining their general capabilities on standard academic benchmarks.},
  isbn = {2406.11780}
}

@article{kangasIdentificationSevenChemical2018,
  title = {The {{Identification}} of {{Seven Chemical Warfare Mimics Using}} a {{Colorimetric Array}}},
  author = {Kangas, {\relax MJ} and Ernest, A and Lukowicz, R and Mora, {\relax AV} and Quossi, A and Perez, M and Kyes, N and Holmes, {\relax AE}},
  year = 2018,
  month = dec,
  journal = {SENSORS},
  volume = {18},
  number = {12},
  issn = {1424-8220},
  doi = {10.3390/s18124291},
  abstract = {Chemical warfare agents pose significant threats in the 21st century, especially for armed forces. A colorimetric detection array was developed to identify warfare mimics, including mustard gas and nerve agents. In total, 188 sensors were screened to determine the best sensor performance, in order to identify warfare mimics 2-chloro ethyl ethylsulfide, 2-2'-thiodiethanol, trifluoroacetic acid, methylphosphonic acid, dimethylphosphite, diethylcyanophosphonate, and diethyl (methylthiomethyl) phosphonate. The highest loadings in the principle component analysis (PCA) plots were used to identify the sensors that were most effective in analyzing the RGB data to classify the warfare mimics. The dataset was reduced to only twelve sensors, and PCA results gave comparable results as the large data did, demonstrating that only twelve sensors are needed to classify the warfare mimics.},
  langid = {english},
  keywords = {colorimetric array,mustard gas,NERVE AGENTS,principle component analysis,RGB data,SENSOR ARRAYS,warfare mimics}
}

@article{kangasIdentificationSevenChemical2018a,
  title = {The {{Identification}} of {{Seven Chemical Warfare Mimics Using}} a {{Colorimetric Array}}},
  author = {Kangas, {\relax MJ} and Ernest, A and Lukowicz, R and Mora, {\relax AV} and Quossi, A and Perez, M and Kyes, N and Holmes, {\relax AE}},
  year = 2018,
  month = dec,
  journal = {SENSORS},
  volume = {18},
  number = {12},
  issn = {1424-8220},
  doi = {10.3390/s18124291},
  abstract = {Chemical warfare agents pose significant threats in the 21st century, especially for armed forces. A colorimetric detection array was developed to identify warfare mimics, including mustard gas and nerve agents. In total, 188 sensors were screened to determine the best sensor performance, in order to identify warfare mimics 2-chloro ethyl ethylsulfide, 2-2'-thiodiethanol, trifluoroacetic acid, methylphosphonic acid, dimethylphosphite, diethylcyanophosphonate, and diethyl (methylthiomethyl) phosphonate. The highest loadings in the principle component analysis (PCA) plots were used to identify the sensors that were most effective in analyzing the RGB data to classify the warfare mimics. The dataset was reduced to only twelve sensors, and PCA results gave comparable results as the large data did, demonstrating that only twelve sensors are needed to classify the warfare mimics.},
  langid = {english},
  keywords = {colorimetric array,mustard gas,NERVE AGENTS,principle component analysis,RGB data,SENSOR ARRAYS,warfare mimics}
}

@article{kellerSkinColourAffects2022,
  title = {Skin Colour Affects the Accuracy of Medical Oxygen Sensors},
  author = {Keller, Matthew D. and {Harrison-Smith}, Brandon and Patil, Chetan and Arefin, Mohammed Shahriar},
  year = 2022,
  month = oct,
  journal = {Nature},
  volume = {610},
  number = {7932},
  pages = {449--451},
  publisher = {Nature Publishing Group},
  doi = {10.1038/d41586-022-03161-1},
  urldate = {2024-10-16},
  abstract = {Two views on the pigmentation dependence of pulse oximetry.},
  copyright = {2022 Springer Nature Limited},
  langid = {english},
  keywords = {Health care,Medical research,Optics and photonics},
  annotation = {Bandiera\_abtest: a\\
Cg\_type: News \& Views Forum\\
Subject\_term: Health care, Medical research, Optics and photonics},
  file = {/Users/adam/Zotero/storage/Q3CFT2BC/Keller et al. - 2022 - Skin colour affects the accuracy of medical oxygen.pdf;/Users/adam/Zotero/storage/XQGB4EDU/d41586-022-03161-1.html}
}

@misc{kernLibraryGuidesComputer,
  title = {Library {{Guides}}: {{Computer Science}} and {{Engineering}}: {{Main Parts}} of a {{Scientific}}/{{Technical Paper}}},
  shorttitle = {Library {{Guides}}},
  author = {Kern, Sara},
  urldate = {2024-09-29},
  abstract = {The Computer Science and Engineering guide provides links to information on all topics related to computer science and computer engineering in relevant databases, journals, conference proceedings, technical reports, websites, professional societies, etc.},
  copyright = {Copyright Penn State University 2024},
  howpublished = {https://guides.libraries.psu.edu/c.php?g=371360\&p=8352512},
  langid = {english},
  file = {/Users/adam/Zotero/storage/K4A742G6/c.html}
}

@misc{kharawalaPhotoRickyKharawala2015,
  title = {Photo by {{Ricky Kharawala}} on {{Unsplash}}},
  author = {Kharawala, Ricky},
  year = 2015,
  month = feb,
  urldate = {2024-07-17},
  abstract = {Download this photo by Ricky  Kharawala on Unsplash},
  howpublished = {https://unsplash.com/photos/selective-focus-photography-of-brown-hamster-adK3Vu70DEQ},
  langid = {american}
}

@misc{khouryHowSecureCode2023,
  title = {How {{Secure}} Is {{Code Generated}} by {{ChatGPT}}?},
  author = {Khoury, Rapha{\"e}l and Avila, Anderson R. and Brunelle, Jacob and Camara, Baba Mamadou},
  year = 2023,
  month = apr,
  number = {arXiv:2304.09655},
  eprint = {2304.09655},
  publisher = {arXiv},
  urldate = {2024-11-15},
  abstract = {In recent years, large language models have been responsible for great advances in the field of artificial intelligence (AI). ChatGPT in particular, an AI chatbot developed and recently released by OpenAI, has taken the field to the next level. The conversational model is able not only to process human-like text, but also to translate natural language into code. However, the safety of programs generated by ChatGPT should not be overlooked. In this paper, we perform an experiment to address this issue. Specifically, we ask ChatGPT to generate a number of program and evaluate the security of the resulting source code. We further investigate whether ChatGPT can be prodded to improve the security by appropriate prompts, and discuss the ethical aspects of using AI to generate code. Results suggest that ChatGPT is aware of potential vulnerabilities, but nonetheless often generates source code that are not robust to certain attacks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Cryptography and Security},
  file = {/Users/adam/Zotero/storage/MNU8L75E/Khoury et al. - 2023 - How Secure is Code Generated by ChatGPT.pdf;/Users/adam/Zotero/storage/LYYFWSQ6/2304.html}
}

@misc{kimNegMergeConsensualWeight2024,
  title = {{{NegMerge}}: {{Consensual Weight Negation}} for {{Strong Machine Unlearning}}},
  shorttitle = {{{NegMerge}}},
  author = {Kim, Hyoseo and Han, Dongyoon and Choe, Junsuk},
  year = 2024,
  month = oct,
  number = {arXiv:2410.05583},
  eprint = {2410.05583},
  publisher = {arXiv},
  urldate = {2024-10-21},
  abstract = {Machine unlearning aims to selectively remove specific knowledge from a model. Current methods, such as task arithmetic, rely on fine-tuning models on the forget set, generating a task vector, and subtracting it from the original model. However, we argue the effectiveness of this approach is highly sensitive to hyperparameter selection, necessitating careful validation to identify the best model among many fine-tuned candidates. In this paper, we propose a novel method that leverages all given fine-tuned models rather than selecting a single one. By constructing task vectors from models trained with varied hyperparameters and merging only the components of the task vectors with consistent signs, we perform unlearning by negating the merged task vector from the original model. Given that existing methods also utilize multiple fine-tuned models, our approach delivers more effective unlearning without incurring additional computational costs. We demonstrate the effectiveness of our method on both vision-language models and standard image classification models, showing improved unlearning performance with minimal degradation on the retain set, outperforming state-of-the-art techniques.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/adam/Zotero/storage/3MEC66MV/Kim et al. - 2024 - NegMerge Consensual Weight Negation for Strong Ma.pdf;/Users/adam/Zotero/storage/77IEM9PR/2410.html}
}

@misc{kossenActiveTestingSampleEfficient2021,
  title = {Active {{Testing}}: {{Sample-Efficient Model Evaluation}}},
  shorttitle = {Active {{Testing}}},
  author = {Kossen, Jannik and Farquhar, Sebastian and Gal, Yarin and Rainforth, Tom},
  year = 2021,
  month = jun,
  number = {arXiv:2103.05331},
  eprint = {2103.05331},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2103.05331},
  urldate = {2025-07-08},
  abstract = {We introduce a new framework for sample-efficient model evaluation that we call active testing. While approaches like active learning reduce the number of labels needed for model training, existing literature largely ignores the cost of labeling test data, typically unrealistically assuming large test sets for model evaluation. This creates a disconnect to real applications, where test labels are important and just as expensive, e.g. for optimizing hyperparameters. Active testing addresses this by carefully selecting the test points to label, ensuring model evaluation is sample-efficient. To this end, we derive theoretically-grounded and intuitive acquisition strategies that are specifically tailored to the goals of active testing, noting these are distinct to those of active learning. As actively selecting labels introduces a bias; we further show how to remove this bias while reducing the variance of the estimator at the same time. Active testing is easy to implement and can be applied to any supervised machine learning method. We demonstrate its effectiveness on models including WideResNets and Gaussian processes on datasets including Fashion-MNIST and CIFAR-100.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/adam/Zotero/storage/WRK8REYR/Kossen et al. - 2021 - Active Testing Sample-Efficient Model Evaluation.pdf;/Users/adam/Zotero/storage/8VYCY8FN/2103.html}
}

@article{kurmanjiUnboundedMachineUnlearning2023,
  title = {Towards Unbounded Machine Unlearning},
  author = {Kurmanji, M and Triantafillou, P and Triantafillou, Eleni},
  year = 2023,
  month = feb,
  journal = {Adv. Neural Inf. Process. Syst.},
  volume = {abs/2302.09880},
  publisher = {proceedings.neurips.cc},
  issn = {1049-5258},
  doi = {10.48550/arXiv.2302.09880},
  abstract = {Deep machine unlearning is the problem of `removing' from a trained neural network a subset of its training set. This problem is very timely and has many applications, including the key tasks of removing biases (RB), resolving confusion (RC) (caused by mislabelled data in trained models), as well as allowing users to exercise their `right to be forgotten' to protect User Privacy (UP). This paper is the first, to our knowledge, to study unlearning for different applications (RB, RC, UP), with the view that each has its own desiderata, definitions for `forgetting' and associated metrics for forget quality. For UP, we propose a novel adaptation of a strong Membership Inference Attack for unlearning. We also propose SCRUB, a novel unlearning algorithm, which is the only method that is consistently a top performer for forget quality across the different application-dependent metrics for RB, RC, and UP. At the same time, SCRUB is also consistently a top performer on metrics that measure model utility (i.e. accuracy on retained data and generalization), and is more efficient than previous work. The above are substantiated through a comprehensive empirical evaluation against previous state-of-the-art.}
}

@article{kusakariEntrustingFutureRural2018,
  title = {Entrusting the {{Future}} of {{Rural Society}} through {{Nurturing Civic Pride}}: {{Endeavors}} in {{Gojome Town}}, {{Akita Prefecture}} of {{Japan}}},
  shorttitle = {Entrusting the {{Future}} of {{Rural Society}} through {{Nurturing Civic Pride}}},
  author = {Kusakari, Yasuko and Chiu, Dominique and Muasa, Lillian and Takahashi, Kyoko and Kudo, Shogo},
  year = 2018,
  month = jul,
  journal = {Consilience},
  number = {20},
  issn = {1948-3074},
  doi = {10.7916/consilience.v0i20.3772},
  urldate = {2023-04-17},
  abstract = {This photo essay illustrates a study on endeavors in a rural Japanese town with a declining population and discusses the future of rural society from a perspective of civic pride as well as sustainability. The population of the town of Gojome, located in Akita Prefecture of northern Japan, has halved from 20,025 in 1960 to 9,481 in 2015. Depopulation is often perceived as a half-empty glass, but we approached our study with the view that it is half-full. We discuss the issue through appreciative approaches by adopting the concept of ``civic pride'' as an alternative lens based on our empirical survey. The methodology adopted for this field survey include individual interviews with 126 respondents (consisting of 79 residents of Gojome and 47 individuals who were visiting the town) across different generations, 15 key informant interviews, drawings of their pride about the town by 34 sixth-graders of Gojome Elementary School, and observations. It was found that various generations of proud Gojome locals and non-locals have started to proactively participate in the process of bettering the town through various social, cultural, economic, and environmental activities. Based on our learning in Gojome town we conclude that, in the wake of rural depopulation, nurturing ``civic pride'' could potentially contribute to rural sustainability through entrusting the future of rural society to next generations.},
  copyright = {Copyright (c) 2018},
  langid = {english},
  file = {/Users/adam/Zotero/storage/AN7Z23LE/Kusakari et al. - 2018 - Entrusting the Future of Rural Society through Nur.pdf}
}

@misc{LabflowReportReport,
  title = {Labflow - Report: {{Report}} - {{Synthesis}} of {{Malachite}} and {{Verdigris}}},
  urldate = {2022-07-21},
  howpublished = {https://labflow.com/app/course/1931/report/39311\#tab=REPORT},
  file = {/Users/adam/Zotero/storage/NKSS3DPT/39311.html}
}

@misc{LabflowReportReporta,
  title = {Labflow - Report: {{Report}} - {{Synthesis}} of {{Malachite}} and {{Verdigris}}},
  urldate = {2022-07-21},
  howpublished = {https://labflow.com/app/course/1931/report/39311\#tab=REPORT},
  file = {/Users/adam/Zotero/storage/XZKQD7F8/39311.html}
}

@article{lecunTutorialEnergyBasedLearning,
  title = {A {{Tutorial}} on {{Energy}}\-{{Based Learning}}},
  author = {LeCun, Yann},
  langid = {english},
  file = {/Users/adam/Zotero/storage/RCADDIRK/LeCun - A Tutorial on EnergyBased Learning.pdf}
}

@unpublished{levFasterMachineUnlearning2024,
  title = {Faster {{Machine Unlearning}} via {{Natural Gradient Descent}}},
  author = {Lev, Omri and Wilson, Ashia},
  year = 2024,
  month = jul,
  journal = {arXiv [cs.LG]},
  abstract = {We address the challenge of efficiently and reliably deleting data from machine learning models trained using Empirical Risk Minimization (ERM), a process known as machine unlearning. To avoid retraining models from scratch, we propose a novel algorithm leveraging Natural Gradient Descent (NGD). Our theoretical framework ensures strong privacy guarantees for convex models, while a practical Min/Max optimization algorithm is developed for non-convex models. Comprehensive evaluations show significant improvements in privacy, computational efficiency, and generalization compared to state-of-the-art methods, advancing both the theoretical and practical aspects of machine unlearning.},
  isbn = {2407.08169}
}

@misc{liCanMultiplechoiceQuestions2024,
  title = {Can Multiple-Choice Questions Really Be Useful in Detecting the Abilities of {{LLMs}}?},
  author = {Li, Wangyue and Li, Liangzhi and Xiang, Tong and Liu, Xiao and Deng, Wei and Garcia, Noa},
  year = 2024,
  month = may,
  number = {arXiv:2403.17752},
  eprint = {2403.17752},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-07-20},
  abstract = {Multiple-choice questions (MCQs) are widely used in the evaluation of large language models (LLMs) due to their simplicity and efficiency. However, there are concerns about whether MCQs can truly measure LLM's capabilities, particularly in knowledge-intensive scenarios where long-form generation (LFG) answers are required. The misalignment between the task and the evaluation method demands a thoughtful analysis of MCQ's efficacy, which we undertake in this paper by evaluating nine LLMs on four question-answering (QA) datasets in two languages: Chinese and English. We identify a significant issue: LLMs exhibit an order sensitivity in bilingual MCQs, favoring answers located at specific positions, i.e., the first position. We further quantify the gap between MCQs and long-form generation questions (LFGQs) by comparing their direct outputs, token logits, and embeddings. Our results reveal a relatively low correlation between answers from MCQs and LFGQs for identical questions. Additionally, we propose two methods to quantify the consistency and confidence of LLMs' output, which can be generalized to other QA evaluation benchmarks. Notably, our analysis challenges the idea that the higher the consistency, the greater the accuracy. We also find MCQs to be less reliable than LFGQs in terms of expected calibration error. Finally, the misalignment between MCQs and LFGQs is not only reflected in the evaluation performance but also in the embedding space. Our code and models can be accessed at https://github.com/Meetyou-AI-Lab/Can-MC-Evaluate-LLMs.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/adam/Zotero/storage/RWKCJ9K4/Li et al. - 2024 - Can multiple-choice questions really be useful in .pdf}
}

@misc{liContrastiveDecodingOpenended2023,
  title = {Contrastive {{Decoding}}: {{Open-ended Text Generation}} as {{Optimization}}},
  shorttitle = {Contrastive {{Decoding}}},
  author = {Li, Xiang Lisa and Holtzman, Ari and Fried, Daniel and Liang, Percy and Eisner, Jason and Hashimoto, Tatsunori and Zettlemoyer, Luke and Lewis, Mike},
  year = 2023,
  month = jul,
  number = {arXiv:2210.15097},
  eprint = {2210.15097},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2210.15097},
  urldate = {2025-06-30},
  abstract = {Given a language model (LM), maximum probability is a poor decoding objective for open-ended generation, because it produces short and repetitive text. On the other hand, sampling can often produce incoherent text that drifts from the original topics. We propose contrastive decoding (CD), a reliable decoding approach that optimizes a contrastive objective subject to a plausibility constraint. The contrastive objective returns the difference between the likelihood under a large LM (called the expert, e.g. OPT-13B) and a small LM (called the amateur, e.g. OPT-125M), and the constraint ensures that the outputs are plausible. CD is inspired by the fact that the failures of larger LMs (e.g., repetition, incoherence) are even more prevalent in smaller LMs, and that this difference signals which texts should be preferred. CD requires zero additional training, and produces higher quality text than decoding from the larger LM alone. It also works across model scales (OPT-13B and GPT2-1.5B) and significantly outperforms four strong decoding algorithms (e.g., nucleus, top-k) in automatic and human evaluations across wikipedia, news and story domains.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/adam/Zotero/storage/V6MT5GBQ/Li et al. - 2023 - Contrastive Decoding Open-ended Text Generation a.pdf}
}

@unpublished{liMachineUnlearningTaxonomy2024,
  title = {Machine {{Unlearning}}: {{Taxonomy}}, {{Metrics}}, {{Applications}}, {{Challenges}}, and {{Prospects}}},
  author = {Li, Na and Zhou, Chunyi and Gao, Yansong and Chen, Hui and Fu, Anmin and Zhang, Zhi and Shui, Yu},
  year = 2024,
  month = mar,
  journal = {arXiv [cs.LG]},
  abstract = {Personal digital data is a critical asset, and governments worldwide have enforced laws and regulations to protect data privacy. Data users have been endowed with the right to be forgotten of their data. In the course of machine learning (ML), the forgotten right requires a model provider to delete user data and its subsequent impact on ML models upon user requests. Machine unlearning emerges to address this, which has garnered ever-increasing attention from both industry and academia. While the area has developed rapidly, there is a lack of comprehensive surveys to capture the latest advancements. Recognizing this shortage, we conduct an extensive exploration to map the landscape of machine unlearning including the (fine-grained) taxonomy of unlearning algorithms under centralized and distributed settings, debate on approximate unlearning, verification and evaluation metrics, challenges and solutions for unlearning under different applications, as well as attacks targeting machine unlearning. The survey concludes by outlining potential directions for future research, hoping to serve as a guide for interested scholars.},
  isbn = {2403.08254}
}

@misc{linkthehollandlopRabbitThumpsLettuce2019,
  title = {Rabbit {{Thumps}} for {{Lettuce}}},
  author = {{Link the Holland Lop}},
  year = 2019,
  month = nov,
  urldate = {2024-07-16},
  abstract = {Link gets angry if he hasn't been fed lettuce in 2 minutes.}
}

@misc{linRho1NotAll2025,
  title = {Rho-1: {{Not All Tokens Are What You Need}}},
  shorttitle = {Rho-1},
  author = {Lin, Zhenghao and Gou, Zhibin and Gong, Yeyun and Liu, Xiao and Shen, Yelong and Xu, Ruochen and Lin, Chen and Yang, Yujiu and Jiao, Jian and Duan, Nan and Chen, Weizhu},
  year = 2025,
  month = jan,
  number = {arXiv:2404.07965},
  eprint = {2404.07965},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2404.07965},
  urldate = {2025-06-30},
  abstract = {Previous language model pre-training methods have uniformly applied a next-token prediction loss to all training tokens. Challenging this norm, we posit that "9l training". Our initial analysis examines token-level training dynamics of language model, revealing distinct loss patterns for different tokens. Leveraging these insights, we introduce a new language model called Rho-1. Unlike traditional LMs that learn to predict every next token in a corpus, Rho-1 employs Selective Language Modeling (SLM), which selectively trains on useful tokens that aligned with the desired distribution. This approach involves scoring pretraining tokens using a reference model, and then training the language model with a focused loss on tokens with higher scores. When continual pretraining on 15B OpenWebMath corpus, Rho-1 yields an absolute improvement in few-shot accuracy of up to 30\% in 9 math tasks. After fine-tuning, Rho-1-1B and 7B achieved state-of-the-art results of 40.6\% and 51.8\% on MATH dataset, respectively - matching DeepSeekMath with only 3\% of the pretraining tokens. Furthermore, when continual pretraining on 80B general tokens, Rho-1 achieves 6.8\% average enhancement across 15 diverse tasks, increasing both efficiency and performance of the language model pre-training.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/adam/Zotero/storage/DVUQU4VE/Lin et al. - 2025 - Rho-1 Not All Tokens Are What You Need.pdf;/Users/adam/Zotero/storage/SZGH3NB2/2404.html}
}

@misc{liuGEMGymAgentic2025,
  title = {{{GEM}}: {{A Gym}} for {{Agentic LLMs}}},
  shorttitle = {{{GEM}}},
  author = {Liu, Zichen and Sims, Anya and Duan, Keyu and Chen, Changyu and Yu, Simon and Zhou, Xiangxin and Xu, Haotian and Xiong, Shaopan and Liu, Bo and Tan, Chenmien and Beh, Chuen Yang and Wang, Weixun and Zhu, Hao and Shi, Weiyan and Yang, Diyi and Shieh, Michael and Teh, Yee Whye and Lee, Wee Sun and Lin, Min},
  year = 2025,
  month = oct,
  journal = {arXiv.org},
  urldate = {2026-02-21},
  abstract = {The training paradigm for large language models (LLMs) is moving from static datasets to experience-based learning, where agents acquire skills via interacting with complex environments. To facilitate this transition we introduce GEM (General Experience Maker), an open-source environment simulator designed for the age of LLMs. Analogous to OpenAI-Gym for traditional reinforcement learning (RL), GEM provides a standardized framework for the environment-agent interface, including asynchronous vectorized execution for high throughput, and flexible wrappers for easy extensibility. GEM also features a diverse suite of environments, robust integrated tools, and single-file example scripts demonstrating using GEM with five popular RL training frameworks. Along with this, we also provide a set of baselines across 24 environments using REINFORCE with Return Batch Normalization (ReBN), which -- unlike GRPO -- is compatible with the full RL setting of dense per-turn rewards and offers better credit assignment. We further conduct apple-to-apple benchmarking of PPO, GRPO and REINFORCE in both single- and multi-turn settings using GEM to shed light on the algorithmic designs. Lastly, GEM also functions as a convenient evaluation toolkit besides a training environment. We hope this framework can help accelerate future agentic LLM research.},
  howpublished = {https://arxiv.org/abs/2510.01051v1},
  langid = {english},
  file = {/Users/adam/Zotero/storage/KN57PTEB/Liu et al. - 2025 - GEM A Gym for Agentic LLMs.pdf}
}

@misc{liuGEMGymAgentic2025a,
  title = {{{GEM}}: {{A Gym}} for {{Agentic LLMs}}},
  shorttitle = {{{GEM}}},
  author = {Liu, Zichen and Sims, Anya and Duan, Keyu and Chen, Changyu and Yu, Simon and Zhou, Xiangxin and Xu, Haotian and Xiong, Shaopan and Liu, Bo and Tan, Chenmien and Beh, Chuen Yang and Wang, Weixun and Zhu, Hao and Shi, Weiyan and Yang, Diyi and Shieh, Michael and Teh, Yee Whye and Lee, Wee Sun and Lin, Min},
  year = 2025,
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2510.01051},
  urldate = {2026-02-21},
  abstract = {The training paradigm for large language models (LLMs) is moving from static datasets to experience-based learning, where agents acquire skills via interacting with complex environments. To facilitate this transition we introduce GEM (General Experience Maker), an open-source environment simulator designed for the age of LLMs. Analogous to OpenAI-Gym for traditional reinforcement learning (RL), GEM provides a standardized framework for the environment-agent interface, including asynchronous vectorized execution for high throughput, and flexible wrappers for easy extensibility. GEM also features a diverse suite of environments, robust integrated tools, and single-file example scripts demonstrating using GEM with five popular RL training frameworks. Along with this, we also provide a set of baselines across 24 environments using REINFORCE with Return Batch Normalization (ReBN), which---unlike GRPO---is compatible with the full RL setting of dense per-turn rewards and offers better credit assignment. We further conduct apple-to-apple benchmarking of PPO, GRPO and REINFORCE in both single- and multi-turn settings using GEM to shed light on the algorithmic designs. Lastly, GEM also functions as a convenient evaluation toolkit besides a training environment. We hope this framework can help accelerate future agentic LLM research1.},
  copyright = {Creative Commons Attribution 4.0 International},
  langid = {english},
  keywords = {Artificial Intelligence (cs.AI),Computation and Language (cs.CL),FOS: Computer and information sciences,Machine Learning (cs.LG)},
  file = {/Users/adam/Zotero/storage/MIHNAU22/Liu et al. - 2025 - GEM A Gym for Agentic LLMs.pdf}
}

@unpublished{liuLargeLanguageModel2024,
  title = {Large {{Language Model Unlearning}} via {{Embedding-Corrupted Prompts}}},
  author = {Liu, Chris Yuhao and Wang, Yaxuan and Flanigan, Jeffrey and Liu, Yang},
  year = 2024,
  month = jun,
  journal = {arXiv [cs.CL]},
  abstract = {Large language models (LLMs) have advanced to encompass extensive knowledge across diverse domains. Yet controlling what a large language model should not know is important for ensuring alignment and thus safe use. However, accurately and efficiently unlearning knowledge from an LLM remains challenging due to the potential collateral damage caused by the fuzzy boundary between retention and forgetting, and the large computational requirements for optimization across state-of-the-art models with hundreds of billions of parameters. In this work, we present Embedding-COrrupted (ECO) Prompts, a lightweight unlearning framework for large language models to address both the challenges of knowledge entanglement and unlearning efficiency. Instead of relying on the LLM itself to unlearn, we enforce an unlearned state during inference by employing a prompt classifier to identify and safeguard prompts to forget. We learn corruptions added to prompt embeddings via zeroth order optimization toward the unlearning objective offline and corrupt prompts flagged by the classifier during inference. We find that these embedding-corrupted prompts not only lead to desirable outputs that satisfy the unlearning objective but also closely approximate the output from a model that has never been trained on the data intended for forgetting. Through extensive experiments on unlearning, we demonstrate the superiority of our method in achieving promising unlearning at nearly zero side effects in general domains and domains closely related to the unlearned ones. Additionally, we highlight the scalability of our method to 100 LLMs, ranging from 0.5B to 236B parameters, incurring no additional cost as the number of parameters increases.},
  isbn = {2406.07933}
}

@unpublished{liuMachineUnlearningGenerative2024,
  title = {Machine {{Unlearning}} in {{Generative AI}}: {{A Survey}}},
  author = {Liu, Zheyuan and Dou, Guangyao and Tan, Zhaoxuan and Tian, Yijun and Jiang, Meng},
  year = 2024,
  month = jul,
  journal = {arXiv [cs.LG]},
  abstract = {Generative AI technologies have been deployed in many places, such as (multimodal) large language models and vision generative models. Their remarkable performance should be attributed to massive training data and emergent reasoning abilities. However, the models would memorize and generate sensitive, biased, or dangerous information originated from the training data especially those from web crawl. New machine unlearning (MU) techniques are being developed to reduce or eliminate undesirable knowledge and its effects from the models, because those that were designed for traditional classification tasks could not be applied for Generative AI. We offer a comprehensive survey on many things about MU in Generative AI, such as a new problem formulation, evaluation methods, and a structured discussion on the advantages and limitations of different kinds of MU techniques. It also presents several critical challenges and promising directions in MU research. A curated list of readings can be found: https://github.com/franciscoliu/GenAI-MU-Reading.},
  isbn = {2407.20516},
  keywords = {Read},
  file = {/Users/adam/Zotero/storage/JHWKXJRI/Liu et al. - 2024 - Machine Unlearning in Generative AI A Survey.pdf}
}

@unpublished{liuRethinkingMachineUnlearning2024,
  title = {Rethinking {{Machine Unlearning}} for {{Large Language Models}}},
  author = {Liu, Sijia and Yao, Yuanshun and Jia, Jinghan and Casper, Stephen and Baracaldo, Nathalie and Hase, Peter and Yao, Yuguang and Liu, Chris Yuhao and Xu, Xiaojun and Li, Hang and Varshney, Kush R and Bansal, Mohit and Koyejo, Sanmi and Liu, Yang},
  year = 2024,
  month = feb,
  journal = {arXiv [cs.LG]},
  abstract = {We explore machine unlearning (MU) in the domain of large language models (LLMs), referred to as LLM unlearning. This initiative aims to eliminate undesirable data influence (e.g., sensitive or illegal information) and the associated model capabilities, while maintaining the integrity of essential knowledge generation and not affecting causally unrelated information. We envision LLM unlearning becoming a pivotal element in the life-cycle management of LLMs, potentially standing as an essential foundation for developing generative AI that is not only safe, secure, and trustworthy, but also resource-efficient without the need of full retraining. We navigate the unlearning landscape in LLMs from conceptual formulation, methodologies, metrics, and applications. In particular, we highlight the often-overlooked aspects of existing LLM unlearning research, e.g., unlearning scope, data-model interaction, and multifaceted efficacy assessment. We also draw connections between LLM unlearning and related areas such as model editing, influence functions, model explanation, adversarial training, and reinforcement learning. Furthermore, we outline an effective assessment framework for LLM unlearning and explore its applications in copyright and privacy safeguards and sociotechnical harm reduction.},
  isbn = {2402.08787}
}

@unpublished{liuRethinkingMachineUnlearning2024a,
  title = {Rethinking {{Machine Unlearning}} for {{Large Language Models}}},
  author = {Liu, Sijia and Yao, Yuanshun and Jia, Jinghan and Casper, Stephen and Baracaldo, Nathalie and Hase, Peter and Yao, Yuguang and Liu, Chris Yuhao and Xu, Xiaojun and Li, Hang and Varshney, Kush R and Bansal, Mohit and Koyejo, Sanmi and Liu, Yang},
  year = 2024,
  month = feb,
  journal = {arXiv [cs.LG]},
  abstract = {We explore machine unlearning (MU) in the domain of large language models (LLMs), referred to as LLM unlearning. This initiative aims to eliminate undesirable data influence (e.g., sensitive or illegal information) and the associated model capabilities, while maintaining the integrity of essential knowledge generation and not affecting causally unrelated information. We envision LLM unlearning becoming a pivotal element in the life-cycle management of LLMs, potentially standing as an essential foundation for developing generative AI that is not only safe, secure, and trustworthy, but also resource-efficient without the need of full retraining. We navigate the unlearning landscape in LLMs from conceptual formulation, methodologies, metrics, and applications. In particular, we highlight the often-overlooked aspects of existing LLM unlearning research, e.g., unlearning scope, data-model interaction, and multifaceted efficacy assessment. We also draw connections between LLM unlearning and related areas such as model editing, influence functions, model explanation, adversarial training, and reinforcement learning. Furthermore, we outline an effective assessment framework for LLM unlearning and explore its applications in copyright and privacy safeguards and sociotechnical harm reduction.},
  isbn = {2402.08787}
}

@unpublished{liuSaferLargeLanguage2024,
  title = {Towards {{Safer Large Language Models}} through {{Machine Unlearning}}},
  author = {Liu, Zheyuan and Dou, Guangyao and Tan, Zhaoxuan and Tian, Yijun and Jiang, Meng},
  year = 2024,
  month = feb,
  journal = {arXiv [cs.CL]},
  abstract = {The rapid advancement of Large Language Models (LLMs) has demonstrated their vast potential across various domains, attributed to their extensive pretraining knowledge and exceptional generalizability. However, LLMs often encounter challenges in generating harmful content when faced with problematic prompts. To address this problem, existing work attempted to implement a gradient ascent based approach to prevent LLMs from producing harmful output. While these methods can be effective, they frequently impact the model utility in responding to normal prompts. To address this gap, we introduce Selective Knowledge negation Unlearning (SKU), a novel unlearning framework for LLMs, designed to eliminate harmful knowledge while preserving utility on normal prompts. Specifically, SKU is consisted of two stages: harmful knowledge acquisition stage and knowledge negation stage. The first stage aims to identify and acquire harmful knowledge within the model, whereas the second is dedicated to remove this knowledge. SKU selectively isolates and removes harmful knowledge in model parameters, ensuring the model's performance remains robust on normal prompts. Our experiments conducted across various LLM architectures demonstrate that SKU identifies a good balance point between removing harmful information and preserving utility.},
  isbn = {2402.10058}
}

@unpublished{liuSophiaScalableStochastic2023,
  title = {Sophia: {{A Scalable Stochastic Second-order Optimizer}} for {{Language Model Pre-training}}},
  author = {Liu, Hong and Li, Zhiyuan and Hall, David and Liang, Percy and Ma, Tengyu},
  year = 2023,
  month = may,
  journal = {arXiv [cs.LG]},
  abstract = {Given the massive cost of language model pre-training, a non-trivial improvement of the optimization algorithm would lead to a material reduction on the time and cost of training. Adam and its variants have been state-of-the-art for years, and more sophisticated second-order (Hessian-based) optimizers often incur too much per-step overhead. In this paper, we propose Sophia, Second-order Clipped Stochastic Optimization, a simple scalable second-order optimizer that uses a light-weight estimate of the diagonal Hessian as the pre-conditioner. The update is the moving average of the gradients divided by the moving average of the estimated Hessian, followed by element-wise clipping. The clipping controls the worst-case update size and tames the negative impact of non-convexity and rapid change of Hessian along the trajectory. Sophia only estimates the diagonal Hessian every handful of iterations, which has negligible average per-step time and memory overhead. On language modeling with GPT models of sizes ranging from 125M to 1.5B, Sophia achieves a 2x speed-up compared to Adam in the number of steps, total compute, and wall-clock time, achieving the same perplexity with 50\% fewer steps, less total compute, and reduced wall-clock time. Theoretically, we show that Sophia, in a much simplified setting, adapts to the heterogeneous curvatures in different parameter dimensions, and thus has a run-time bound that does not depend on the condition number of the loss.},
  isbn = {2305.14342}
}

@book{liuWhoEarthUsing2024,
  title = {Who on {{Earth Is Using Generative AI}} ?},
  author = {Liu, Yan and Wang, He},
  year = 2024,
  month = aug,
  eprint = {10986/42071},
  eprinttype = {hdl},
  publisher = {Washington, DC: World Bank},
  doi = {10.1596/1813-9450-10870},
  urldate = {2024-11-28},
  langid = {english},
  file = {/Users/adam/Zotero/storage/D5VALGLQ/Liu and Wang - 2024 - Who on Earth Is Using Generative AI .pdf}
}

@article{liuYourCodeGenerated,
  title = {Is {{Your Code Generated}} by {{ChatGPT Really Correct}}?},
  author = {Liu, Jiawei and Xia, Chunqiu Steven and Wang, Yuyao and Zhang, Lingming},
  abstract = {Program synthesis has been long studied with recent approaches focused on directly using the power of Large Language Models (LLMs) to generate code. Programming benchmarks, with curated synthesis problems and test-cases, are used to measure the performance of various LLMs on code synthesis. However, these test-cases can be limited in both quantity and quality for fully assessing the functional correctness of the generated code. Such limitation in the existing benchmarks begs the following question: In the era of LLMs, is the code generated really correct? To answer this, we propose EvalPlus -- a code synthesis evaluation framework to rigorously benchmark the functional correctness of LLM-synthesized code. EvalPlus augments a given evaluation dataset with large amounts of test-cases newly produced by an automatic test input generator, powered by both LLM- and mutation-based strategies. While EvalPlus is general, we extend the test-cases of the popular HUMANEVAL benchmark by 80\texttimes{} to build HUMANEVAL+. Our extensive evaluation across 26 popular LLMs (e.g., GPT-4 and ChatGPT) demonstrates that HUMANEVAL+ is able to catch significant amounts of previously undetected wrong code synthesized by LLMs, reducing the pass@k by up-to 19.3-28.9\%. We also surprisingly found that test insufficiency can lead to mis-ranking. For example, both WizardCoder-CodeLlama and Phind-CodeLlama now outperform ChatGPT on HUMANEVAL+, while none of them could on HUMANEVAL. Our work not only indicates that prior popular code synthesis evaluation results do not accurately reflect the true performance of LLMs for code synthesis, but also opens up a new direction to improve such programming benchmarks through automated testing. We have open-sourced our tools, enhanced datasets as well as all LLM-generated code at https://github.com/evalplus/evalplus to facilitate and accelerate future LLM-for-code research.},
  langid = {english},
  file = {/Users/adam/Zotero/storage/9AWGCM4P/Liu et al. - Is Your Code Generated by ChatGPT Really Correct.pdf}
}

@misc{liuYourCodeGenerated2023,
  title = {Is {{Your Code Generated}} by {{ChatGPT Really Correct}}? {{Rigorous Evaluation}} of {{Large Language Models}} for {{Code Generation}}},
  shorttitle = {Is {{Your Code Generated}} by {{ChatGPT Really Correct}}?},
  author = {Liu, Jiawei and Xia, Chunqiu Steven and Wang, Yuyao and Zhang, Lingming},
  year = 2023,
  month = oct,
  number = {arXiv:2305.01210},
  eprint = {2305.01210},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.01210},
  urldate = {2024-11-12},
  abstract = {Program synthesis has been long studied with recent approaches focused on directly using the power of Large Language Models (LLMs) to generate code. Programming benchmarks, with curated synthesis problems and test-cases, are used to measure the performance of various LLMs on code synthesis. However, these test-cases can be limited in both quantity and quality for fully assessing the functional correctness of the generated code. Such limitation in the existing benchmarks begs the following question: In the era of LLMs, is the code generated really correct? To answer this, we propose EvalPlus -- a code synthesis evaluation framework to rigorously benchmark the functional correctness of LLM-synthesized code. EvalPlus augments a given evaluation dataset with large amounts of test-cases newly produced by an automatic test input generator, powered by both LLM- and mutation-based strategies. While EvalPlus is general, we extend the test-cases of the popular HumanEval benchmark by 80x to build HumanEval+. Our extensive evaluation across 26 popular LLMs (e.g., GPT-4 and ChatGPT) demonstrates that HumanEval+ is able to catch significant amounts of previously undetected wrong code synthesized by LLMs, reducing the pass@k by up-to 19.3-28.9\%. We also surprisingly found that test insufficiency can lead to mis-ranking. For example, both WizardCoder-CodeLlama and Phind-CodeLlama now outperform ChatGPT on HumanEval+, while none of them could on HumanEval. Our work not only indicates that prior popular code synthesis evaluation results do not accurately reflect the true performance of LLMs for code synthesis, but also opens up a new direction to improve such programming benchmarks through automated testing. We have open-sourced our tools, enhanced datasets as well as all LLM-generated code at https://github.com/evalplus/evalplus to facilitate and accelerate future LLM-for-code research.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Software Engineering},
  file = {/Users/adam/Zotero/storage/KFBR3QKD/Liu et al. - 2023 - Is Your Code Generated by ChatGPT Really Correct .pdf;/Users/adam/Zotero/storage/4E5V4VAM/2305.html}
}

@book{LiveLongProsper2015,
  title = {Live {{Long}} and {{Prosper}}: {{Aging}} in {{East Asia}} and {{Pacific}}},
  shorttitle = {Live {{Long}} and {{Prosper}}},
  author = {, World Bank},
  year = 2015,
  month = dec,
  series = {World {{Bank East Asia}} and {{Pacific Regional Report}}},
  publisher = {The World Bank},
  doi = {10.1596/978-1-4648-0469-4},
  urldate = {2023-04-08},
  isbn = {978-1-4648-0469-4},
  keywords = {AGING AND LONG-TERM CARE,AGING AND POVERTY,AGING AND THE MACRO ECONOMY,AGING POLICY,AGING SOCIETY,DEMOGRAPHIC IMPLICATIONS OF AGING,EPIDEMIOLOGICAL TRANSITION,FISCALLY SUSTAINABLE AGING,HEALTHY AGING,OLD AGE SUPPORT,PENSIONS}
}

@book{LiveLongProsper2015a,
  title = {Live {{Long}} and {{Prosper}}: {{Aging}} in {{East Asia}} and {{Pacific}}},
  year = 2015,
  month = dec,
  series = {World {{Bank East Asia}} and {{Pacific Regional Report}}},
  publisher = {The World Bank},
  doi = {10.1596/978-1-4648-0469-4},
  urldate = {2023-04-06},
  isbn = {978-1-4648-0469-4}
}

@unpublished{liWMDPBenchmarkMeasuring2024,
  title = {The {{WMDP Benchmark}}: {{Measuring}} and {{Reducing Malicious Use With Unlearning}}},
  author = {Li, Nathaniel and Pan, Alexander and Gopal, Anjali and Yue, Summer and Berrios, Daniel and Gatti, Alice and Li, Justin D and Dombrowski, Ann-Kathrin and Goel, Shashwat and Phan, Long and Mukobi, Gabriel and {Helm-Burger}, Nathan and Lababidi, Rassin and Justen, Lennart and Liu, Andrew B and Chen, Michael and Barrass, Isabelle and Zhang, Oliver and Zhu, Xiaoyuan and Tamirisa, Rishub and Bharathi, Bhrugu and Khoja, Adam and Zhao, Zhenqi and {Herbert-Voss}, Ariel and Breuer, Cort B and Marks, Samuel and Patel, Oam and Zou, Andy and Mazeika, Mantas and Wang, Zifan and Oswal, Palash and Lin, Weiran and Hunt, Adam A and {Tienken-Harder}, Justin and Shih, Kevin Y and Talley, Kemper and Guan, John and Kaplan, Russell and Steneker, Ian and Campbell, David and Jokubaitis, Brad and Levinson, Alex and Wang, Jean and Qian, William and Karmakar, Kallol Krishna and Basart, Steven and Fitz, Stephen and Levine, Mindy and Kumaraguru, Ponnurangam and Tupakula, Uday and Varadharajan, Vijay and Wang, Ruoyu and Shoshitaishvili, Yan and Ba, Jimmy and Esvelt, Kevin M and Wang, Alexandr and Hendrycks, Dan},
  year = 2024,
  month = mar,
  journal = {arXiv [cs.LG]},
  abstract = {The White House Executive Order on Artificial Intelligence highlights the risks of large language models (LLMs) empowering malicious actors in developing biological, cyber, and chemical weapons. To measure these risks of malicious use, government institutions and major AI labs are developing evaluations for hazardous capabilities in LLMs. However, current evaluations are private, preventing further research into mitigating risk. Furthermore, they focus on only a few, highly specific pathways for malicious use. To fill these gaps, we publicly release the Weapons of Mass Destruction Proxy (WMDP) benchmark, a dataset of 3,668 multiple-choice questions that serve as a proxy measurement of hazardous knowledge in biosecurity, cybersecurity, and chemical security. WMDP was developed by a consortium of academics and technical consultants, and was stringently filtered to eliminate sensitive information prior to public release. WMDP serves two roles: first, as an evaluation for hazardous knowledge in LLMs, and second, as a benchmark for unlearning methods to remove such hazardous knowledge. To guide progress on unlearning, we develop RMU, a state-of-the-art unlearning method based on controlling model representations. RMU reduces model performance on WMDP while maintaining general capabilities in areas such as biology and computer science, suggesting that unlearning may be a concrete path towards reducing malicious use from LLMs. We release our benchmark and code publicly at https://wmdp.ai},
  isbn = {2403.03218},
  file = {/Users/adam/Zotero/storage/VRXN9BHZ/Li et al. - 2024 - The WMDP Benchmark Measuring and Reducing Malicio.pdf}
}

@misc{liWMDPBenchmarkMeasuring2024a,
  title = {The {{WMDP Benchmark}}: {{Measuring}} and {{Reducing Malicious Use With Unlearning}}},
  shorttitle = {The {{WMDP Benchmark}}},
  author = {Li, Nathaniel and Pan, Alexander and Gopal, Anjali and Yue, Summer and Berrios, Daniel and Gatti, Alice and Li, Justin D. and Dombrowski, Ann-Kathrin and Goel, Shashwat and Phan, Long and Mukobi, Gabriel and {Helm-Burger}, Nathan and Lababidi, Rassin and Justen, Lennart and Liu, Andrew B. and Chen, Michael and Barrass, Isabelle and Zhang, Oliver and Zhu, Xiaoyuan and Tamirisa, Rishub and Bharathi, Bhrugu and Khoja, Adam and Zhao, Zhenqi and {Herbert-Voss}, Ariel and Breuer, Cort B. and Marks, Samuel and Patel, Oam and Zou, Andy and Mazeika, Mantas and Wang, Zifan and Oswal, Palash and Lin, Weiran and Hunt, Adam A. and {Tienken-Harder}, Justin and Shih, Kevin Y. and Talley, Kemper and Guan, John and Kaplan, Russell and Steneker, Ian and Campbell, David and Jokubaitis, Brad and Levinson, Alex and Wang, Jean and Qian, William and Karmakar, Kallol Krishna and Basart, Steven and Fitz, Stephen and Levine, Mindy and Kumaraguru, Ponnurangam and Tupakula, Uday and Varadharajan, Vijay and Wang, Ruoyu and Shoshitaishvili, Yan and Ba, Jimmy and Esvelt, Kevin M. and Wang, Alexandr and Hendrycks, Dan},
  year = 2024,
  month = may,
  number = {arXiv:2403.03218},
  eprint = {2403.03218},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2403.03218},
  urldate = {2024-12-07},
  abstract = {The White House Executive Order on Artificial Intelligence highlights the risks of large language models (LLMs) empowering malicious actors in developing biological, cyber, and chemical weapons. To measure these risks of malicious use, government institutions and major AI labs are developing evaluations for hazardous capabilities in LLMs. However, current evaluations are private, preventing further research into mitigating risk. Furthermore, they focus on only a few, highly specific pathways for malicious use. To fill these gaps, we publicly release the Weapons of Mass Destruction Proxy (WMDP) benchmark, a dataset of 3,668 multiple-choice questions that serve as a proxy measurement of hazardous knowledge in biosecurity, cybersecurity, and chemical security. WMDP was developed by a consortium of academics and technical consultants, and was stringently filtered to eliminate sensitive information prior to public release. WMDP serves two roles: first, as an evaluation for hazardous knowledge in LLMs, and second, as a benchmark for unlearning methods to remove such hazardous knowledge. To guide progress on unlearning, we develop RMU, a state-of-the-art unlearning method based on controlling model representations. RMU reduces model performance on WMDP while maintaining general capabilities in areas such as biology and computer science, suggesting that unlearning may be a concrete path towards reducing malicious use from LLMs. We release our benchmark and code publicly at https://wmdp.ai},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computers and Society,Computer Science - Machine Learning},
  file = {/Users/adam/Zotero/storage/BPK6ECUD/Li et al. - 2024 - The WMDP Benchmark Measuring and Reducing Malicio.pdf;/Users/adam/Zotero/storage/YK4RZ8DI/2403.html}
}

@unpublished{luEraserJailbreakingDefense2024,
  title = {Eraser: {{Jailbreaking Defense}} in {{Large Language Models}} via {{Unlearning Harmful Knowledge}}},
  author = {Lu, Weikai and Zeng, Ziqian and Wang, Jianwei and Lu, Zhengdong and Chen, Zelin and Zhuang, Huiping and Chen, Cen},
  year = 2024,
  month = apr,
  journal = {arXiv [cs.CL]},
  abstract = {Jailbreaking attacks can enable Large Language Models (LLMs) to bypass the safeguard and generate harmful content. Existing jailbreaking defense methods have failed to address the fundamental issue that harmful knowledge resides within the model, leading to potential jailbreak risks for LLMs. In this paper, we propose a novel defense method called Eraser, which mainly includes three goals: unlearning harmful knowledge, retaining general knowledge, and maintaining safety alignment. The intuition is that if an LLM forgets the specific knowledge required to answer a harmful question, it will no longer have the ability to answer harmful questions. The training of Erase does not actually require the model's own harmful knowledge, and it can benefit from unlearning general answers related to harmful queries, which means it does not need assistance from the red team. The experimental results show that Eraser can significantly reduce the jailbreaking success rate for various attacks without compromising the general capabilities of the model. Our codes are available at https://github.com/ZeroNLP/Eraser.},
  isbn = {2404.05880}
}

@misc{luLLMDiscussionEnhancing2024,
  title = {{{LLM Discussion}}: {{Enhancing}} the {{Creativity}} of {{Large Language Models}} via {{Discussion Framework}} and {{Role-Play}}},
  shorttitle = {{{LLM Discussion}}},
  author = {Lu, Li-Chun and Chen, Shou-Jen and Pai, Tsung-Min and Yu, Chan-Hung and Lee, Hung-yi and Sun, Shao-Hua},
  year = 2024,
  month = may,
  number = {arXiv:2405.06373},
  eprint = {2405.06373},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-07-20},
  abstract = {Large language models (LLMs) have shown exceptional proficiency in natural language processing but often fall short of generating creative and original responses to open-ended questions. To enhance LLM creativity, our key insight is to emulate the human process of inducing collective creativity through engaging discussions with participants from diverse backgrounds and perspectives. To this end, we propose LLM Discussion, a three-phase discussion framework that facilitates vigorous and diverging idea exchanges and ensures convergence to creative answers. Moreover, we adopt a role-playing technique by assigning distinct roles to LLMs to combat the homogeneity of LLMs. We evaluate the efficacy of the proposed framework with the Alternative Uses Test, Similarities Test, Instances Test, and Scientific Creativity Test through both LLM evaluation and human study. Our proposed framework outperforms single-LLM approaches and existing multi-LLM frameworks across various creativity metrics.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/adam/Zotero/storage/BGMUYV7W/Lu et al. - 2024 - LLM Discussion Enhancing the Creativity of Large .pdf}
}

@misc{luoAgentLightningTrain2025,
  title = {Agent {{Lightning}}: {{Train ANY AI Agents}} with {{Reinforcement Learning}}},
  shorttitle = {Agent {{Lightning}}},
  author = {Luo, Xufang and Zhang, Yuge and He, Zhiyuan and Wang, Zilong and Zhao, Siyun and Li, Dongsheng and Qiu, Luna K. and Yang, Yuqing},
  year = 2025,
  month = aug,
  journal = {arXiv.org},
  urldate = {2026-02-21},
  abstract = {We present Agent Lightning, a flexible and extensible framework that enables Reinforcement Learning (RL)-based training of Large Language Models (LLMs) for any AI agent. Unlike existing methods that tightly couple RL training with agent or rely on sequence concatenation with masking, Agent Lightning achieves complete decoupling between agent execution and training, allowing seamless integration with existing agents developed via diverse ways (e.g., using frameworks like LangChain, OpenAI Agents SDK, AutoGen, and building from scratch) with almost ZERO code modifications. By formulating agent execution as Markov decision process, we define an unified data interface and propose a hierarchical RL algorithm, LightningRL, which contains a credit assignment module, allowing us to decompose trajectories generated by ANY agents into training transition. This enables RL to handle complex interaction logic, such as multi-agent scenarios and dynamic workflows. For the system design, we introduce a Training-Agent Disaggregation architecture, and brings agent observability frameworks into agent runtime, providing a standardized agent finetuning interface. Experiments across text-to-SQL, retrieval-augmented generation, and math tool-use tasks demonstrate stable, continuous improvements, showcasing the framework's potential for real-world agent training and deployment.},
  howpublished = {https://arxiv.org/abs/2508.03680v1},
  langid = {english},
  file = {/Users/adam/Zotero/storage/2MQKH9DT/Luo et al. - 2025 - Agent Lightning Train ANY AI Agents with Reinforcement Learning.pdf}
}

@article{luWorldBetterProtected2022,
  title = {World Better Protected against {{Covid}} If Rich Nations Donated 50\% of Vaccines to Low-Income Countries},
  author = {Lu, Donna},
  year = 2022,
  month = jan,
  journal = {The Guardian},
  issn = {0261-3077},
  urldate = {2024-11-08},
  abstract = {Vaccine inequity will prolong pandemic and increase risk of new variants developing, modelling finds},
  chapter = {Australia news},
  langid = {british},
  keywords = {Australia news,Coronavirus,Health,Omicron variant},
  file = {/Users/adam/Zotero/storage/HSFAJ2IE/world-better-protected-against-covid-if-rich-nations-donated-50-of-vaccines-to-low-income-count.html}
}

@unpublished{lynchEightMethodsEvaluate2024,
  title = {Eight {{Methods}} to {{Evaluate Robust Unlearning}} in {{LLMs}}},
  author = {Lynch, Aengus and Guo, Phillip and Ewart, Aidan and Casper, Stephen and {Hadfield-Menell}, Dylan},
  year = 2024,
  month = feb,
  journal = {arXiv [cs.CL]},
  abstract = {Machine unlearning can be useful for removing harmful capabilities and memorized text from large language models (LLMs), but there are not yet standardized methods for rigorously evaluating it. In this paper, we first survey techniques and limitations of existing unlearning evaluations. Second, we apply a comprehensive set of tests for the robustness and competitiveness of unlearning in the "Who's Harry Potter" (WHP) model from Eldan and Russinovich (2023). While WHP's unlearning generalizes well when evaluated with the "Familiarity" metric from Eldan and Russinovich, we find i) higher-than-baseline amounts of knowledge can reliably be extracted, ii) WHP performs on par with the original model on Harry Potter Q\&A tasks, iii) it represents latent knowledge comparably to the original model, and iv) there is collateral unlearning in related domains. Overall, our results highlight the importance of comprehensive unlearning evaluation that avoids ad-hoc metrics.},
  isbn = {2402.16835}
}

@misc{MachineUnlearning2024,
  title = {Machine {{Unlearning}} in 2024},
  journal = {Ken Ziyu Liu - Stanford Computer Science},
  urldate = {2024-11-26},
  howpublished = {https://ai.stanford.edu/\textasciitilde kzliu/blog/unlearning},
  langid = {english}
}

@misc{MachineUnlearningLLMs2021,
  title = {Machine Unlearning for {{LLMs}}},
  year = 2021,
  month = feb,
  journal = {IBM Research},
  urldate = {2024-11-26},
  abstract = {LLM unlearning is focused on removing the influence of unwanted data without degrading its performance.},
  copyright = {\copyright{} Copyright IBM Corp. 2021},
  howpublished = {https://research.ibm.com/blog/llm-unlearning},
  langid = {american}
}

@misc{MachineUnlearningLLMs2021a,
  title = {Machine Unlearning for {{LLMs}}},
  year = 2021,
  month = feb,
  journal = {IBM Research},
  urldate = {2024-11-26},
  abstract = {LLM unlearning is focused on removing the influence of unwanted data without degrading its performance.},
  copyright = {\copyright{} Copyright IBM Corp. 2021},
  howpublished = {https://research.ibm.com/blog/llm-unlearning},
  langid = {american},
  file = {/Users/adam/Zotero/storage/YFI3CP9S/llm-unlearning.html}
}

@unpublished{mainiTOFUTaskFictitious2024,
  title = {{{TOFU}}: {{A Task}} of {{Fictitious Unlearning}} for {{LLMs}}},
  author = {Maini, Pratyush and Feng, Zhili and Schwarzschild, Avi and Lipton, Zachary C and Zico Kolter, J},
  year = 2024,
  month = jan,
  journal = {arXiv [cs.LG]},
  abstract = {Large language models trained on massive corpora of data from the web can memorize and reproduce sensitive or private data raising both legal and ethical concerns. Unlearning, or tuning models to forget information present in their training data, provides us with a way to protect private data after training. Although several methods exist for such unlearning, it is unclear to what extent they result in models equivalent to those where the data to be forgotten was never learned in the first place. To address this challenge, we present TOFU, a Task of Fictitious Unlearning, as a benchmark aimed at helping deepen our understanding of unlearning. We offer a dataset of 200 diverse synthetic author profiles, each consisting of 20 question-answer pairs, and a subset of these profiles called the forget set that serves as the target for unlearning. We compile a suite of metrics that work together to provide a holistic picture of unlearning efficacy. Finally, we provide a set of baseline results from existing unlearning algorithms. Importantly, none of the baselines we consider show effective unlearning motivating continued efforts to develop approaches for unlearning that effectively tune models so that they truly behave as if they were never trained on the forget data at all.},
  isbn = {2401.06121}
}

@misc{mairwenguardmbeScentMarkingChinning2013,
  title = {Scent Marking or Chinning in Rabbits},
  author = {{Mairwen Guard MBE}},
  year = 2013,
  month = oct,
  urldate = {2024-07-18},
  abstract = {Angus demonstrates how a rabbit scent marks objects around him to show ownership and establish territory.  Rabbits have scent glands under their chins, so by rubbing their chins on objects he is effectively saying "this is mine and I live here".  It is a bit similar to a cat rubbing itself against your leg!}
}

@unpublished{maRethinkingEntitylevelUnlearning2024,
  title = {Rethinking {{Entity-level Unlearning}} for {{Large Language Models}}},
  author = {Ma, Weitao and Feng, Xiaocheng and Zhong, Weihong and Huang, Lei and Ye, Yangfan and Qin, Bing},
  year = 2024,
  month = jun,
  journal = {arXiv [cs.CL]},
  abstract = {Large language model unlearning has gained increasing attention due to its potential to mitigate security and privacy concerns. Current research predominantly focuses on Instance-level unlearning, specifically aiming at forgetting predefined instances of sensitive content. However, a notable gap still exists in exploring the deletion of complete entity-related information, which is crucial in many real-world scenarios, such as copyright protection. To this end, we propose a novel task of Entity-level unlearning, where the entity-related knowledge within the target model is supposed to be entirely erased. Given the challenge of practically accessing all entity-related knowledge within a model, we begin by simulating entity-level unlearning scenarios through fine-tuning models to introduce pseudo entities. Following this, we develop baseline methods inspired by trending unlearning techniques and conduct a detailed comparison of their effectiveness in this task. Extensive experiments reveal that current unlearning algorithms struggle to achieve effective entity-level unlearning. Additionally, our analyses further indicate that entity-related knowledge injected through fine-tuning is more susceptible than original entities from pre-training during unlearning, highlighting the necessity for more thorough pseudo-entity injection methods to make them closer to pre-trained knowledge.},
  isbn = {2406.15796}
}

@misc{matanleAgeingDepopulationJapan2014,
  type = {{{SSRN Scholarly Paper}}},
  title = {Ageing and {{Depopulation}} in {{Japan}}: {{Understanding}} the {{Consequences}} for {{East}} and {{Southeast Asia}} in the 21st {{Century}}},
  shorttitle = {Ageing and {{Depopulation}} in {{Japan}}},
  author = {Matanle, Peter},
  year = 2014,
  number = {2406498},
  address = {Rochester, NY},
  urldate = {2023-04-08},
  abstract = {Japan is one of the most rapidly ageing and depopulating countries in the world. Government projections indicate that Japan may shrink 32 per cent from the high of 128 million in 2008 to approximately 87 million by 2060, due to a sustained fall in rates of human re-production. Whereas in 1947 each woman expected to give birth in her lifetime to 4.54 children, this had dropped below the population replacement rate of 2.1 children per woman by 1974, and remained at below replacement to stand at 1.39 children in 2010.},
  langid = {english},
  keywords = {depopulation,East Asia,Japan,rural decline},
  file = {/Users/adam/Zotero/storage/28LRF3MB/Matanle - 2014 - Ageing and Depopulation in Japan Understanding th.pdf}
}

@article{matanleComingSoonCity2010,
  title = {Coming {{Soon}} to a {{City Near You}}! {{Learning}} to {{Live}} '{{Beyond Growth}}' {{In Japan}}'s {{Shrinking Regions}}},
  author = {Matanle, Peter and Sato, Yasuyuki},
  year = 2010,
  month = dec,
  journal = {Social Science Japan Journal},
  volume = {13},
  pages = {197--210},
  doi = {10.1093/ssjj/jyq013},
  abstract = {This article analyses rural depopulation in Japan and its implications by means of a case study of Niigata Prefecture and Sado Island. In the first part of the article we present population maps to show that rural demographic shrinkage is both deepening as well as broadening to include urban centres. We focus initially on Niigata Prefecture in the national context and then discuss migratory patterns in Sado. The data show that Sado, and now Niigata Prefecture as a whole, have entered what we call a 'double negative population disequilibrium', whereby both the migratory and natural reproduction population contributions have turned negative. Recent evidence also indicates that Niigata City itself may also have begun to shrink. In the second part we discuss the implications of depopulation for Sado Island via extracts from qualitative interviews gathered from local residents. We found that many residents now accept the inevitability of continued shrinkage and, rather than seeking to re-establish growth, many institutional and social and environmental entrepreneurs are instead working towards achieving community stability and sustainability. We conclude by suggesting that the example of Japan's rural communities presents Japan's regional cities with the occasion to consider life 'beyond growth', as their populations also begin to shrink in the years to come.},
  file = {/Users/adam/Zotero/storage/AQVE5ZVP/Matanle and Sato - 2010 - Coming Soon to a City Near You! Learning to Live '.pdf}
}

@article{matanleGreatEastJapan2011,
  title = {The {{Great East Japan Earthquake}}, Tsunami, and Nuclear Meltdown: Towards the (Re)Construction of a Safe, Sustainable, and Compassionate Society in {{Japan}}'s Shrinking Regions},
  shorttitle = {The {{Great East Japan Earthquake}}, Tsunami, and Nuclear Meltdown},
  author = {Matanle, Peter},
  year = 2011,
  month = oct,
  journal = {Local Environment},
  volume = {16},
  number = {9},
  pages = {823--847},
  publisher = {Routledge},
  issn = {1354-9839},
  doi = {10.1080/13549839.2011.607160},
  urldate = {2023-04-08},
  abstract = {Japan's rural regions have been shrinking for the entire post-war period, and successive efforts to revitalise rural society have failed. This article examines whether the Great East Japan Earthquake and tsunami, and the subsequent meltdown at the Fukushima Daiichi nuclear power plant, present the Japanese state and society with a watershed opportunity to rethink regional revitalisation and national energy procurement strategies. The article begins by summarising the events of March and April 2011, examines possible approaches to the reconstruction of communities in the T\=ohoku region, and critiques problems of governance in post-war Japan that the disaster reveals. It concludes by pulling together the information and analysis presented into a discussion of the prospects for achieving the three-point vision for a safe, sustainable, and compassionate society that Prime Minister Naoto Kan set the Reconstruction Design Council.},
  keywords = {disaster reconstruction,Japan's shrinking regions,rural revitalisation,sustainability,Tohoku Earthquake},
  file = {/Users/adam/Zotero/storage/5YA2XQ2F/Matanle - 2011 - The Great East Japan Earthquake, tsunami, and nucl.pdf}
}

@misc{mccanndogtrainingHowStopYour2018,
  title = {How {{To Stop Your Dog From Barking In Their Crate At Night}}},
  author = {{McCann Dog Training}},
  year = 2018,
  month = sep,
  urldate = {2024-07-17},
  abstract = {It can be really frustrating and troublesome when your dog is barking in their crate at night. In this video, we have a few tips to help you work through the task of teaching your dog to be quiet in their crate at night. So you can get some sleep, and so can your dog! With these tips, your dog will quickly learn to be quiet in their crate the whole night through! BRAND NEW! We Now Have A FULLY SUPPORTED Puppy Essentials Training Program Online! Train With Us To Make Your Puppy Training More Enjoyable And To Give Your Puppy The Best Start Possible:  https://www.McCannDogs.Link/PuppyEsse... Download the crate training checklist: https://mccanndogs.link/crate Are you looking for a personalized training plan for YOUR dog? We now have a HomeSchool program, that's fully supported with a McCann Dogs trainer for YOU. Check out: https://mccanndogs.link/HomeSchool We Currently Have Some Of Our Signature Training Equipment Available Online! (Limited Quantities) To Get Yours, Visit: https://www.McCannDogs.store Watch our latest video HERE: https://goo.gl/UNriTZ Here Is A Link To Our Video Podcast Channel: ~~~/~@mccanndogspodcast~~ Let's train together! Train with us online: https://mydogcan.mccanndogs.com/cours... Check Out Our Podcast! https://anchor.fm/mccann-dogs Visit Our Amazon Store And See The Dog Products We Love: https://www.amazon.com/shop/mccanndogs Are you a brand looking to showcase your dog-related product or service?  Email me HERE: ken@mccanndogs.com Thanks for watching, Happy Training! \textasciitilde Ken \#puppycratetraining \#cratetraining}
}

@article{mclemanMigrationContextVulnerability2010,
  title = {Migration in the Context of Vulnerability and Adaptation to Climate Change: Insights from Analogues},
  shorttitle = {Migration in the Context of Vulnerability and Adaptation to Climate Change},
  author = {McLeman, Robert A. and Hunter, Lori M.},
  year = 2010,
  journal = {Wiley interdisciplinary reviews. Climate change},
  volume = {1},
  number = {3},
  pages = {450--461},
  issn = {1757-7780},
  doi = {10.1002/wcc.51},
  urldate = {2023-04-08},
  abstract = {Migration is one of the variety of ways by which human populations adapt to environmental changes. The study of migration in the context of anthropogenic climate change is often approached using the concept of vulnerability and its key functional elements: exposure, system sensitivity, and adaptive capacity. This article explores the interaction of climate change and vulnerability through review of case studies of dry-season migration in the West African Sahel, hurricane-related population displacements in the Caribbean basin, winter migration of `snowbirds' to the US Sun-belt, and 1930s drought migration on the North American Great Plains. These examples are then used as analogues for identifying general causal, temporal, and spatial dimensions of climate migration, along with potential considerations for policy-making and future research needs.},
  pmcid = {PMC3183747},
  pmid = {22022342},
  file = {/Users/adam/Zotero/storage/K74VPD6G/McLeman and Hunter - 2010 - Migration in the context of vulnerability and adap.pdf}
}

@misc{MiniProjectClass,
  title = {Mini-{{Project}} 3 {{In-Class}}},
  journal = {Google Docs},
  urldate = {2023-04-19},
  abstract = {Team 4 Appendix  Shared Characteristics Key     Shared with Australia Shared with  Brazil Shared with Italy Shared with Qatar Shared with all countries * If bullet points are highlighted partially in multiple colors it means that it has that in common with more than one country. ...},
  howpublished = {https://docs.google.com/document/u/1/d/1fa9osP7yQecG5b2DgfEj08pKbExCvFK1O5q3tIPCZwQ/edit?usp=drive\_web\&ouid=103030888654436358758\&usp=embed\_facebook},
  langid = {english},
  file = {/Users/adam/Zotero/storage/QXEV72NR/edit.html}
}

@unpublished{minSILOLanguageModels2023,
  title = {{{SILO Language Models}}: {{Isolating Legal Risk In}} a {{Nonparametric Datastore}}},
  author = {Min, Sewon and Gururangan, Suchin and Wallace, Eric and Hajishirzi, Hannaneh and Smith, Noah A and Zettlemoyer, Luke},
  year = 2023,
  month = aug,
  journal = {arXiv [cs.CL]},
  abstract = {The legality of training language models (LMs) on copyrighted or otherwise restricted data is under intense debate. However, as we show, model performance significantly degrades if trained only on low-risk text (e.g., out-of-copyright books or government documents), due to its limited size and domain coverage. We present SILO, a new language model that manages this risk-performance tradeoff during inference. SILO is built by (1) training a parametric LM on Open License Corpus (OLC), a new corpus we curate with 228B tokens of public domain and permissively licensed text and (2) augmenting it with a more general and easily modifiable nonparametric datastore (e.g., containing copyrighted books or news) that is only queried during inference. The datastore allows use of high-risk data without training on it, supports sentence-level data attribution, and enables data producers to opt out from the model by removing content from the store. These capabilities can foster compliance with data-use regulations such as the fair use doctrine in the United States and the GDPR in the European Union. Our experiments show that the parametric LM struggles on domains not covered by OLC. However, access to the datastore greatly improves out of domain performance, closing 90\% of the performance gap with an LM trained on the Pile, a more diverse corpus with mostly high-risk text. We also analyze which nonparametric approach works best, where the remaining errors lie, and how performance scales with datastore size. Our results suggest that it is possible to build high quality language models while mitigating their legal risk.},
  isbn = {2308.04430}
}

@article{miyamotoCITIZENSVIEWDEPOPULATION,
  title = {{{CITIZENS}}' {{VIEW ON DEPOPULATION AND ITS LOCAL GOVERNMENT POLICY OF RURAL AREA IN JAPAN}}},
  author = {Miyamoto, Michiko and Akasaka, Yuta},
  volume = {4},
  number = {7},
  abstract = {Depopulation is a worldwide phenomenon. Japan has been identified as one of the super aging societies and one of the largest low-fertility countries. Japan\quotedblbase s population decline is eminent, and that the shrinking population is concentrated more than ever in Tokyo and its environs. Akita is one of prefectures in Japan facing serious depopulation, and its local government has attempted to return to a growth strategy. This study examines how Akita citizens view the local government policies, such as 1) Industry and Energy, 2) Agriculture, Forestry and Fisheries, 3) Tourism 4) Transportation, 5) Health, Medical, Welfare, 6) Education and their relationships with 7) Depopulationby using data from ``Citizens Awareness Survey`` conducted by Akita local government in 2015. Results show that relationships of all policies and depopulation are statistically positive and significant; however, standardized path coefficients between depopulation and transportation or industry/energy indicate ``small'' effects on depopulation.},
  langid = {english},
  file = {/Users/adam/Zotero/storage/NHLHCXIW/Miyamoto and Akasaka - CITIZENS VIEW ON DEPOPULATION AND ITS LOCAL GOVER.pdf}
}

@article{miyamotoCITIZENSVIEWDEPOPULATIONa,
  title = {{{CITIZENS}}' {{VIEW ON DEPOPULATION AND ITS LOCAL GOVERNMENT POLICY OF RURAL AREA IN JAPAN}}},
  author = {Miyamoto, Michiko and Akasaka, Yuta},
  volume = {4},
  number = {7},
  abstract = {Depopulation is a worldwide phenomenon. Japan has been identified as one of the super aging societies and one of the largest low-fertility countries. Japan\quotedblbase s population decline is eminent, and that the shrinking population is concentrated more than ever in Tokyo and its environs. Akita is one of prefectures in Japan facing serious depopulation, and its local government has attempted to return to a growth strategy. This study examines how Akita citizens view the local government policies, such as 1) Industry and Energy, 2) Agriculture, Forestry and Fisheries, 3) Tourism 4) Transportation, 5) Health, Medical, Welfare, 6) Education and their relationships with 7) Depopulationby using data from ``Citizens Awareness Survey`` conducted by Akita local government in 2015. Results show that relationships of all policies and depopulation are statistically positive and significant; however, standardized path coefficients between depopulation and transportation or industry/energy indicate ``small'' effects on depopulation.},
  langid = {english},
  file = {/Users/adam/Zotero/storage/BRVJQ6XS/Miyamoto and Akasaka - CITIZENS VIEW ON DEPOPULATION AND ITS LOCAL GOVER.pdf}
}

@misc{MolecularWeightChemistry,
  title = {Molecular Weight \textbar{} Chemistry \textbar{} {{Britannica}}},
  urldate = {2022-07-26},
  abstract = {molecular weight, also called molecular mass, mass of a molecule of a substance, based on 12 as the atomic weight of carbon-12. It is calculated in practice by summing the atomic weights of the atoms making up the substance's molecular formula. The molecular weight of a hydrogen molecule (chemical formula H2) is 2 (after rounding off); for many complex organic molecules (e.g., proteins, polymers) it may be in the millions.},
  howpublished = {https://www.britannica.com/science/molecular-weight},
  langid = {english},
  file = {/Users/adam/Zotero/storage/ZUEIFHT3/molecular-weight.html}
}

@misc{MolecularWeightInfoplease,
  title = {Molecular Weight \textbar{} {{Infoplease}}},
  urldate = {2022-07-26},
  abstract = {weight of a molecule of a substance expressed in atomic mass units (amu). The molecular weight may be calculated from the molecular formula of the substance; it is the sum of the atomic weights of the atoms making up the molecule. For example, water has},
  howpublished = {https://www.infoplease.com/encyclopedia/science/chemistry/concepts/molecular-weight},
  langid = {english},
  file = {/Users/adam/Zotero/storage/IDP3EHE6/molecular-weight.html}
}

@misc{muhleisenFinanceDevelopment,
  title = {Finance and {{Development}}},
  author = {Muhleisen, Martin and Hamid, Faruqee},
  journal = {Finance and Development \textbar{} F\&D},
  urldate = {2023-04-08},
  abstract = {By Martin Muhleisen and Hamid Faruqee - With Japan facing a demographic crisis, government finances--stretched to the limit to keep the economy afloat--have to cope with the rising strain on public pension and health systems. This article looks at the economic and fiscal costs of aging in Japan.},
  howpublished = {https://www.imf.org/external/pubs/ft/fandd/2001/03/muhleise.htm},
  langid = {english},
  file = {/Users/adam/Zotero/storage/Y6IU6A4U/muhleise.html}
}

@article{murayamaSekenteiSocioCulturalDeterminant2020,
  title = {Sekentei as a {{Socio-Cultural Determinant}} of {{Cognitive Function}} among {{Older Japanese People}}: {{Findings}} from the {{NEIGE Study}}},
  shorttitle = {Sekentei as a {{Socio-Cultural Determinant}} of {{Cognitive Function}} among {{Older Japanese People}}},
  author = {Murayama, Hiroshi and Inoue, Shigeru and Fujiwara, Takeo and Fukui, Naoki and Yokoyama, Yuichi and Shobugawa, Yugo},
  year = 2020,
  month = jun,
  journal = {International Journal of Environmental Research and Public Health},
  volume = {17},
  number = {12},
  pages = {4480},
  issn = {1661-7827},
  doi = {10.3390/ijerph17124480},
  urldate = {2023-04-08},
  abstract = {Sekentei (social appearance) is a Japanese concept that describes a person's sense of implicit societal pressure to conform to social norms. However, evidence of a relationship between sekentei and health outcomes is sparse. This study examined the association between sekentei and cognitive function among community-dwelling older Japanese people. Baseline data were obtained from the Neuron to Environmental Impact across Generations (NEIGE) study conducted in 2017; 526 randomly sampled community-dwelling individuals aged 65--84 years living in Tokamachi, Niigata Prefecture, Japan were analyzed. The 12-item Sekentei Scale was used to assess sekentei. Cognitive function levels were evaluated with the Japanese version of Mini-Mental State Examination (MMSE-J; ranging from 0--30). Approximately 10\% and 25\% had cognitive decline and mild cognitive impairment, respectively (MMSE-J scores of {$\leq$}23 and 24--26, respectively). Multinomial logistic regression analysis showed that both high and low levels of sekentei were associated with lower cognitive function, particularly mild cognitive impairment, after adjusting for sociodemographic factors, health behaviors, health conditions, and genetic factors. The current findings suggest that a moderate level of sekentei consciousness is beneficial for cognitive health, and that sekentei could be an important socio-cultural factor affecting cognitive function.},
  pmcid = {PMC7345683},
  pmid = {32580416},
  file = {/Users/adam/Zotero/storage/JXAXZZLV/Murayama et al. - 2020 - Sekentei as a Socio-Cultural Determinant of Cognit.pdf}
}

@unpublished{muresanuUnlearnableAlgorithmsContext2024,
  title = {Unlearnable {{Algorithms}} for {{In-context Learning}}},
  author = {Muresanu, Andrei and Thudi, Anvith and Zhang, Michael R and Papernot, Nicolas},
  year = 2024,
  month = feb,
  journal = {arXiv [cs.LG]},
  abstract = {Machine unlearning is a desirable operation as models get increasingly deployed on data with unknown provenance. However, achieving exact unlearning -- obtaining a model that matches the model distribution when the data to be forgotten was never used -- is challenging or inefficient, often requiring significant retraining. In this paper, we focus on efficient unlearning methods for the task adaptation phase of a pretrained large language model (LLM). We observe that an LLM's ability to do in-context learning for task adaptation allows for efficient exact unlearning of task adaptation training data. We provide an algorithm for selecting few-shot training examples to prepend to the prompt given to an LLM (for task adaptation), ERASE, whose unlearning operation cost is independent of model and dataset size, meaning it scales to large models and datasets. We additionally compare our approach to fine-tuning approaches and discuss the trade-offs between the two approaches. This leads us to propose a new holistic measure of unlearning cost which accounts for varying inference costs, and conclude that in-context learning can often be more favourable than fine-tuning for deployments involving unlearning requests.},
  isbn = {2402.00751}
}

@misc{nairDiscover10Facts2019,
  title = {Discover 10 {{Facts About Education}} in {{Brazil}}},
  author = {{nair}, madhu},
  year = 2019,
  month = may,
  journal = {University of the People},
  urldate = {2023-04-19},
  abstract = {From the high dropout rates, and high literacy, to large class sizes, discover what makes Brazil's education system.},
  howpublished = {https://www.uopeople.edu/blog/10-facts-about-education-in-brazil/},
  langid = {american},
  file = {/Users/adam/Zotero/storage/I2SDBYM6/10-facts-about-education-in-brazil.html}
}

@article{naohiroUrbanruralDifferentialsHealth2004,
  title = {Urban-rural Differentials in Health Conditions and Labor Force Participation among the {{Japanese}} Elderly},
  author = {Naohiro, Ogawa},
  year = 2004,
  month = sep,
  journal = {Geriatrics \& Gerontology International},
  volume = {4},
  pages = {S60-S62},
  doi = {10.1111/j.1447-0594.2004.00150.x}
}

@misc{nasrScalableExtractionTraining2023,
  title = {Scalable {{Extraction}} of {{Training Data}} from ({{Production}}) {{Language Models}}},
  author = {Nasr, Milad and Carlini, Nicholas and Hayase, Jonathan and Jagielski, Matthew and Cooper, A. Feder and Ippolito, Daphne and {Choquette-Choo}, Christopher A. and Wallace, Eric and Tram{\`e}r, Florian and Lee, Katherine},
  year = 2023,
  month = nov,
  number = {arXiv:2311.17035},
  eprint = {2311.17035},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2311.17035},
  urldate = {2024-11-30},
  abstract = {This paper studies extractable memorization: training data that an adversary can efficiently extract by querying a machine learning model without prior knowledge of the training dataset. We show an adversary can extract gigabytes of training data from open-source language models like Pythia or GPT-Neo, semi-open models like LLaMA or Falcon, and closed models like ChatGPT. Existing techniques from the literature suffice to attack unaligned models; in order to attack the aligned ChatGPT, we develop a new divergence attack that causes the model to diverge from its chatbot-style generations and emit training data at a rate 150x higher than when behaving properly. Our methods show practical attacks can recover far more data than previously thought, and reveal that current alignment techniques do not eliminate memorization.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Cryptography and Security,Computer Science - Machine Learning},
  file = {/Users/adam/Zotero/storage/MLTVAND6/Nasr et al. - 2023 - Scalable Extraction of Training Data from (Product.pdf;/Users/adam/Zotero/storage/N5BPJWEY/2311.html}
}

@misc{NationalElectronicDisease2021,
  title = {National {{Electronic Disease Surveillance System}} \textbar{} {{CDC}}},
  year = 2021,
  month = sep,
  urldate = {2022-08-01},
  abstract = {Learn about how integrated surveillance information systems based on National Electronic Disease Surveillance System (NEDSS) architectural standards are primary sources of data for the National Notifiable Diseases Surveillance System (NNDSS).},
  howpublished = {https://www.cdc.gov/nndss/about/nedss.html},
  langid = {american}
}

@misc{NaturalThoughtsSelectingDistilling,
  title = {{{NaturalThoughts}}: {{Selecting}} and {{Distilling Reasoning Traces}} for {{General Reasoning Tasks}}},
  urldate = {2025-07-15},
  howpublished = {https://arxiv.org/html/2507.01921v1}
}

@misc{NCGGOfficialWeb,
  title = {{{NCGG Official Web Site}} \textbar{} {{National Center}} for {{Geriatrics}} and {{Gerontology}}},
  urldate = {2023-04-08},
  howpublished = {https://www.ncgg.go.jp/english/},
  file = {/Users/adam/Zotero/storage/BCS4KD3B/english.html}
}

@article{neeleyGlobalTeamsThat2015,
  title = {Global {{Teams That Work}}},
  author = {Neeley, Tsedal},
  year = 2015,
  month = oct,
  journal = {Harvard Business Review},
  issn = {0017-8012},
  urldate = {2023-04-26},
  abstract = {Many companies today rely on employees around the world, leveraging their diversity and local expertise to gain a competitive edge. However, geographically dispersed teams face a big challenge: Physical separation and cultural differences can create social distance, or a lack of emotional connection, that leads to misunderstandings and mistrust. To help global team leaders manage effectively, the author shares her SPLIT framework for mitigating social distance. It has five components: 1. Structure. If a team is made up of groups with different views about their relative power, the leader should connect frequently with those who are farthest away and emphasize unity. 2. Process. Meeting processes should allow for informal interactions that build empathy. 3. Language. Everyone, regardless of language fluency, should be empowered to speak up. 4. Identity. Team members must be active cultural learners and teachers to understand one another's identity and avoid misinterpreting behaviors. 5. Technology. When choosing between videoconferencing, e-mail, and other modes of communication, leaders should ask themselves if real-time conversation is desirable, if their message needs reinforcement, and if they are opting for the technology they want others to use. HBR Reprint R1510D},
  chapter = {Global strategy},
  keywords = {Cross-cultural management,Emotional intelligence,Global strategy,Virtual teams},
  file = {/Users/adam/Zotero/storage/YT4ZRG2S/global-teams-that-work.html}
}

@misc{neePhotoAlvanNee2020,
  title = {Photo by {{Alvan Nee}} on {{Unsplash}}},
  author = {Nee, Alvan},
  year = 2020,
  month = jun,
  urldate = {2024-07-17},
  abstract = {Download this photo by Alvan Nee on Unsplash},
  howpublished = {https://unsplash.com/photos/white-and-brown-long-fur-cat-ZCHj\_2lJP00},
  langid = {american},
  file = {/Users/adam/Zotero/storage/52XELP78/white-and-brown-long-fur-cat-ZCHj_2lJP00.html}
}

@misc{NetherlandsDwarfRabbits2020,
  title = {Netherlands {{Dwarf Rabbits}} - {{The Facts}}},
  year = 2020,
  month = sep,
  journal = {Home \& Roost},
  urldate = {2024-07-17},
  abstract = {It's a fact: the Netherland Dwarf is one of the most popular rabbit breeds in the world. These rabbits may be small in stature, but they're big in personality. They also have some special needs as far as care. The Netherland Dwarf is one of a handful of dwarf breeds. It's the smallest of these breeds, but owners will a},
  howpublished = {https://homeandroost.co.uk/blogs/rabbits/netherlands-dwarf-rabbits},
  langid = {english},
  file = {/Users/adam/Zotero/storage/WVJ6XIKM/netherlands-dwarf-rabbits.html}
}

@article{nomuraStrategyAgingSociety2016,
  title = {Strategy against Aging Society with Declining Birthrate in {{Japan}}},
  author = {NOMURA, Kyoko and KOIZUMI, Akio},
  year = 2016,
  journal = {Industrial Health},
  volume = {54},
  number = {6},
  pages = {477--479},
  doi = {10.2486/indhealth.54-477}
}

@misc{nunezAutoSafeCoderMultiAgentFramework2024,
  title = {{{AutoSafeCoder}}: {{A Multi-Agent Framework}} for {{Securing LLM Code Generation}} through {{Static Analysis}} and {{Fuzz Testing}}},
  shorttitle = {{{AutoSafeCoder}}},
  author = {Nunez, Ana and Islam, Nafis Tanveer and Jha, Sumit Kumar and Najafirad, Peyman},
  year = 2024,
  month = nov,
  number = {arXiv:2409.10737},
  eprint = {2409.10737},
  publisher = {arXiv},
  urldate = {2024-11-10},
  abstract = {Recent advancements in automatic code generation using large language models (LLMs) have brought us closer to fully automated secure software development. However, existing approaches often rely on a single agent for code generation, which struggles to produce secure, vulnerability-free code. Traditional program synthesis with LLMs has primarily focused on functional correctness, often neglecting critical dynamic security implications that happen during runtime. To address these challenges, we propose AutoSafeCoder, a multi-agent framework that leverages LLM-driven agents for code generation, vulnerability analysis, and security enhancement through continuous collaboration. The framework consists of three agents: a Coding Agent responsible for code generation, a Static Analyzer Agent identifying vulnerabilities, and a Fuzzing Agent performing dynamic testing using a mutation-based fuzzing approach to detect runtime errors. Our contribution focuses on ensuring the safety of multi-agent code generation by integrating dynamic and static testing in an iterative process during code generation by LLM that improves security. Experiments using the SecurityEval dataset demonstrate a 13\% reduction in code vulnerabilities compared to baseline LLMs, with no compromise in functionality.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Software Engineering},
  file = {/Users/adam/Zotero/storage/59U7XSVN/Nunez et al. - 2024 - AutoSafeCoder A Multi-Agent Framework for Securin.pdf;/Users/adam/Zotero/storage/3CL89WQN/2409.html}
}

@misc{OceanEngineering,
  title = {Ocean {{Engineering}}},
  urldate = {2022-07-14},
  howpublished = {https://eng.vt.edu/content/eng\_vt\_edu/en/academics/undergraduate-students/explore-engineering/ocean-engineering.html},
  langid = {english},
  file = {/Users/adam/Zotero/storage/JUKKN5UP/ocean-engineering.html}
}

@misc{OkPull2021,
  title = {Ok {{I Pull Up}}},
  year = 2021,
  month = aug,
  journal = {Know Your Meme},
  urldate = {2024-07-09},
  abstract = {Ok I Pull Up, continued Hop Out at the After Party, are the opening lyrics to Don Toliver's 2020 song "After Party." That year, people began using a sample},
  howpublished = {https://knowyourmeme.com/memes/ok-i-pull-up},
  file = {/Users/adam/Zotero/storage/R74ATHS8/ok-i-pull-up.html}
}

@misc{OpenAIPlatform,
  title = {{{OpenAI Platform}}},
  urldate = {2024-07-05},
  abstract = {Explore developer resources, tutorials, API docs, and dynamic examples to get the most out of OpenAI's platform.},
  howpublished = {https://platform.openai.com},
  langid = {english},
  file = {/Users/adam/Zotero/storage/WJZZ3744/chat.html}
}

@misc{OpenKnowledgeRepository,
  title = {Open {{Knowledge Repository}}},
  urldate = {2024-11-28},
  howpublished = {https://openknowledge.worldbank.org/entities/publication/5a876bd0-f85a-479b-ae32-cf0b7f33792f},
  file = {/Users/adam/Zotero/storage/K9NJZD6Z/5a876bd0-f85a-479b-ae32-cf0b7f33792f.html}
}

@misc{palaFerretFasterEffective2024,
  title = {Ferret: {{Faster}} and {{Effective Automated Red Teaming}} with {{Reward-Based Scoring Technique}}},
  shorttitle = {Ferret},
  author = {Pala, Tej Deep and Toh, Vernon Y. H. and Bhardwaj, Rishabh and Poria, Soujanya},
  year = 2024,
  month = aug,
  journal = {arXiv.org},
  urldate = {2024-10-04},
  abstract = {In today's era, where large language models (LLMs) are integrated into numerous real-world applications, ensuring their safety and robustness is crucial for responsible AI usage. Automated red-teaming methods play a key role in this process by generating adversarial attacks to identify and mitigate potential vulnerabilities in these models. However, existing methods often struggle with slow performance, limited categorical diversity, and high resource demands. While Rainbow Teaming, a recent approach, addresses the diversity challenge by framing adversarial prompt generation as a quality-diversity search, it remains slow and requires a large fine-tuned mutator for optimal performance. To overcome these limitations, we propose Ferret, a novel approach that builds upon Rainbow Teaming by generating multiple adversarial prompt mutations per iteration and using a scoring function to rank and select the most effective adversarial prompt. We explore various scoring functions, including reward models, Llama Guard, and LLM-as-a-judge, to rank adversarial mutations based on their potential harm to improve the efficiency of the search for harmful mutations. Our results demonstrate that Ferret, utilizing a reward model as a scoring function, improves the overall attack success rate (ASR) to 95\%, which is 46\% higher than Rainbow Teaming. Additionally, Ferret reduces the time needed to achieve a 90\% ASR by 15.2\% compared to the baseline and generates adversarial prompts that are transferable i.e. effective on other LLMs of larger size. Our codes are available at https://github.com/declare-lab/ferret.},
  howpublished = {https://arxiv.org/abs/2408.10701v1},
  langid = {english},
  file = {/Users/adam/Zotero/storage/ERNX42Z5/Pala et al. - 2024 - Ferret Faster and Effective Automated Red Teaming.pdf}
}

@misc{paulmcwhorterArduinoTutorialSetting2019,
  title = {Arduino {{Tutorial}} 1: {{Setting Up}} and {{Programming}} the {{Arduino}} for {{Absolute Beginners}}},
  shorttitle = {Arduino {{Tutorial}} 1},
  author = {{Paul McWhorter}},
  year = 2019,
  month = may,
  urldate = {2022-10-31}
}

@misc{pearceAsleepKeyboardAssessing2021,
  title = {Asleep at the {{Keyboard}}? {{Assessing}} the {{Security}} of {{GitHub Copilot}}'s {{Code Contributions}}},
  shorttitle = {Asleep at the {{Keyboard}}?},
  author = {Pearce, Hammond and Ahmad, Baleegh and Tan, Benjamin and {Dolan-Gavitt}, Brendan and Karri, Ramesh},
  year = 2021,
  month = dec,
  number = {arXiv:2108.09293},
  eprint = {2108.09293},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2108.09293},
  urldate = {2024-11-15},
  abstract = {There is burgeoning interest in designing AI-based systems to assist humans in designing computing systems, including tools that automatically generate computer code. The most notable of these comes in the form of the first self-described `AI pair programmer', GitHub Copilot, a language model trained over open-source GitHub code. However, code often contains bugs - and so, given the vast quantity of unvetted code that Copilot has processed, it is certain that the language model will have learned from exploitable, buggy code. This raises concerns on the security of Copilot's code contributions. In this work, we systematically investigate the prevalence and conditions that can cause GitHub Copilot to recommend insecure code. To perform this analysis we prompt Copilot to generate code in scenarios relevant to high-risk CWEs (e.g. those from MITRE's "Top 25" list). We explore Copilot's performance on three distinct code generation axes -- examining how it performs given diversity of weaknesses, diversity of prompts, and diversity of domains. In total, we produce 89 different scenarios for Copilot to complete, producing 1,689 programs. Of these, we found approximately 40\% to be vulnerable.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Cryptography and Security},
  file = {/Users/adam/Zotero/storage/IK3P97FM/Pearce et al. - 2021 - Asleep at the Keyboard Assessing the Security of .pdf;/Users/adam/Zotero/storage/CMHQ3NGD/2108.html}
}

@misc{PeerMentoring,
  title = {Peer {{Mentoring}}},
  urldate = {2022-07-30},
  abstract = {Experienced students offer support and advice to new students},
  howpublished = {https://eng.vt.edu/content/eng\_vt\_edu/en/ceed/ceed-undergraduate-programs/mentoring.html},
  langid = {english},
  file = {/Users/adam/Zotero/storage/7H7J98CU/mentoring.html}
}

@misc{PEPStyleGuide,
  title = {{{PEP}} 8 -- {{Style Guide}} for {{Python Code}} \textbar{} Peps.Python.Org},
  journal = {Python Enhancement Proposals (PEPs)},
  urldate = {2024-09-29},
  abstract = {This document gives coding conventions for the Python code comprising the standard library in the main Python distribution.  Please see the companion informational PEP describing style guidelines for the C code in the C implementation of Python.},
  howpublished = {https://peps.python.org/pep-0008/},
  langid = {english},
  file = {/Users/adam/Zotero/storage/DFL7THAA/pep-0008.html}
}

@article{petkovicIntegratedPortableMultiplex2019,
  title = {An {{Integrated Portable Multiplex Microchip Device}} for {{Fingerprinting Chemical Warfare Agents}}},
  author = {Petkovic, K and Swallow, A and Stewart, R and Gao, Y and Li, S and Glenn, F and Gotama, J and Dell'Olio, M and Best, M and Doward, J and Ovendon, S and Zhu, {\relax YG}},
  year = 2019,
  month = sep,
  journal = {MICROMACHINES},
  volume = {10},
  number = {9},
  issn = {2072-666X},
  doi = {10.3390/mi10090617},
  abstract = {The rapid and reliable detection of chemical and biological agents in the field is important for many applications such as national security, environmental monitoring, infectious diseases screening, and so on. Current commercially available devices may suffer from low field deployability, specificity, and reproducibility, as well as a high false alarm rate. This paper reports the development of a portable lab-on-a-chip device that could address these issues. The device integrates a polymer multiplexed microchip system, a contactless conductivity detector, a data acquisition and signal processing system, and a graphic/user interface. The samples are pre-treated by an on-chip capillary electrophoresis system. The separated analytes are detected by conductivity-based microsensors. Extensive studies are carried out to achieve satisfactory reproducibility of the microchip system. Chemical warfare agents soman (GD), sarin (GB), O-ethyl S-[2-diisoproylaminoethyl] methylphsophonothioate (VX), and their degradation products have been tested on the device. It was demonstrated that the device can fingerprint the tested chemical warfare agents. In addition, the detection of ricin and metal ions in water samples was demonstrated. Such a device could be used for the rapid and sensitive on-site detection of both chemical and biological agents in the future.},
  langid = {english},
  keywords = {CAPILLARY-ELECTROPHORESIS INSTRUMENT,chemical warfare agent,CHROMATOGRAPHY,conductivity sensor,CONTACTLESS CONDUCTIVITY DETECTION,DEGRADATION-PRODUCTS,LAB,lab on a chip,MASS-SPECTROMETRY,microchip capillary electrophoresis,MICROFLUIDICS,NERVE AGENTS,SAMPLES,SEPARATION}
}

@article{petkovicIntegratedPortableMultiplex2019a,
  title = {An {{Integrated Portable Multiplex Microchip Device}} for {{Fingerprinting Chemical Warfare Agents}}},
  author = {Petkovic, K and Swallow, A and Stewart, R and Gao, Y and Li, S and Glenn, F and Gotama, J and Dell'Olio, M and Best, M and Doward, J and Ovendon, S and Zhu, {\relax YG}},
  year = 2019,
  month = sep,
  journal = {MICROMACHINES},
  volume = {10},
  number = {9},
  issn = {2072-666X},
  doi = {10.3390/mi10090617},
  abstract = {The rapid and reliable detection of chemical and biological agents in the field is important for many applications such as national security, environmental monitoring, infectious diseases screening, and so on. Current commercially available devices may suffer from low field deployability, specificity, and reproducibility, as well as a high false alarm rate. This paper reports the development of a portable lab-on-a-chip device that could address these issues. The device integrates a polymer multiplexed microchip system, a contactless conductivity detector, a data acquisition and signal processing system, and a graphic/user interface. The samples are pre-treated by an on-chip capillary electrophoresis system. The separated analytes are detected by conductivity-based microsensors. Extensive studies are carried out to achieve satisfactory reproducibility of the microchip system. Chemical warfare agents soman (GD), sarin (GB), O-ethyl S-[2-diisoproylaminoethyl] methylphsophonothioate (VX), and their degradation products have been tested on the device. It was demonstrated that the device can fingerprint the tested chemical warfare agents. In addition, the detection of ricin and metal ions in water samples was demonstrated. Such a device could be used for the rapid and sensitive on-site detection of both chemical and biological agents in the future.},
  langid = {english},
  keywords = {CAPILLARY-ELECTROPHORESIS INSTRUMENT,chemical warfare agent,CHROMATOGRAPHY,conductivity sensor,CONTACTLESS CONDUCTIVITY DETECTION,DEGRADATION-PRODUCTS,LAB,lab on a chip,MASS-SPECTROMETRY,microchip capillary electrophoresis,MICROFLUIDICS,NERVE AGENTS,SAMPLES,SEPARATION}
}

@misc{plaugicSpotifysYearMusic2015,
  title = {Spotify's {{Year}} in {{Music}} Shows Just How Little We Pay Artists for Their Music},
  author = {Plaugic, Lizzie},
  year = 2015,
  month = dec,
  journal = {The Verge},
  urldate = {2023-04-20},
  abstract = {It's probably less than you think},
  howpublished = {https://www.theverge.com/2015/12/7/9861372/spotify-year-in-review-artist-payment-royalties},
  langid = {american},
  file = {/Users/adam/Zotero/storage/VIJBLW9D/spotify-year-in-review-artist-payment-royalties.html}
}

@article{ploskonkaInsightOrganophosphateChemical2019,
  title = {Insight into Organophosphate Chemical Warfare Agent Simulant Hydrolysis in Metal-Organic Frameworks},
  author = {Ploskonka, {\relax AM} and DeCoste, {\relax JB}},
  year = 2019,
  month = aug,
  journal = {JOURNAL OF HAZARDOUS MATERIALS},
  volume = {375},
  pages = {191--197},
  issn = {0304-3894},
  doi = {10.1016/j.jhazmat.2019.04.044},
  abstract = {Metal-organic frameworks (MOFs) are porous 3-dimensional crystalline structures that have shown promise for a variety of applications including adsorption, catalysis, and sensing. Modern warfare has placed chemical warfare agent (CWA) destruction at the forefront of chemical applications for MOFs. However, experiments involving CWAs can only be performed by a small number of highly trained individuals as they are extremely dangerous and available only to certain laboratories. As such, it is imperative that suitable chemical simulants and reaction conditions are determined for CWAs of interest. In this work, we determine the reaction rate for heterogeneous catalytic hydrolysis of eight commonly used G-agent simulants with zirconium-based MOFs. Of the simulants tested, only dimethyl chlorophosphate (DMCP), diisopropylfluorophosphate (DFP), and dimethyl p-nitrophenylphosphate (DMNP) exhibit the ability to be catalytically hydrolyzed in a manner similar to the G-agents by the MOFs studied. Two different base-catalyzed reaction mechanisms are proposed for the hydrolysis reaction on the different MOF secondary building units, and the effect of pH and buffer properties is determined using an N-ethylmorpholine (NEM) buffer at pH 8-10 and a 3-(cyclohexylamino)-1-propanesulofinic acid (CAPS) buffer at pH 10-11.},
  langid = {english},
  keywords = {Catalysis,CATALYST,Chemical warfare agents,DEGRADATION,DESIGN,DETOXIFICATION,DRUG-DELIVERY,GAS,Metal-organic frameworks,Organophosphates,REMOVAL,STABILITY,UIO-66}
}

@article{ploskonkaInsightOrganophosphateChemical2019a,
  title = {Insight into Organophosphate Chemical Warfare Agent Simulant Hydrolysis in Metal-Organic Frameworks},
  author = {Ploskonka, {\relax AM} and DeCoste, {\relax JB}},
  year = 2019,
  month = aug,
  journal = {JOURNAL OF HAZARDOUS MATERIALS},
  volume = {375},
  pages = {191--197},
  issn = {0304-3894},
  doi = {10.1016/j.jhazmat.2019.04.044},
  abstract = {Metal-organic frameworks (MOFs) are porous 3-dimensional crystalline structures that have shown promise for a variety of applications including adsorption, catalysis, and sensing. Modern warfare has placed chemical warfare agent (CWA) destruction at the forefront of chemical applications for MOFs. However, experiments involving CWAs can only be performed by a small number of highly trained individuals as they are extremely dangerous and available only to certain laboratories. As such, it is imperative that suitable chemical simulants and reaction conditions are determined for CWAs of interest. In this work, we determine the reaction rate for heterogeneous catalytic hydrolysis of eight commonly used G-agent simulants with zirconium-based MOFs. Of the simulants tested, only dimethyl chlorophosphate (DMCP), diisopropylfluorophosphate (DFP), and dimethyl p-nitrophenylphosphate (DMNP) exhibit the ability to be catalytically hydrolyzed in a manner similar to the G-agents by the MOFs studied. Two different base-catalyzed reaction mechanisms are proposed for the hydrolysis reaction on the different MOF secondary building units, and the effect of pH and buffer properties is determined using an N-ethylmorpholine (NEM) buffer at pH 8-10 and a 3-(cyclohexylamino)-1-propanesulofinic acid (CAPS) buffer at pH 10-11.},
  langid = {english},
  keywords = {Catalysis,CATALYST,Chemical warfare agents,DEGRADATION,DESIGN,DETOXIFICATION,DRUG-DELIVERY,GAS,Metal-organic frameworks,Organophosphates,REMOVAL,STABILITY,UIO-66}
}

@misc{policinskiRightBeForgotten2024,
  title = {The {{Right}} to {{Be Forgotten}}: {{Everything}} to {{Know About Erasing Digital Footprints}}},
  shorttitle = {The {{Right}} to {{Be Forgotten}}},
  author = {Policinski, By Gene},
  year = 2024,
  month = jun,
  journal = {Freedom Forum},
  urldate = {2024-11-26},
  abstract = {Does content on the internet live forever?\, Here's everything you need to know about the right to be forgotten.},
  howpublished = {https://www.freedomforum.org/right-to-be-forgotten/},
  langid = {american},
  file = {/Users/adam/Zotero/storage/W4S3D4EC/right-to-be-forgotten.html}
}

@misc{PolymerSearchResults,
  title = {Polymer {{Search Results Polymers}}: {{A Property Database}} 2021},
  urldate = {2022-07-24},
  howpublished = {https://poly-chemnetbase-com.ezproxy.lib.vt.edu/faces/polymers/PolymerSearchResults.xhtml},
  file = {/Users/adam/Zotero/storage/MVJK297L/PolymerSearchResults.html}
}

@misc{PopulationStatistics|NationalInstitute,
  title = {Population {{Statistics}}\textbar{{National Institute}} of {{Population}} and {{Social Security Research}}},
  urldate = {2023-04-08},
  howpublished = {https://www.ipss.go.jp/pp-zenkoku/e/zenkoku\_e2017/pp\_zenkoku2017e.asp},
  file = {/Users/adam/Zotero/storage/HKFTAZMD/pp_zenkoku2017e.html}
}

@misc{prasadSonyWH1000XM4Wireless,
  title = {Sony {{WH-1000XM4}} Wireless Noise-Canceling Headphones Review},
  author = {Prasad},
  journal = {GSMArena.com},
  urldate = {2024-10-27},
  abstract = {Sony has brought minor improvements across the board and further refined what was already a good product. The WH-1000XM4 are the latest in Sony's popular...},
  howpublished = {https://www.gsmarena.com/sony\_wh1000xm4\_wireless\_noisecanceling\_headphones\_review-news-45351.php},
  langid = {american},
  file = {/Users/adam/Zotero/storage/CS6IC3AT/sony_wh1000xm4_wireless_noisecanceling_headphones_review-news-45351.html}
}

@misc{prattHowKeepYour2020,
  title = {How to {{Keep Your Rabbit Quiet}} at {{Night}}},
  author = {Pratt, Amy},
  year = 2020,
  month = dec,
  journal = {The Bunny Lady},
  urldate = {2024-07-16},
  abstract = {Most of the time rabbits are very quiet pets. They can't bark loudly like a dog or incessantly meow at you like a cat. However, rabbits that are kept in},
  howpublished = {https://bunnylady.com/quiet-at-night/},
  langid = {american},
  file = {/Users/adam/Zotero/storage/DW8H8M5R/quiet-at-night.html}
}

@misc{ProceduralKnowledgePretraining,
  title = {Procedural {{Knowledge}} in {{Pretraining Drives Reasoning}} in {{Large Language Models}}},
  urldate = {2025-07-08},
  howpublished = {https://arxiv.org/html/2411.12580v1}
}

@misc{ProductDesignEngineer,
  title = {Product {{Design Engineer}} at {{Moog}}, {{Inc}}. - {{Tarta}}.Ai},
  urldate = {2022-07-29},
  howpublished = {https://tarta.ai/j/o6pq24ABifRoE52N7D2C0522-product-design-engineer-in-blacksburg-virginia-at-moog-inc?utm\_campaign=google\_jobs\_apply\&utm\_source=google\_jobs\_apply\&utm\_medium=organic},
  file = {/Users/adam/Zotero/storage/SW5YVK99/o6pq24ABifRoE52N7D2C0522-product-design-engineer-in-blacksburg-virginia-at-moog-inc.html}
}

@misc{PsychiatryorgWhatPsychiatry,
  title = {Psychiatry.Org - {{What}} Is {{Psychiatry}}?},
  urldate = {2022-07-29},
  abstract = {Learn more about psychiatry, psychiatric training, and more at psychiatry.org},
  howpublished = {https://psychiatry.org:443/patients-families/what-is-psychiatry},
  langid = {english},
  file = {/Users/adam/Zotero/storage/AB5P5PQF/what-is-psychiatry.html}
}

@misc{PurpleLlamaCyberSecEval,
  title = {Purple {{Llama CyberSecEval}}: {{A}} Benchmark for Evaluating the Cybersecurity Risks of Large Language Models \textbar{} {{Research}} - {{AI}} at {{Meta}}},
  urldate = {2024-11-15},
  howpublished = {https://ai.meta.com/research/publications/purple-llama-cyberseceval-a-benchmark-for-evaluating-the-cybersecurity-risks-of-large-language-models/},
  file = {/Users/adam/Zotero/storage/X4VC7ECW/purple-llama-cyberseceval-a-benchmark-for-evaluating-the-cybersecurity-risks-of-large-language-.html}
}

@misc{QatarCountriesOffice,
  title = {Qatar - {{Countries}} - {{Office}} of the {{Historian}}},
  urldate = {2023-04-19},
  howpublished = {https://history.state.gov/countries/qatar},
  file = {/Users/adam/Zotero/storage/9R8WHX4H/qatar.html}
}

@misc{QatarGovernmentSociety,
  title = {Qatar - {{Government}} and Society \textbar{} {{Britannica}}},
  urldate = {2023-04-19},
  abstract = {A constitutional emirate with one advisory body, Qatar is ruled by a hereditary emir from the \=Al Th\=an\=i. Members of the ruling family hold almost all the major ministerial posts, which are appointed by the emir. The family, however, is large and fragmented. As oil revenues rose after World War II, contention within the ruling family grew, and there have been several bloodless palace coups. The emir's power is constrained by the need to maintain the support of important family members, many of whom occupy high governmental posts. The homogeneity of the ruling family and the country's wealth contribute to},
  howpublished = {https://www.britannica.com/place/Qatar/Government-and-society},
  langid = {english},
  file = {/Users/adam/Zotero/storage/SJ5C4W3P/Government-and-society.html}
}

@misc{QatarGradingSystem,
  title = {Qatar {{Grading System}}},
  urldate = {2023-04-19},
  howpublished = {https://www.scholaro.com/db/Countries/Qatar/Grading-System}
}

@misc{QatarGradingSystema,
  title = {Qatar {{Grading System}}},
  urldate = {2023-04-19},
  howpublished = {https://www.scholaro.com/db/Countries/Qatar/Grading-System}
}

@misc{QatarNationalDay,
  title = {Qatar {{National Day}} 2021 {{Celebrations}}},
  urldate = {2023-04-19},
  howpublished = {https://www.marhaba.qa/qatar-national-day-celebrations/},
  file = {/Users/adam/Zotero/storage/LFTQNI3V/qatar-national-day-celebrations.html}
}

@misc{QATARNATIONALDAYa,
  title = {{{QATAR NATIONAL DAY}} - {{December}} 18, 2023 - {{National Today}}},
  urldate = {2023-04-19},
  howpublished = {https://nationaltoday.com/qatar-national-day/},
  file = {/Users/adam/Zotero/storage/C3NKIUWQ/qatar-national-day.html}
}

@misc{qinCOLDDecodingEnergybased2022,
  title = {{{COLD Decoding}}: {{Energy-based Constrained Text Generation}} with {{Langevin Dynamics}}},
  shorttitle = {{{COLD Decoding}}},
  author = {Qin, Lianhui and Welleck, Sean and Khashabi, Daniel and Choi, Yejin},
  year = 2022,
  month = oct,
  number = {arXiv:2202.11705},
  eprint = {2202.11705},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-03-30},
  abstract = {Many applications of text generation require incorporating different constraints to control the semantics or style of generated text. These constraints can be hard (e.g., ensuring certain keywords are included in the output) and soft (e.g., contextualizing the output with the left- or right-hand context). In this paper, we present Energy-based Constrained Decoding with Langevin Dynamics (COLD), a decoding framework which unifies constrained generation as specifying constraints through an energy function, then performing efficient differentiable reasoning over the constraints through gradient-based sampling. COLD decoding is a flexible framework that can be applied directly to off-the-shelf left-to-right language models without the need for any task-specific fine-tuning, as demonstrated through three challenging text generation applications: lexically-constrained generation, abductive reasoning, and counterfactual reasoning. Our experiments on these constrained generation tasks point to the effectiveness of our approach, both in terms of automatic and human evaluation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/adam/Zotero/storage/GXWATD58/Qin et al. - 2022 - COLD Decoding Energy-based Constrained Text Gener.pdf;/Users/adam/Zotero/storage/9Q7ZZM64/2202.html}
}

@article{QuantificationIronWholeGrain,
  title = {Quantification of {{Iron}} in {{Whole-Grain Cereal}}},
  pages = {5},
  langid = {english},
  file = {/Users/adam/Zotero/storage/6JGZRMJG/Quantification of Iron in Whole-Grain Cereal.pdf}
}

@misc{quoraBestPracticesManaging,
  title = {4 {{Best Practices For Managing Teams Across Time Zones}}},
  author = {Quora},
  journal = {Forbes},
  urldate = {2023-04-23},
  abstract = {How can companies manage teams across time zones? Answer by Charlene Walters, PhD, Business Mentor, Consultant, Corporate Trainer \& Author.},
  chapter = {Consumer Tech},
  howpublished = {https://www.forbes.com/sites/quora/2022/09/11/4-best-practices-for-managing-teams-across-time-zones/},
  langid = {english},
  file = {/Users/adam/Zotero/storage/Z8T92DRI/4-best-practices-for-managing-teams-across-time-zones.html}
}

@misc{RabbitBehaviourRabbit,
  title = {Rabbit Behaviour \textbar{} Rabbit Advice and Welfare \textbar{} Rspca Advice - {{RSPCA}} - Rspca.Org.Uk},
  journal = {RSPCA},
  urldate = {2024-07-16},
  abstract = {Do you wonder why your rabbits behave in certain ways? Learn more and read our top tips to help with rabbit behaviour.},
  howpublished = {https://www.rspca.org.uk/adviceandwelfare/pets/rabbits/behaviour},
  langid = {american},
  file = {/Users/adam/Zotero/storage/RZ3AWCVH/behaviour.html}
}

@misc{racomeauBunnyMakingBed2014,
  title = {Bunny Making the Bed},
  author = {{RA Comeau}},
  year = 2014,
  month = feb,
  urldate = {2024-07-18}
}

@misc{ResearchTopicsMaterials,
  title = {Research {{Topics}} \textbar{} {{Materials Science}} and {{Engineering}}},
  urldate = {2022-07-14},
  howpublished = {https://www.mse.cornell.edu/mse/research/research-topics},
  file = {/Users/adam/Zotero/storage/IFBJIYRW/research-topics.html}
}

@misc{ReserveRentSpaces,
  title = {Reserve \& {{Rent Spaces}} - {{UVA Library}}},
  urldate = {2022-09-12},
  howpublished = {https://www.library.virginia.edu/services/reserve-rooms/},
  file = {/Users/adam/Zotero/storage/WVM8T3K8/reserve-rooms.html}
}

@misc{resnikoffSpotifyPaidMe2015,
  title = {Spotify {{Paid Me}} \$40,000 for 10 {{Million Streams}}. {{Is That Fair}}?},
  author = {Resnikoff, Paul},
  year = 2015,
  month = oct,
  journal = {Digital Music News},
  urldate = {2023-04-20},
  abstract = {The debate over streaming royalties continues to roil on, and the central question is just how much artists deserve and what's fair.},
  langid = {american},
  file = {/Users/adam/Zotero/storage/FCWUL2SA/spotify-paid-me-40000-for-10-million-streams-is-that-fair.html}
}

@incollection{rileyGreatestHappinessPrinciple2013,
  title = {Greatest {{Happiness Principle}}},
  booktitle = {International {{Encyclopedia}} of {{Ethics}}},
  author = {Riley, Jonathan},
  year = 2013,
  publisher = {John Wiley \& Sons, Ltd},
  doi = {10.1002/9781444367072.wbiee762},
  urldate = {2024-11-08},
  abstract = {The greatest happiness principle is the ultimate standard of morality set up by classical utilitarianism (see Utilitarianism). That classical creed conceives of good as happiness (see Happiness) and holds that right actions are those which maximize the total happiness of the members of the community. As John Stuart Mill (see Mill, John Stuart) explains in Utilitarianism (1861), ``The creed which accepts as the foundation of morals, Utility, or the Greatest Happiness Principle, holds that actions are right in proportion as they tend to promote happiness, wrong as they tend to produce the reverse of happiness'' (1969: 210). Moreover, happiness is understood in hedonistic terms, ``By happiness is intended pleasure, and the absence of pain; by unhappiness, pain, and the privation of pleasure'' (see Hedonism; Pleasure).},
  copyright = {Copyright \copyright{} 2013 Blackwell Publishing Ltd. All rights reserved.},
  isbn = {978-1-4443-6707-2},
  langid = {english},
  keywords = {Bentham,Bernard,ethics in economics,Henry,history of philosophy,Jeremy,John,John Stuart,justice,Mill,normative ethics,pleasure,Rawls,Sidgwick,Williams},
  file = {/Users/adam/Zotero/storage/B2R5R7ST/Riley - 2013 - Greatest Happiness Principle.pdf;/Users/adam/Zotero/storage/DY58PU9T/9781444367072.html}
}

@article{ritchieHowManySpecies2022,
  title = {How Many Species Are There?},
  author = {Ritchie, Hannah},
  year = 2022,
  journal = {Our World in Data}
}

@misc{rottgerXSTestTestSuite2024,
  title = {{{XSTest}}: {{A Test Suite}} for {{Identifying Exaggerated Safety Behaviours}} in {{Large Language Models}}},
  shorttitle = {{{XSTest}}},
  author = {R{\"o}ttger, Paul and Kirk, Hannah Rose and Vidgen, Bertie and Attanasio, Giuseppe and Bianchi, Federico and Hovy, Dirk},
  year = 2024,
  month = apr,
  number = {arXiv:2308.01263},
  eprint = {2308.01263},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2308.01263},
  urldate = {2024-12-01},
  abstract = {Without proper safeguards, large language models will readily follow malicious instructions and generate toxic content. This risk motivates safety efforts such as red-teaming and large-scale feedback learning, which aim to make models both helpful and harmless. However, there is a tension between these two objectives, since harmlessness requires models to refuse to comply with unsafe prompts, and thus not be helpful. Recent anecdotal evidence suggests that some models may have struck a poor balance, so that even clearly safe prompts are refused if they use similar language to unsafe prompts or mention sensitive topics. In this paper, we introduce a new test suite called XSTest to identify such eXaggerated Safety behaviours in a systematic way. XSTest comprises 250 safe prompts across ten prompt types that well-calibrated models should not refuse to comply with, and 200 unsafe prompts as contrasts that models, for most applications, should refuse. We describe XSTest's creation and composition, and then use the test suite to highlight systematic failure modes in state-of-the-art language models as well as more general challenges in building safer language models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/adam/Zotero/storage/XJ8MKQKZ/Rttger et al. - 2024 - XSTest A Test Suite for Identifying Exaggerated S.pdf;/Users/adam/Zotero/storage/RMRIKW5W/2308.html}
}

@misc{samvelyanRainbowTeamingOpenEnded2024,
  title = {Rainbow {{Teaming}}: {{Open-Ended Generation}} of {{Diverse Adversarial Prompts}}},
  shorttitle = {Rainbow {{Teaming}}},
  author = {Samvelyan, Mikayel and Raparthy, Sharath Chandra and Lupu, Andrei and Hambro, Eric and Markosyan, Aram H. and Bhatt, Manish and Mao, Yuning and Jiang, Minqi and {Parker-Holder}, Jack and Foerster, Jakob and Rockt{\"a}schel, Tim and Raileanu, Roberta},
  year = 2024,
  month = jul,
  number = {arXiv:2402.16822},
  eprint = {2402.16822},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2402.16822},
  urldate = {2024-10-04},
  abstract = {As large language models (LLMs) become increasingly prevalent across many real-world applications, understanding and enhancing their robustness to adversarial attacks is of paramount importance. Existing methods for identifying adversarial prompts tend to focus on specific domains, lack diversity, or require extensive human annotations. To address these limitations, we present Rainbow Teaming, a novel black-box approach for producing a diverse collection of adversarial prompts. Rainbow Teaming casts adversarial prompt generation as a quality-diversity problem, and uses open-ended search to generate prompts that are both effective and diverse. Focusing on the safety domain, we use Rainbow Teaming to target various state-of-the-art LLMs, including the Llama 2 and Llama 3 models. Our approach reveals hundreds of effective adversarial prompts, with an attack success rate exceeding 90\% across all tested models. Furthermore, we demonstrate that fine-tuning models with synthetic data generated by the Rainbow Teaming method significantly enhances their safety without sacrificing general performance or helpfulness. We additionally explore the versatility of Rainbow Teaming by applying it to question answering and cybersecurity, showcasing its potential to drive robust open-ended self-improvement in a wide range of applications.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/adam/Zotero/storage/T3KBNX7R/Samvelyan et al. - 2024 - Rainbow Teaming Open-Ended Generation of Diverse .pdf;/Users/adam/Zotero/storage/DTFRLAZ5/2402.html}
}

@misc{sandlinListenFullVersion2023,
  title = {Listen to the Full Version of the Viral "Capybara" Song, Created by a 22-Year-Old {{Russian}} Blogger},
  author = {Sandlin, Jennifer},
  year = 2023,
  month = sep,
  journal = {Boing Boing},
  urldate = {2024-07-06},
  abstract = {By now you've surely heard that "capybara" song that went viral all over TikTok (and particularly in the corner of TikTok affectionately called "CapybaraTok") earlier this year---you know, the one\dots},
  howpublished = {https://boingboing.net/2023/09/15/listen-to-the-full-version-of-the-viral-capybara-song-created-by-a-22-year-old-russian-blogger.html},
  langid = {american},
  file = {/Users/adam/Zotero/storage/WLRS8YV5/listen-to-the-full-version-of-the-viral-capybara-song-created-by-a-22-year-old-russian-blogger.html}
}

@article{scasperDeepForgettingUnlearning2023,
  title = {Deep {{Forgetting}} \& {{Unlearning}} for {{Safely-Scoped LLMs}}},
  author = {{scasper}},
  year = 2023,
  month = dec,
  urldate = {2024-11-26},
  abstract = {Thanks to Phillip Christoffersen, Adam Gleave, Anjali Gopal, Soroush Pour, and Fabien Roger for useful discussions and feedback.~ \dots},
  langid = {english}
}

@misc{schneiderShrinkanomicsPolicyLessons,
  title = {Shrinkanomics: {{Policy Lessons}} from {{Japan}} on {{Aging}} -- {{IMF F}}\&{{D}}},
  shorttitle = {Shrinkanomics: {{Lessons}} from {{Japan}}},
  author = {Schneider, Todd},
  journal = {IMF},
  urldate = {2023-04-08},
  abstract = {Japan is the world's policy laboratory for dealing with an aging, shrinking population, writes the IMF's Gee Hee Hong and Todd Schneider.},
  howpublished = {https://www.imf.org/en/Publications/fandd/issues/2020/03/shrinkanomics-policy-lessons-from-japan-on-population-aging-schneider},
  langid = {english},
  file = {/Users/adam/Zotero/storage/CES5QFHA/shrinkanomics-policy-lessons-from-japan-on-population-aging-schneider.html}
}

@article{schwenkChemicalWarfareAgents2018,
  title = {Chemical Warfare Agents. {{Classes}} and Targets},
  author = {Schwenk, M},
  year = 2018,
  month = sep,
  journal = {TOXICOLOGY LETTERS},
  volume = {293},
  pages = {253--263},
  issn = {0378-4274},
  doi = {10.1016/j.toxlet.2017.11.040},
  abstract = {Synthetic toxic chemicals (toxicants) and biological poisons (toxins) have been developed as chemical warfare agents in the last century. At the time of their initial consideration as chemical weapon, only restricted knowledge existed about their mechanisms of action. There exist two different types of acute toxic action: nonspecific cytotoxic mechanisms with multiple chemo-biological interactions versus specific mechanisms that tend to have just a single or a few target biomolecules. TRPV1- and TRPA-receptors are often involved as chemosensors that induce neurogenic inflammation. The present work briefly surveys classes and toxicologically relevant features of chemical warfare agents and describes mechanisms of toxic action.},
  langid = {english},
  keywords = {ACTIVATION,Chemical warfare agent,Chemical weapon,CUTANEOUS EXPOSURE,DNA-DAMAGE,LUNG INJURY,MECHANISMS,MOLECULAR TARGETS,Neurogenic inflammation,NEUROGENIC INFLAMMATION,OSMIUM-TETROXIDE,POTENTIAL TRP CHANNELS,SULFUR MUSTARD,Toxic,TRP}
}

@article{schwenkChemicalWarfareAgents2018a,
  title = {Chemical Warfare Agents. {{Classes}} and Targets},
  author = {Schwenk, M},
  year = 2018,
  month = sep,
  journal = {TOXICOLOGY LETTERS},
  volume = {293},
  pages = {253--263},
  issn = {0378-4274},
  doi = {10.1016/j.toxlet.2017.11.040},
  abstract = {Synthetic toxic chemicals (toxicants) and biological poisons (toxins) have been developed as chemical warfare agents in the last century. At the time of their initial consideration as chemical weapon, only restricted knowledge existed about their mechanisms of action. There exist two different types of acute toxic action: nonspecific cytotoxic mechanisms with multiple chemo-biological interactions versus specific mechanisms that tend to have just a single or a few target biomolecules. TRPV1- and TRPA-receptors are often involved as chemosensors that induce neurogenic inflammation. The present work briefly surveys classes and toxicologically relevant features of chemical warfare agents and describes mechanisms of toxic action.},
  langid = {english},
  keywords = {ACTIVATION,Chemical warfare agent,Chemical weapon,CUTANEOUS EXPOSURE,DNA-DAMAGE,LUNG INJURY,MECHANISMS,MOLECULAR TARGETS,Neurogenic inflammation,NEUROGENIC INFLAMMATION,OSMIUM-TETROXIDE,POTENTIAL TRP CHANNELS,SULFUR MUSTARD,Toxic,TRP}
}

@book{scottwisorEthicsGlobalPoverty2016,
  title = {The {{Ethics}} of {{Global Poverty}}},
  author = {{Scott Wisor}},
  year = 2016,
  series = {An Introduction},
  edition = {1},
  publisher = {Routledge},
  doi = {10.4324/9781315738765},
  isbn = {978-1-315-73876-5},
  langid = {english}
}

@misc{SeniorCybersecurityEngineer,
  title = {Senior {{Cybersecurity Engineer}} - {{ESG}} at {{Boeing}}},
  urldate = {2022-07-14},
  abstract = {Learn more about applying for Senior Cybersecurity Engineer - ESG at Boeing},
  howpublished = {https://jobs.boeing.com/job/seal-beach/senior-cybersecurity-engineer-esg/185/32535841728},
  langid = {english},
  file = {/Users/adam/Zotero/storage/WZ6HLTXW/32535841728.html}
}

@article{sharmaChemicalProtectionStudies2019,
  title = {Chemical {{Protection Studies}} of {{Activated Carbon Spheres}} Based {{Permeable Protective Clothing Against Sulfur Mustard}}, a {{Chemical Warfare Agent}}},
  author = {Sharma, {\relax PK} and Singh, {\relax VV} and Tripathi, {\relax NK} and Sathe, M and Verma, V and Sharma, {\relax SP} and Tomar, {\relax LNS} and Chaturvedi, A and Yadav, {\relax SS} and Thakare, {\relax VB} and Acharya, J and Gupta, {\relax AK} and Ganesan, K},
  year = 2019,
  month = nov,
  journal = {DEFENCE SCIENCE JOURNAL},
  volume = {69},
  number = {6},
  pages = {577--584},
  issn = {0011-748X},
  doi = {10.14429/dsj.69.13958},
  abstract = {Technological advancements in the field of chemical threat have made it possible to create extremely dangerous chemical warfare agents (CWA). Hence, the effective protection of personnel is very important in a chemical warfare scenario amidst the current climate of terrorism awareness. In particular, body protection plays a substantial role in the chemical defence considering the urgency of situation in the nuclear, biological and chemical environment. Activated carbon spheres (ACS) based permeable chemical protective clothing (coverall) was developed for protection against CWA. The adsorbent material i.e, ACS used in this protective clothing provided higher adsorption capacity (1029 mg/g in terms of iodine) and low thermal burden (34 degrees C WBGT index) compared to earlier indigenously developed NBC suit. This article focuses on the extensive evaluation of chemical protective clothing against sulfur mustard (HD), a CWA. The results revealed that the developed protective clothing provided more than 24 h protection against HD. This chemical protective suit is light weight ({$<$} 2.75 kg for XL size). It also has higher air permeability ({$>$} 30 cm(3)/s/cm(2)) as well as less water vapour resistance ({$<$} 9.6 m(2)Pa/W). With continued innovations in materials and attention to key challenges it is expected that advanced, multifunction chemical protective suit will play a pivotal role in the CWA protection scenario.},
  langid = {english},
  keywords = {Activated carbon spheres,Chemical protection,Chemical protective suit,Chemical warfare agents,NBC protective suit,Sulphur mustard}
}

@article{sharmaChemicalProtectionStudies2019a,
  title = {Chemical {{Protection Studies}} of {{Activated Carbon Spheres}} Based {{Permeable Protective Clothing Against Sulfur Mustard}}, a {{Chemical Warfare Agent}}},
  author = {Sharma, {\relax PK} and Singh, {\relax VV} and Tripathi, {\relax NK} and Sathe, M and Verma, V and Sharma, {\relax SP} and Tomar, {\relax LNS} and Chaturvedi, A and Yadav, {\relax SS} and Thakare, {\relax VB} and Acharya, J and Gupta, {\relax AK} and Ganesan, K},
  year = 2019,
  month = nov,
  journal = {DEFENCE SCIENCE JOURNAL},
  volume = {69},
  number = {6},
  pages = {577--584},
  issn = {0011-748X},
  doi = {10.14429/dsj.69.13958},
  abstract = {Technological advancements in the field of chemical threat have made it possible to create extremely dangerous chemical warfare agents (CWA). Hence, the effective protection of personnel is very important in a chemical warfare scenario amidst the current climate of terrorism awareness. In particular, body protection plays a substantial role in the chemical defence considering the urgency of situation in the nuclear, biological and chemical environment. Activated carbon spheres (ACS) based permeable chemical protective clothing (coverall) was developed for protection against CWA. The adsorbent material i.e, ACS used in this protective clothing provided higher adsorption capacity (1029 mg/g in terms of iodine) and low thermal burden (34 degrees C WBGT index) compared to earlier indigenously developed NBC suit. This article focuses on the extensive evaluation of chemical protective clothing against sulfur mustard (HD), a CWA. The results revealed that the developed protective clothing provided more than 24 h protection against HD. This chemical protective suit is light weight ({$<$} 2.75 kg for XL size). It also has higher air permeability ({$>$} 30 cm(3)/s/cm(2)) as well as less water vapour resistance ({$<$} 9.6 m(2)Pa/W). With continued innovations in materials and attention to key challenges it is expected that advanced, multifunction chemical protective suit will play a pivotal role in the CWA protection scenario.},
  langid = {english},
  keywords = {Activated carbon spheres,Chemical protection,Chemical protective suit,Chemical warfare agents,NBC protective suit,Sulphur mustard}
}

@unpublished{shiMUSEMachineUnlearning2024,
  title = {{{MUSE}}: {{Machine Unlearning Six-Way Evaluation}} for {{Language Models}}},
  author = {Shi, Weijia and Lee, Jaechan and Huang, Yangsibo and Malladi, Sadhika and Zhao, Jieyu and Holtzman, Ari and Liu, Daogao and Zettlemoyer, Luke and Smith, Noah A and Zhang, Chiyuan},
  year = 2024,
  month = jul,
  journal = {arXiv [cs.CL]},
  abstract = {Language models (LMs) are trained on vast amounts of text data, which may include private and copyrighted content. Data owners may request the removal of their data from a trained model due to privacy or copyright concerns. However, exactly unlearning only these datapoints (i.e., retraining with the data removed) is intractable in modern-day models. This has led to the development of many approximate unlearning algorithms. The evaluation of the efficacy of these algorithms has traditionally been narrow in scope, failing to precisely quantify the success and practicality of the algorithm from the perspectives of both the model deployers and the data owners. We address this issue by proposing MUSE, a comprehensive machine unlearning evaluation benchmark that enumerates six diverse desirable properties for unlearned models: (1) no verbatim memorization, (2) no knowledge memorization, (3) no privacy leakage, (4) utility preservation on data not intended for removal, (5) scalability with respect to the size of removal requests, and (6) sustainability over sequential unlearning requests. Using these criteria, we benchmark how effectively eight popular unlearning algorithms on 7B-parameter LMs can unlearn Harry Potter books and news articles. Our results demonstrate that most algorithms can prevent verbatim memorization and knowledge memorization to varying degrees, but only one algorithm does not lead to severe privacy leakage. Furthermore, existing algorithms fail to meet deployer's expectations because they often degrade general model utility and also cannot sustainably accommodate successive unlearning requests or large-scale content removal. Our findings identify key issues with the practicality of existing unlearning algorithms on language models, and we release our benchmark to facilitate further evaluations: muse-bench.github.io},
  isbn = {2407.06460},
  file = {/Users/adam/Zotero/storage/BPJLLIPZ/Shi et al. - 2024 - MUSE Machine Unlearning Six-Way Evaluation for La.pdf}
}

@misc{shiMUSEMachineUnlearning2024a,
  title = {{{MUSE}}: {{Machine Unlearning Six-Way Evaluation}} for {{Language Models}}},
  shorttitle = {{{MUSE}}},
  author = {Shi, Weijia and Lee, Jaechan and Huang, Yangsibo and Malladi, Sadhika and Zhao, Jieyu and Holtzman, Ari and Liu, Daogao and Zettlemoyer, Luke and Smith, Noah A. and Zhang, Chiyuan},
  year = 2024,
  month = jul,
  number = {arXiv:2407.06460},
  eprint = {2407.06460},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2407.06460},
  urldate = {2024-12-07},
  abstract = {Language models (LMs) are trained on vast amounts of text data, which may include private and copyrighted content. Data owners may request the removal of their data from a trained model due to privacy or copyright concerns. However, exactly unlearning only these datapoints (i.e., retraining with the data removed) is intractable in modern-day models. This has led to the development of many approximate unlearning algorithms. The evaluation of the efficacy of these algorithms has traditionally been narrow in scope, failing to precisely quantify the success and practicality of the algorithm from the perspectives of both the model deployers and the data owners. We address this issue by proposing MUSE, a comprehensive machine unlearning evaluation benchmark that enumerates six diverse desirable properties for unlearned models: (1) no verbatim memorization, (2) no knowledge memorization, (3) no privacy leakage, (4) utility preservation on data not intended for removal, (5) scalability with respect to the size of removal requests, and (6) sustainability over sequential unlearning requests. Using these criteria, we benchmark how effectively eight popular unlearning algorithms on 7B-parameter LMs can unlearn Harry Potter books and news articles. Our results demonstrate that most algorithms can prevent verbatim memorization and knowledge memorization to varying degrees, but only one algorithm does not lead to severe privacy leakage. Furthermore, existing algorithms fail to meet deployer's expectations because they often degrade general model utility and also cannot sustainably accommodate successive unlearning requests or large-scale content removal. Our findings identify key issues with the practicality of existing unlearning algorithms on language models, and we release our benchmark to facilitate further evaluations: muse-bench.github.io},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/adam/Zotero/storage/FQEXFSJH/Shi et al. - 2024 - MUSE Machine Unlearning Six-Way Evaluation for La.pdf;/Users/adam/Zotero/storage/KB7P86D9/2407.html}
}

@unpublished{shumailovUnUnlearningUnlearningNot2024,
  title = {{{UnUnlearning}}: {{Unlearning}} Is Not Sufficient for Content Regulation in Advanced Generative {{AI}}},
  author = {Shumailov, Ilia and Hayes, Jamie and Triantafillou, Eleni and {Ortiz-Jimenez}, Guillermo and Papernot, Nicolas and Jagielski, Matthew and Yona, Itay and Howard, Heidi and Bagdasaryan, Eugene},
  year = 2024,
  month = jun,
  journal = {arXiv [cs.LG]},
  abstract = {Exact unlearning was first introduced as a privacy mechanism that allowed a user to retract their data from machine learning models on request. Shortly after, inexact schemes were proposed to mitigate the impractical costs associated with exact unlearning. More recently unlearning is often discussed as an approach for removal of impermissible knowledge i.e. knowledge that the model should not possess such as unlicensed copyrighted, inaccurate, or malicious information. The promise is that if the model does not have a certain malicious capability, then it cannot be used for the associated malicious purpose. In this paper we revisit the paradigm in which unlearning is used for in Large Language Models (LLMs) and highlight an underlying inconsistency arising from in-context learning. Unlearning can be an effective control mechanism for the training phase, yet it does not prevent the model from performing an impermissible act during inference. We introduce a concept of ununlearning, where unlearned knowledge gets reintroduced in-context, effectively rendering the model capable of behaving as if it knows the forgotten knowledge. As a result, we argue that content filtering for impermissible knowledge will be required and even exact unlearning schemes are not enough for effective content regulation. We discuss feasibility of ununlearning for modern LLMs and examine broader implications.},
  isbn = {2407.00106}
}

@unpublished{siKnowledgeUnlearningLLMs2023,
  title = {Knowledge {{Unlearning}} for {{LLMs}}: {{Tasks}}, {{Methods}}, and {{Challenges}}},
  author = {Si, Nianwen and Zhang, Hao and Chang, Heyu and Zhang, Wenlin and Qu, Dan and Zhang, Weiqiang},
  year = 2023,
  month = nov,
  journal = {arXiv [cs.CL]},
  abstract = {In recent years, large language models (LLMs) have spurred a new research paradigm in natural language processing. Despite their excellent capability in knowledge-based question answering and reasoning, their potential to retain faulty or even harmful knowledge poses risks of malicious application. The challenge of mitigating this issue and transforming these models into purer assistants is crucial for their widespread applicability. Unfortunately, Retraining LLMs repeatedly to eliminate undesirable knowledge is impractical due to their immense parameters. Knowledge unlearning, derived from analogous studies on machine unlearning, presents a promising avenue to address this concern and is notably advantageous in the context of LLMs. It allows for the removal of harmful knowledge in an efficient manner, without affecting unrelated knowledge in the model. To this end, we provide a survey of knowledge unlearning in the era of LLMs. Firstly, we formally define the knowledge unlearning problem and distinguish it from related works. Subsequently, we categorize existing knowledge unlearning methods into three classes: those based on parameter optimization, parameter merging, and in-context learning, and introduce details of these unlearning methods. We further present evaluation datasets used in existing methods, and finally conclude this survey by presenting the ongoing challenges and future directions.},
  isbn = {2311.15766}
}

@misc{siKnowledgeUnlearningLLMs2023a,
  title = {Knowledge {{Unlearning}} for {{LLMs}}: {{Tasks}}, {{Methods}}, and {{Challenges}}},
  shorttitle = {Knowledge {{Unlearning}} for {{LLMs}}},
  author = {Si, Nianwen and Zhang, Hao and Chang, Heyu and Zhang, Wenlin and Qu, Dan and Zhang, Weiqiang},
  year = 2023,
  month = dec,
  number = {arXiv:2311.15766},
  eprint = {2311.15766},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2311.15766},
  urldate = {2024-12-05},
  abstract = {In recent years, large language models (LLMs) have spurred a new research paradigm in natural language processing. Despite their excellent capability in knowledge-based question answering and reasoning, their potential to retain faulty or even harmful knowledge poses risks of malicious application. The challenge of mitigating this issue and transforming these models into purer assistants is crucial for their widespread applicability. Unfortunately, Retraining LLMs repeatedly to eliminate undesirable knowledge is impractical due to their immense parameters. Knowledge unlearning, derived from analogous studies on machine unlearning, presents a promising avenue to address this concern and is notably advantageous in the context of LLMs. It allows for the removal of harmful knowledge in an efficient manner, without affecting unrelated knowledge in the model. To this end, we provide a survey of knowledge unlearning in the era of LLMs. Firstly, we formally define the knowledge unlearning problem and distinguish it from related works. Subsequently, we categorize existing knowledge unlearning methods into three classes: those based on parameter optimization, parameter merging, and in-context learning, and introduce details of these unlearning methods. We further present evaluation datasets used in existing methods, and finally conclude this survey by presenting the ongoing challenges and future directions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/adam/Zotero/storage/JGQN9BPW/Si et al. - 2023 - Knowledge Unlearning for LLMs Tasks, Methods, and.pdf;/Users/adam/Zotero/storage/HCXRMYYH/2311.html}
}

@incollection{sinnott-armstrongConsequentialism2023,
  title = {Consequentialism},
  booktitle = {The {{Stanford Encyclopedia}} of {{Philosophy}}},
  author = {{Sinnott-Armstrong}, Walter},
  editor = {Zalta, Edward N. and Nodelman, Uri},
  year = 2023,
  edition = {Winter 2023},
  publisher = {Metaphysics Research Lab, Stanford University},
  urldate = {2024-10-06},
  abstract = {Consequentialism, as its name suggests, is simply the view thatnormative properties depend only on consequences. This historicallyimportant and still popular theory embodies the basic intuition thatwhat is best or right is whatever makes the world best in the future,because we cannot change the past, so worrying about the past is nomore useful than crying over spilled milk. This general approach canbe applied at different levels to different normative properties ofdifferent kinds of things, but the most prominent example is probablyconsequentialism about the moral rightness of acts, which holds thatwhether an act is morally right depends only on the consequences ofthat act or of something related to that act, such as the motivebehind the act or a general rule requiring acts of the same kind.},
  keywords = {Bentham Jeremy,consequentialism: rule,consequentializing,hedonism,Mill John Stuart,Moore George Edward,reasons for action: agent-neutral vs. agent-relative,Sidgwick Henry},
  file = {/Users/adam/Zotero/storage/W8HWXIL8/consequentialism.html}
}

@article{smijsTitaniumDioxideZinc2011,
  title = {Titanium Dioxide and Zinc Oxide Nanoparticles in Sunscreens: Focus on Their Safety and Effectiveness},
  shorttitle = {Titanium Dioxide and Zinc Oxide Nanoparticles in Sunscreens},
  author = {Smijs, Threes G and Pavel, Stanislav},
  year = 2011,
  month = oct,
  journal = {Nanotechnology, Science and Applications},
  volume = {4},
  pages = {95--112},
  issn = {1177-8903},
  doi = {10.2147/NSA.S19419},
  urldate = {2022-08-02},
  abstract = {Sunscreens are used to provide protection against adverse effects of ultraviolet (UV)B (290--320 nm) and UVA (320--400 nm) radiation. According to the United States Food and Drug Administration, the protection factor against UVA should be at least one-third of the overall sun protection factor. Titanium dioxide (TiO2) and zinc oxide (ZnO) minerals are frequently employed in sunscreens as inorganic physical sun blockers. As TiO2 is more effective in UVB and ZnO in the UVA range, the combination of these particles assures a broad-band UV protection. However, to solve the cosmetic drawback of these opaque sunscreens, microsized TiO2 and ZnO have been increasingly replaced by TiO2 and ZnO nanoparticles (NPs) ({$<$}100 nm). This review focuses on significant effects on the UV attenuation of sunscreens when microsized TiO2 and ZnO particles are replaced by NPs and evaluates physicochemical aspects that affect effectiveness and safety of NP sunscreens. With the use of TiO2 and ZnO NPs, the undesired opaqueness disappears but the required balance between UVA and UVB protection can be altered. Utilization of mixtures of micro- and nanosized ZnO dispersions and nanosized TiO2 particles may improve this situation. Skin exposure to NP-containing sunscreens leads to incorporation of TiO2 and ZnO NPs in the stratum corneum, which can alter specific NP attenuation properties due to particle--particle, particle--skin, and skin--particle--light physicochemical interactions. Both sunscreen NPs induce (photo)cyto- and genotoxicity and have been sporadically observed in viable skin layers especially in case of long-term exposures and ZnO. Photocatalytic effects, the highest for anatase TiO2, cannot be completely prevented by coating of the particles, but silica-based coatings are most effective. Caution should still be exercised when new sunscreens are developed and research that includes sunscreen NP stabilization, chronic exposures, and reduction of NPs' free-radical production should receive full attention.,},
  pmcid = {PMC3781714},
  pmid = {24198489},
  file = {/Users/adam/Zotero/storage/NVWVT3H3/Smijs and Pavel - 2011 - Titanium dioxide and zinc oxide nanoparticles in s.pdf}
}

@misc{SocialEatingConnects2017,
  title = {Social Eating Connects Communities \textbar{} {{University}} of {{Oxford}}},
  year = 2017,
  month = mar,
  urldate = {2023-04-23},
  abstract = {Research has revealed that the more often people eat with others the more likely they are to feel happy and satisfied with their lives.},
  howpublished = {https://www.ox.ac.uk/news/2017-03-16-social-eating-connects-communities},
  langid = {english},
  file = {/Users/adam/Zotero/storage/WMINP4HI/2017-03-16-social-eating-connects-communities.html}
}

@misc{SocialNormsEtiquette,
  title = {Social Norms and Etiquette in {{Qatar}}},
  journal = {Expatica Qatar},
  urldate = {2023-04-19},
  abstract = {Learn how to greet like a local, be the perfect house guest, and much more with our guide to the culture and social etiquette in Qatar.},
  howpublished = {https://www.expatica.com/qa/living/integration/culture-and-social-etiquette-in-qatar-70936/},
  langid = {american},
  file = {/Users/adam/Zotero/storage/LCS46HQM/culture-and-social-etiquette-in-qatar-70936.html}
}

@misc{SoftwareEngineerArchitect,
  title = {Software {{Engineer}} \& {{Architect}} - 5+ Years Experience Required - {{Remote}} - {{Indeed}}.Com},
  urldate = {2022-07-15},
  abstract = {Radian Generation},
  howpublished = {https://www.indeed.com/viewjob?from=socialmediatagsbigj\&jk=b079dab2b825c097},
  langid = {english},
  file = {/Users/adam/Zotero/storage/NAC784VV/viewjob.html}
}

@misc{SoftwareEngineerEntry,
  title = {Software {{Engineer Entry}}/{{Mid-Level}} - {{Herndon}}, {{VA}} 20170 - {{Indeed}}.Com},
  urldate = {2022-07-15},
  abstract = {Optimal Satcom Inc},
  howpublished = {https://www.indeed.com/viewjob?from=socialmediatagsbigj\&jk=b0cfa432721bb086},
  langid = {english},
  file = {/Users/adam/Zotero/storage/KINM4AA5/viewjob.html}
}

@inproceedings{songLLMPlannerFewShotGrounded2023,
  title = {{{LLM-Planner}}: {{Few-Shot Grounded Planning}} for {{Embodied Agents}} with {{Large Language Models}}},
  shorttitle = {{{LLM-Planner}}},
  booktitle = {2023 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Song, Chan Hee and Sadler, Brian M. and Wu, Jiaman and Chao, Wei-Lun and Washington, Clayton and Su, Yu},
  year = 2023,
  month = oct,
  pages = {2986--2997},
  publisher = {IEEE},
  address = {Paris, France},
  doi = {10.1109/ICCV51070.2023.00280},
  urldate = {2024-07-21},
  copyright = {https://doi.org/10.15223/policy-029},
  isbn = {979-8-3503-0718-4},
  langid = {english},
  file = {/Users/adam/Zotero/storage/BFRF56RA/Song et al. - 2023 - LLM-Planner Few-Shot Grounded Planning for Embodi.pdf}
}

@misc{SonyWH1000XM4Premium,
  title = {Sony {{WH1000XM4}}/{{B Premium Noise Cancelling Wireless Over-The-Ear Headphones}}, {{Black}}},
  journal = {Sony Electronics},
  urldate = {2024-10-27},
  abstract = {Sony WH-1000XM4 Wireless Noise Canceling Over-the-Ear Headphone in black. Wireless Bluetooth streaming and Smartphone compatibility for hands-free.},
  howpublished = {https://electronics.sony.com/audio/headphones/headband/p/wh1000xm4-b},
  langid = {english},
  file = {/Users/adam/Zotero/storage/ENRNKKHT/wh1000xm4-b.html}
}

@misc{speedwaylabsPelicantEatCapybara2021,
  title = {Pelican't Eat a {{Capybara}}},
  author = {{Speedway Labs}},
  year = 2021,
  month = jan,
  urldate = {2024-07-06},
  abstract = {Subscribe if you like racing games}
}

@article{staffJapansAgeingPopulation2023,
  title = {Japan's Ageing Population Poses Urgent Risk to Society, Says {{PM}}},
  author = {Staff},
  year = 2023,
  month = jan,
  journal = {The Guardian},
  issn = {0261-3077},
  urldate = {2023-04-08},
  abstract = {Fumio Kishida says country may be unable to function if birthrate does not rise},
  chapter = {World news},
  langid = {british},
  keywords = {Asia Pacific,Japan,Population,World news},
  file = {/Users/adam/Zotero/storage/PKAGKITN/japans-ageing-population-poses-urgent-risk-to-society-says-pm.html}
}

@misc{stasyan117RusskiyFlagkartaAvstralii2015,
  title = {\cyrchar\CYRR\cyrchar\cyru\cyrchar\cyrs\cyrchar\cyrs\cyrchar\cyrk\cyrchar\cyri\cyrchar\cyrishrt :  {{\cyrchar\CYRF\cyrchar\cyrl\cyrchar\cyra\cyrchar\cyrg -\cyrchar\cyrk\cyrchar\cyra\cyrchar\cyrr\cyrchar\cyrt\cyrchar\cyra{} \cyrchar\CYRA\cyrchar\cyrv\cyrchar\cyrs\cyrchar\cyrt\cyrchar\cyrr\cyrchar\cyra\cyrchar\cyrl\cyrchar\cyri\cyrchar\cyri}}},
  shorttitle = {\cyrchar\CYRR\cyrchar\cyru\cyrchar\cyrs\cyrchar\cyrs\cyrchar\cyrk\cyrchar\cyri\cyrchar\cyrishrt},
  author = {{Stasyan117}},
  year = 2015,
  month = jan,
  urldate = {2023-05-02},
  file = {/Users/adam/Zotero/storage/DLAIC7NE/FileFlag-map_of_Australia.html}
}

@misc{statisticsStatisticalLanguageQuantitative,
  title = {Statistical {{Language}} - {{Quantitative}} and {{Qualitative Data}}},
  author = {of Statistics, c=AU; o=Commonwealth of Australia; ou=Australian Bureau},
  publisher = {c=AU; o=Commonwealth of Australia; ou=Australian Bureau of Statistics},
  urldate = {2022-07-21},
  abstract = {Statistical Language},
  copyright = {\copyright{} Commonwealth of Australia, 2020},
  howpublished = {https://www.abs.gov.au/websitedbs/D3310114.nsf/Home/Statistical+Language+-+quantitative+and+qualitative+data},
  langid = {english},
  file = {/Users/adam/Zotero/storage/94TZ56SE/Statistical+Language+-+quantitative+and+qualitative+data.html}
}

@misc{statisticsStatisticalLanguageQuantitativea,
  title = {Statistical {{Language}} - {{Quantitative}} and {{Qualitative Data}}},
  author = {of Statistics, c=AU; o=Commonwealth of Australia; ou=Australian Bureau},
  publisher = {c=AU; o=Commonwealth of Australia; ou=Australian Bureau of Statistics},
  urldate = {2022-07-21},
  abstract = {Statistical Language},
  copyright = {\copyright{} Commonwealth of Australia, 2020},
  howpublished = {https://www.abs.gov.au/websitedbs/D3310114.nsf/Home/Statistical+Language+-+quantitative+and+qualitative+data},
  langid = {english},
  file = {/Users/adam/Zotero/storage/6ZPWSKKA/Statistical+Language+-+quantitative+and+qualitative+data.html}
}

@misc{sto-lichnyyona-nas-topicCapybara2023,
  title = {Capybara},
  author = {{\cyrchar\CYRS\cyrchar\cyrt\cyrchar\cyro -\cyrchar\CYRL\cyrchar\cyri\cyrchar\cyrch\cyrchar\cyrn\cyrchar\cyrery\cyrchar\cyrishrt{} \cyrchar\CYRO\cyrchar\cyrn\cyrchar\cyra -\cyrchar\CYRN\cyrchar\cyra\cyrchar\cyrs{} - Topic}},
  year = 2023,
  month = jan,
  urldate = {2024-07-08},
  abstract = {Provided to YouTube by SAVAGE\$TATION Capybara {$\cdot$} \cyrchar\CYRS\cyrchar\cyrt\cyrchar\cyro -\cyrchar\CYRL\cyrchar\cyri\cyrchar\cyrch\cyrchar\cyrn\cyrchar\cyrery\cyrchar\cyrishrt{} \cyrchar\CYRO\cyrchar\cyrn\cyrchar\cyra -\cyrchar\CYRN\cyrchar\cyra\cyrchar\cyrs{} {$\cdot$} Betsy {$\cdot$} Corty {$\cdot$} \cyrchar\CYRP\cyrchar\cyrl\cyrchar\cyru\cyrchar\cyrzh\cyrchar\cyrn\cyrchar\cyri\cyrchar\cyrk\cyrchar\cyro\cyrchar\cyrv{} \cyrchar\CYRA\cyrchar\cyrl\cyrchar\cyre\cyrchar\cyrk\cyrchar\cyrs\cyrchar\cyre\cyrchar\cyrishrt{} \cyrchar\CYRYU\cyrchar\cyrr\cyrchar\cyrsftsn\cyrchar\cyre\cyrchar\cyrv\cyrchar\cyri\cyrchar\cyrch{} Capybara \textcircledP{} SAVAGE\$TATION Released on: 2022-07-16 Auto-generated by YouTube.}
}

@misc{stoffelbauerHowLargeLanguage2023,
  title = {How {{Large Language Models Work}}},
  author = {St{\"o}ffelbauer, Andreas},
  year = 2023,
  month = oct,
  journal = {Data Science at Microsoft},
  urldate = {2024-11-26},
  abstract = {From zero to ChatGPT},
  langid = {english},
  file = {/Users/adam/Zotero/storage/MXH82JB8/how-large-language-models-work-91c362f5b78f.html}
}

@misc{StrategicResearchAreas,
  title = {Strategic {{Research Areas}} \textbar{} {{Electrical}} and {{Computer Engineering}}},
  urldate = {2022-07-16},
  howpublished = {https://www.ece.cornell.edu/ece/research/strategic-research-areas},
  file = {/Users/adam/Zotero/storage/TRMDRM7I/strategic-research-areas.html}
}

@article{svabThermalDesorptionChemical2019,
  title = {Thermal Desorption of Chemical Warfare Agents Surrogate from Polluted Materials: From Laboratory to Pilot Scale},
  author = {Svab, M and Masin, P and Krouzek, J and Durdak, V and Kubinova, P},
  year = 2019,
  month = oct,
  journal = {INTERNATIONAL JOURNAL OF ENVIRONMENTAL SCIENCE AND TECHNOLOGY},
  volume = {16},
  number = {10},
  pages = {5917--5926},
  issn = {1735-1472},
  doi = {10.1007/s13762-019-02331-5},
  abstract = {The persistent threat of attack by chemical warfare agents should lead security and rescue services both to avoiding or minimising their serious acute impacts and rapidly and safely eliminating hazardous contaminated material. Low-volatile chemical warfare agents can be treated by thermal desorption that is efficient, versatile and easily available technology for solid waste remediation. We studied the application of thermal desorption technology, using diethyl phthalate as an appropriate chemical warfare agents surrogate. The conventional concept of indirectly heated material by thermal conduction was extended by innovative microwave heating in this study. In laboratory tests, the efficient desorption temperature was evaluated for six different spiked matrices. In addition, the technology using both heating approaches was verified in developed pilot-scale apparatuses for the treatment of several tens of kg of two material samples. For the diethyl phthalate removal, the mild conditions of 250 degrees C temperature were efficient in all experiments, with the temperature being a driving parameter for desorption. We observed insignificant differences in removal efficiency in various matrices or with differently applied heating methods; all residual concentrations were less than the detection limit. The achieved results confirmed the high potential of thermal desorption technology implementation in handling material contaminated by chemical warfare agents.},
  langid = {english},
  keywords = {Chemical warfare agents,Decontamination,DECONTAMINATION,DESIGN,Diethyl phthalate,REMOVAL,SIMULANTS,SOIL REMEDIATION,Solid waste,Surrogates,TECHNOLOGY,Thermal desorption}
}

@article{svabThermalDesorptionChemical2019a,
  title = {Thermal Desorption of Chemical Warfare Agents Surrogate from Polluted Materials: From Laboratory to Pilot Scale},
  author = {Svab, M and Masin, P and Krouzek, J and Durdak, V and Kubinova, P},
  year = 2019,
  month = oct,
  journal = {INTERNATIONAL JOURNAL OF ENVIRONMENTAL SCIENCE AND TECHNOLOGY},
  volume = {16},
  number = {10},
  pages = {5917--5926},
  issn = {1735-1472},
  doi = {10.1007/s13762-019-02331-5},
  abstract = {The persistent threat of attack by chemical warfare agents should lead security and rescue services both to avoiding or minimising their serious acute impacts and rapidly and safely eliminating hazardous contaminated material. Low-volatile chemical warfare agents can be treated by thermal desorption that is efficient, versatile and easily available technology for solid waste remediation. We studied the application of thermal desorption technology, using diethyl phthalate as an appropriate chemical warfare agents surrogate. The conventional concept of indirectly heated material by thermal conduction was extended by innovative microwave heating in this study. In laboratory tests, the efficient desorption temperature was evaluated for six different spiked matrices. In addition, the technology using both heating approaches was verified in developed pilot-scale apparatuses for the treatment of several tens of kg of two material samples. For the diethyl phthalate removal, the mild conditions of 250 degrees C temperature were efficient in all experiments, with the temperature being a driving parameter for desorption. We observed insignificant differences in removal efficiency in various matrices or with differently applied heating methods; all residual concentrations were less than the detection limit. The achieved results confirmed the high potential of thermal desorption technology implementation in handling material contaminated by chemical warfare agents.},
  langid = {english},
  keywords = {Chemical warfare agents,Decontamination,DECONTAMINATION,DESIGN,Diethyl phthalate,REMOVAL,SIMULANTS,SOIL REMEDIATION,Solid waste,Surrogates,TECHNOLOGY,Thermal desorption}
}

@incollection{taguetChapter16Rheological2020,
  title = {Chapter 16 - {{Rheological}} Characterization of Compatibilized Polymer Blends},
  booktitle = {Compatibilization of {{Polymer Blends}}},
  author = {Taguet, Aur{\'e}lie},
  editor = {A.R., Ajitha and Thomas, Sabu},
  year = 2020,
  month = jan,
  pages = {453--487},
  publisher = {Elsevier},
  doi = {10.1016/B978-0-12-816006-0.00016-5},
  abstract = {As the addition of a compatibilizer into a polymer blend dramatically influences the rheology of the system, rheological measurements can be performed to investigate the efficiency of the compatibilizer. This compatibilizer can be a copolymer (either inert such as block copolymers or reactive) or a filler. The rheological behavior of those three different routes of compatibilization is described. One of the main fingerprints of the compatibilization in rheology is the low-frequency modulus. A maximum of G{$\prime$} at low frequency allows to isolate the optimum effectiveness of the compatibilizer. The relaxation spectra H({$\lambda$}) versus {$\lambda$} are commonly plotted to identify a third relaxation assigned to the interface between the two polymeric phases. Palierne model is used to evidence the decrease of interfacial tension when adding the compatibilizer. When nanofillers are used as compatibilizers, varying the localization of nanoparticles (NPs) allows for the enhancement of the final properties of the ternary nanocomposite and dynamic rheological measurements can get insight into the localization of the NPs in the blend.},
  isbn = {978-0-12-816006-0},
  keywords = {Block copolymer,Dynamic shearing,Nanofillers Janus nanoparticles,Reactive copolymer,Steady state shearing}
}

@article{takeoJapansPopulationProblem2019,
  title = {Japan's {{Population Problem Is Straining Its Economy}}. {{The World Is Watching}} for a {{Solution}}},
  author = {Takeo, Y},
  year = 2019,
  month = sep,
  journal = {Bloomberg.com},
  urldate = {2023-04-08},
  abstract = {The government is struggling to entice people to live in rural areas.},
  langid = {english},
  keywords = {aging population,akita,japan,population,population decline,rural,rural economy,Shinzo Abe,tokyo,urban,urban economy,urban rural population},
  file = {/Users/adam/Zotero/storage/THECVQEJ/2019-japan-economy-aging-population.html}
}

@article{tanakaAgingRuralJapan2010,
  title = {Aging in {{Rural Japan}}---{{Limitations}} in the {{Current Social Care Policy}}},
  author = {Tanaka, Kimiko and Iwasawa, Miho},
  year = 2010,
  month = oct,
  journal = {Journal of aging \& social policy},
  volume = {22},
  number = {4},
  pages = {394--406},
  issn = {0895-9420},
  doi = {10.1080/08959420.2010.507651},
  urldate = {2023-04-08},
  abstract = {Owing to equal and increased opportunities for education and employment, today's trend in Japanese marriages is characterized by late and less frequent marriage. This paper discusses unavoidable diversity in rural families to point out the anticipated consequences of aging in rural areas and to discuss limitations in current public social care policies. Specifically, the averaged proportion of never married and singles at ages 45--49 and 50--54 in legally recognized depopulated cities, towns, and villages in Japan is calculated to illustrate the expected diversity in families in rural depopulated areas. It also illustrates the need for future studies to develop better social care policies for increasing numbers of single caregivers and single elders.},
  pmcid = {PMC2951623},
  pmid = {20924894},
  file = {/Users/adam/Zotero/storage/UY7GUT8L/Tanaka and Iwasawa - 2010 - Aging in Rural JapanLimitations in the Current So.pdf}
}

@article{tatarCOVID19VaccineInequality2022,
  title = {{{COVID-19}} Vaccine Inequality: {{A}} Global Perspective.},
  author = {Tatar, Moosa and Shoorekchali, Jalal Montazeri and Faraji, Mohammad Reza and Seyyedkolaee, Mohammad Abdi and Pag{\'a}n, Jos{\'e} A. and Wilson, Fernando A.},
  year = 2022,
  month = oct,
  journal = {Journal of global health},
  volume = {12},
  pages = {03072},
  address = {Scotland},
  issn = {2047-2986 2047-2978},
  doi = {10.7189/jogh.12.03072},
  langid = {english},
  pmcid = {PMC9559176},
  pmid = {36227706},
  keywords = {*COVID-19 Vaccines,*COVID-19/prevention & control,Global Health,Humans,Socioeconomic Factors}
}

@misc{TCPUDPVulnerabilities2020,
  title = {{$\rhd$} {{TCP}} and {{UDP Vulnerabilities}} >> {{CCNA}} 200-301},
  year = 2020,
  month = sep,
  urldate = {2023-10-31},
  abstract = {\textbar\ding{232} This topic explain how TCP and UDP vulnerabilities are exploited by threat actors. Start learning CCNA 200-301 for free right now!! },
  chapter = {CCNA 3},
  howpublished = {https://ccna-200-301.online/tcp-and-udp-vulnerabilities/},
  langid = {american},
  file = {/Users/adam/Zotero/storage/D7YHK7X8/tcp-and-udp-vulnerabilities.html}
}

@misc{TechnicalWriting,
  title = {Technical {{Writing}}},
  journal = {Google for Developers},
  urldate = {2024-09-29},
  abstract = {Technical Writing Courses for Engineers},
  howpublished = {https://developers.google.com/tech-writing},
  langid = {english},
  file = {/Users/adam/Zotero/storage/VETEQ2DX/tech-writing.html}
}

@misc{technologyEducationSystemOverview,
  title = {Education System Overview},
  author = {Technology, Elcom},
  publisher = {Elcom Technology},
  urldate = {2023-04-19},
  abstract = {The Australian education system offers primary, secondary and tertiary education linked across the country and across the world.},
  howpublished = {https://www.studyaustralia.gov.au/english/study/education-system/australian-education-system},
  langid = {english},
  file = {/Users/adam/Zotero/storage/ADVHSCXX/education-system.html}
}

@misc{technologyEducationSystemOverviewa,
  title = {Education System Overview},
  author = {Technology, Elcom},
  publisher = {Elcom Technology},
  urldate = {2023-04-19},
  abstract = {The Australian education system offers primary, secondary and tertiary education linked across the country and across the world.},
  howpublished = {https://www.studyaustralia.gov.au/english/study/education-system/australian-education-system},
  langid = {english},
  file = {/Users/adam/Zotero/storage/EGS8SA2Q/education-system.html}
}

@incollection{teixeira32CoirFiber2022,
  title = {32 - {{Coir}} Fiber as Reinforcement in Cement-Based Materials},
  booktitle = {Advances in {{Bio-Based Fiber}}},
  author = {Teixeira, Ronaldo Soares and Bufalino, Lina and Denzin Tonoli, Gustavo Henrique and {dos Santos}, Sergio Francisco and Savastano Junior, Holmer},
  editor = {Rangappa, Sanjay Mavinkere and Puttegowda, Madhu and Parameswaranpillai, Jyotishkumar and Siengchin, Suchart and Gorbatyuk, Sergey},
  year = 2022,
  month = jan,
  pages = {707--739},
  publisher = {Woodhead Publishing},
  doi = {10.1016/B978-0-12-824543-9.00008-6},
  abstract = {The disposal of coconut waste in many coastal habitats worldwide is an enormous environmental problem. A valorization alternative suggests coir fibers as attractive reinforcements of many kinds of cement-based composites. The features of the raw or enhanced fibers, the compatibility between the matrix and the reinforcement phases, and the production parameters are critical for developing high-performance composites. Coir technological fibers consist of fiber bundles, phloem, and parenchyma cells. The relatively high lignin level (27\%--35\%) provides coir fibers great weather, fungal and bacterial resistance, and elongation at break. It has natural compatibility with cement because of relatively low levels of soluble sugars. However, water or NaOH soaking pretreatments of coir fibers release soluble sugars that hinder cement hydration, keep cellulose integrity, enhance fiber morphology, and improve chemical compatibility and interface between the matrix and reinforcement. Pressed coir-reinforced cemented-bonded fiberboards and wool boards exhibit remarkable physical strength compared to commercial products and based on quality standards. Still, the production process demands adjustments to meet mechanical strength requirements. Extruded composites reinforced with 1\% and 2\% of coir fibers were compared to neat cement regarding rheological, mechanical, and physical properties. The 2\% coir fiber reinforcement exhibited ram pressure 38\% higher compared to the neat cement paste. Accelerate aging by 200 cycles improved the modulus of rupture and water absorption of the extruded 2\% reinforced composites because of the effective cement hydration, which filled the pores and densified the material structure, improving the fiber/matrix ratio.},
  isbn = {978-0-12-824543-9},
  keywords = {durability,extrusion process,mechanical properties,physical properties,Rheological test}
}

@unpublished{thakerGuardrailBaselinesUnlearning2024,
  title = {Guardrail {{Baselines}} for {{Unlearning}} in {{LLMs}}},
  author = {Thaker, Pratiksha and Maurya, Yash and Hu, Shengyuan and Wu, Zhiwei Steven and Smith, Virginia},
  year = 2024,
  month = mar,
  journal = {arXiv [cs.CL]},
  abstract = {Recent work has demonstrated that finetuning is a promising approach to 'unlearn' concepts from large language models. However, finetuning can be expensive, as it requires both generating a set of examples and running iterations of finetuning to update the model. In this work, we show that simple guardrail-based approaches such as prompting and filtering can achieve unlearning results comparable to finetuning. We recommend that researchers investigate these lightweight baselines when evaluating the performance of more computationally intensive finetuning methods. While we do not claim that methods such as prompting or filtering are universal solutions to the problem of unlearning, our work suggests the need for evaluation metrics that can better separate the power of guardrails vs. finetuning, and highlights scenarios where guardrails expose possible unintended behavior in existing metrics and benchmarks.},
  isbn = {2403.03329}
}

@misc{thakerPositionLLMUnlearning2024,
  title = {Position: {{LLM Unlearning Benchmarks}} Are {{Weak Measures}} of {{Progress}}},
  shorttitle = {Position},
  author = {Thaker, Pratiksha and Hu, Shengyuan and Kale, Neil and Maurya, Yash and Wu, Zhiwei Steven and Smith, Virginia},
  year = 2024,
  month = oct,
  number = {arXiv:2410.02879},
  eprint = {2410.02879},
  publisher = {arXiv},
  urldate = {2024-10-15},
  abstract = {Unlearning methods have the potential to improve the privacy and safety of large language models (LLMs) by removing sensitive or harmful information post hoc. The LLM unlearning research community has increasingly turned toward empirical benchmarks to assess the effectiveness of such methods. In this paper, we find that existing benchmarks provide an overly optimistic and potentially misleading view on the effectiveness of candidate unlearning methods. By introducing simple, benign modifications to a number of popular benchmarks, we expose instances where supposedly unlearned information remains accessible, or where the unlearning process has degraded the model's performance on retained information to a much greater extent than indicated by the original benchmark. We identify that existing benchmarks are particularly vulnerable to modifications that introduce even loose dependencies between the forget and retain information. Further, we show that ambiguity in unlearning targets in existing benchmarks can easily lead to the design of methods that overfit to the given test queries. Based on our findings, we urge the community to be cautious when interpreting benchmark results as reliable measures of progress, and we provide several recommendations to guide future LLM unlearning research.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/adam/Zotero/storage/VZJ9FMJC/Thaker et al. - 2024 - Position LLM Unlearning Benchmarks are Weak Measu.pdf}
}

@misc{thakerPositionLLMUnlearning2024a,
  title = {Position: {{LLM Unlearning Benchmarks}} Are {{Weak Measures}} of {{Progress}}},
  shorttitle = {Position},
  author = {Thaker, Pratiksha and Hu, Shengyuan and Kale, Neil and Maurya, Yash and Wu, Zhiwei Steven and Smith, Virginia},
  year = 2024,
  month = oct,
  number = {arXiv:2410.02879},
  eprint = {2410.02879},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2410.02879},
  urldate = {2024-10-15},
  abstract = {Unlearning methods have the potential to improve the privacy and safety of large language models (LLMs) by removing sensitive or harmful information post hoc. The LLM unlearning research community has increasingly turned toward empirical benchmarks to assess the effectiveness of such methods. In this paper, we find that existing benchmarks provide an overly optimistic and potentially misleading view on the effectiveness of candidate unlearning methods. By introducing simple, benign modifications to a number of popular benchmarks, we expose instances where supposedly unlearned information remains accessible, or where the unlearning process has degraded the model's performance on retained information to a much greater extent than indicated by the original benchmark. We identify that existing benchmarks are particularly vulnerable to modifications that introduce even loose dependencies between the forget and retain information. Further, we show that ambiguity in unlearning targets in existing benchmarks can easily lead to the design of methods that overfit to the given test queries. Based on our findings, we urge the community to be cautious when interpreting benchmark results as reliable measures of progress, and we provide several recommendations to guide future LLM unlearning research.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language}
}

@misc{theroyaldonPicsAmericanBlue2022,
  type = {Reddit {{Post}}},
  title = {[4 Pics] {{American Blue Rabbits}}},
  author = {TheRoyalDon},
  year = 2022,
  month = jan,
  journal = {r/Bunnies},
  urldate = {2024-07-17},
  file = {/Users/adam/Zotero/storage/MNWV8UL3/4_pics_american_blue_rabbits.html}
}

@misc{TitaniumDioxideUse2014,
  title = {Titanium {{Dioxide}} \textbar{} {{Use}}, {{Benefits}}, and {{Chemical Safety Facts}}},
  year = 2014,
  month = jul,
  journal = {ChemicalSafetyFacts.org},
  urldate = {2022-08-02},
  abstract = {Titanium dioxide, popular for its part in producing sunscreen, is a versatile chemical involved in producing everything from adhesives to paint and more.},
  langid = {american}
}

@misc{ToolsIEEEAuthors,
  title = {Tools for {{IEEE Authors}}},
  journal = {IEEE Author Center Journals},
  urldate = {2024-09-29},
  abstract = {Find out how easy it can be to~quickly prepare your article for submission with help from IEEE Author Tools.},
  howpublished = {https://journals.ieeeauthorcenter.ieee.org/create-your-ieee-journal-article/authoring-tools-and-templates/tools-for-ieee-authors/},
  langid = {american},
  file = {/Users/adam/Zotero/storage/G6WSDWR2/tools-for-ieee-authors.html}
}

@misc{TopFiveSimple,
  title = {Top {{Five Simple Ways}} to {{Bring Out}} the {{Best}} in {{Your Team}} \textbar{} {{West Valley Staffing Group}}},
  urldate = {2023-04-26},
  howpublished = {https://www.westvalley.com/resources/employer-resources/top-five-simple-ways-to-bring-out-the-best-in-your-team},
  file = {/Users/adam/Zotero/storage/URXAJXEF/top-five-simple-ways-to-bring-out-the-best-in-your-team.html}
}

@misc{TopologyAlgebraicTopology,
  title = {Topology - {{Algebraic}} Topology \textbar{} {{Britannica}}},
  urldate = {2022-07-26},
  howpublished = {https://www.britannica.com/science/topology/Algebraic-topology},
  file = {/Users/adam/Zotero/storage/X4LC2GYI/Algebraic-topology.html}
}

@misc{TopologyBrilliantMath,
  title = {Topology \textbar{} {{Brilliant Math}} \& {{Science Wiki}}},
  urldate = {2022-07-26},
  abstract = {Topology is the study of properties of geometric spaces which are preserved by continuous deformations (intuitively, stretching, rotating, or bending are continuous deformations; tearing or gluing are not). The theory originated as a way to classify and study properties of shapes in ...},
  howpublished = {https://brilliant.org/wiki/topology/},
  langid = {american},
  file = {/Users/adam/Zotero/storage/XP5NIYHJ/topology.html}
}

@article{tusekModernDecontaminantsChemical2020,
  title = {{Modern Decontaminants for Chemical Warfare Agents: Part II - Metal Oxides}},
  author = {Tusek, D and Cetina, I and Pehar, V},
  year = 2020,
  journal = {KEMIJA U INDUSTRIJI-JOURNAL OF CHEMISTS AND CHEMICAL ENGINEERS},
  volume = {69},
  number = {1-2},
  pages = {25--34},
  issn = {0022-9830},
  doi = {10.15255/KUI.2019.021b},
  abstract = {Metal oxides are nontoxic compounds that are, in their powder form, used as decontaminating agents; they are simple to use and apply, easy to store, and have very high active surface capacity. Metal oxides, especially in nanoform, show excellent physisorption and chemisorption properties towards toxic compounds like chemical warfare agents and their simulants. Although sorption and decontamination properties of many metal oxides are being researched, only magnesium, aluminum, and titanium oxide showed the greatest commercial usability. Some metal oxides in nanoform show destructive adsorption properties, which is very interesting in research of the metal oxides suitable for decontamination of chemical warfare agents and other chemical, biological, and radiological toxic substances. Besides metal oxides, research focus is also on metal cations used as dopants in other metal oxides and zeolites in order to improve sorption properties.},
  langid = {croatian},
  keywords = {ADSORPTION,Chemical warfare agents,DECOMPOSITION,decontamination,DEGRADATION,DETOXIFICATION,DIMETHYL METHYLPHOSPHONATE,ETHYL PHENYL SULFIDE,metal dopant,metal oxides,SARIN,SULFUR MUSTARD,SURFACE,TIO2}
}

@article{tusekModernDecontaminantsChemical2020a,
  title = {{Modern Decontaminants for Chemical Warfare Agents: Part II - Metal Oxides}},
  author = {Tusek, D and Cetina, I and Pehar, V},
  year = 2020,
  journal = {KEMIJA U INDUSTRIJI-JOURNAL OF CHEMISTS AND CHEMICAL ENGINEERS},
  volume = {69},
  number = {1-2},
  pages = {25--34},
  issn = {0022-9830},
  doi = {10.15255/KUI.2019.021b},
  abstract = {Metal oxides are nontoxic compounds that are, in their powder form, used as decontaminating agents; they are simple to use and apply, easy to store, and have very high active surface capacity. Metal oxides, especially in nanoform, show excellent physisorption and chemisorption properties towards toxic compounds like chemical warfare agents and their simulants. Although sorption and decontamination properties of many metal oxides are being researched, only magnesium, aluminum, and titanium oxide showed the greatest commercial usability. Some metal oxides in nanoform show destructive adsorption properties, which is very interesting in research of the metal oxides suitable for decontamination of chemical warfare agents and other chemical, biological, and radiological toxic substances. Besides metal oxides, research focus is also on metal cations used as dopants in other metal oxides and zeolites in order to improve sorption properties.},
  langid = {croatian},
  keywords = {ADSORPTION,Chemical warfare agents,DECOMPOSITION,decontamination,DEGRADATION,DETOXIFICATION,DIMETHYL METHYLPHOSPHONATE,ETHYL PHENYL SULFIDE,metal dopant,metal oxides,SARIN,SULFUR MUSTARD,SURFACE,TIO2}
}

@misc{UndergraduateCatalogAOE,
  title = {Undergraduate {{Catalog--AOE}}},
  urldate = {2022-07-16},
  howpublished = {https://www.undergradcatalog.registrar.vt.edu/0708/eng/aoe.html},
  file = {/Users/adam/Zotero/storage/7KX24TJS/aoe.html}
}

@misc{UndergraduateCatalogMSE,
  title = {Undergraduate {{Catalog-- MSE Course Descriptions}}},
  urldate = {2022-07-16},
  howpublished = {https://www.undergradcatalog.registrar.vt.edu/1011/eng/mse.html},
  file = {/Users/adam/Zotero/storage/SHMCWG7A/mse.html}
}

@misc{UndergraduateCatalogMSEa,
  title = {Undergraduate {{Catalog-- MSE Course Descriptions}}},
  urldate = {2022-07-14},
  howpublished = {https://www.undergradcatalog.registrar.vt.edu/1011/eng/mse.html},
  file = {/Users/adam/Zotero/storage/SYUW6IE5/mse.html}
}

@misc{universityRethinkingLLMMemorization2024,
  title = {Rethinking {{LLM Memorization}}},
  author = {University, Carnegie Mellon, Machine Learning Department},
  year = 2024,
  month = sep,
  journal = {Machine Learning Blog \textbar{} ML@CMU \textbar{} Carnegie Mellon University},
  urldate = {2024-11-26},
  abstract = {Introduction A central question in the discussion of large language models (LLMs) concerns the extent to which they memorize their training data versus how they generalize to new tasks and settings. Most practitioners seem to (at least informally) believe that LLMs do some degree of both: they cl},
  chapter = {machine learning},
  langid = {american},
  file = {/Users/adam/Zotero/storage/EGLC4IIR/rethinking-llm-memorization.html}
}

@misc{unsplashPhotoBrianMcGowan2020,
  title = {Photo by {{Brian McGowan}} on {{Unsplash}}},
  author = {Unsplash},
  year = 2020,
  month = jul,
  urldate = {2024-07-06},
  abstract = {Capybara enjoying a swim on a hot summer day. -- Download this photo by Brian McGowan on Unsplash},
  howpublished = {https://unsplash.com/photos/brown-rodent-on-body-of-water-during-daytime-P1-6ioOcGNU},
  langid = {american},
  file = {/Users/adam/Zotero/storage/UUDX4B2S/brown-rodent-on-body-of-water-during-daytime-P1-6ioOcGNU.html}
}

@misc{unsplashPhotoGoogleDeepMind2024,
  title = {Photo by {{Google DeepMind}} on {{Unsplash}}},
  author = {Unsplash},
  year = 2024,
  month = jun,
  urldate = {2024-12-05},
  abstract = {An artist's illustration of artificial intelligence (AI). This image explores how multimodal models understand a users input and generate an output. It was created by Bakken \& Baeck as part of the Visualising AI project launched by Google DeepMind. -- Download this photo by Google DeepMind on Unsplash},
  howpublished = {https://unsplash.com/photos/a-black-and-white-image-of-a-computer-keyboard-BMn5Z-MGv5c},
  langid = {american}
}

@misc{UserDatagramProtocol,
  title = {User {{Datagram Protocol}} ({{UDP}}) (Article)},
  journal = {Khan Academy},
  urldate = {2023-10-31},
  abstract = {Learn for free about math, art, computer programming, economics, physics, chemistry, biology, medicine, finance, history, and more. Khan Academy is a nonprofit with the mission of providing a free, world-class education for anyone, anywhere.},
  howpublished = {https://www.khanacademy.org/computing/computers-and-internet/xcae6f4a7ff015e7d:the-internet/xcae6f4a7ff015e7d:transporting-packets/a/user-datagram-protocol-udp},
  langid = {english},
  file = {/Users/adam/Zotero/storage/V6B9PJCL/user-datagram-protocol-udp.html}
}

@misc{UserDatagramProtocol2017,
  title = {User {{Datagram Protocol}} ({{UDP}})},
  year = 2017,
  month = sep,
  journal = {GeeksforGeeks},
  urldate = {2023-10-31},
  abstract = {A Computer Science portal for geeks. It contains well written, well thought and well explained computer science and programming articles, quizzes and practice/competitive programming/company interview Questions.},
  chapter = {Computer Networks},
  langid = {american},
  file = {/Users/adam/Zotero/storage/RFLSBC2L/user-datagram-protocol-udp.html}
}

@misc{UtilitarianismActRule,
  title = {Utilitarianism, {{Act}} and {{Rule}} \textbar{} {{Internet Encyclopedia}} of {{Philosophy}}},
  urldate = {2024-09-14},
  langid = {american},
  file = {/Users/adam/Zotero/storage/B6N732ZY/util-a-r.html}
}

@misc{UtilitarianismConsequentialismGM,
  title = {Utilitarianism and {{Consequentialism}} - {{The GM Bailout}}},
  journal = {Seven Pillars Institute},
  urldate = {2024-09-14},
  abstract = {Utilitarianism is a consequentialist moral theory focused on maximizing the overall good; the good of others as well as the good of one's self. The notable thinkers associated with utilitarianism are Jeremy Bentham and John Stuart Mill.},
  howpublished = {https://sevenpillarsinstitute.org/ethics-101/applying-utilitarianism-are-insider-trading-and-the-bailout-of-gm-ethical/},
  langid = {american},
  file = {/Users/adam/Zotero/storage/R53GPJGR/applying-utilitarianism-are-insider-trading-and-the-bailout-of-gm-ethical.html}
}

@article{vandenbogerdGreeningRoomQuasiexperimental2021,
  title = {Greening the Room: {{A}} Quasi-Experimental Study on the Presence of Potted Plants in Study Rooms on Mood, Cognitive Performance, and Perceived Environmental Quality among University Students},
  author = {{van den Bogerd}, Nicole and Dijkstra, S. Coosje and Koole, Sander L. and Seidell, Jacob C. and Maas, Jolanda},
  year = 2021,
  month = feb,
  journal = {Journal of Environmental Psychology},
  volume = {73},
  pages = {101557},
  issn = {0272-4944},
  doi = {10.1016/j.jenvp.2021.101557},
  abstract = {Due to mounting concerns about the psychological well-being of university students, it is useful to consider whether and how the quality of the physical study environment can improve students' functioning. The present study examined the presence of potted plants within a university library study room on students' self-reported mood (i.e., fatigue and vigor), self-reported cognitive performance (i.e., attention and productivity), perceived environmental quality (i.e., room satisfaction, attractiveness, and comfort), and recorded duration of stay in the study room. We conducted a real-life quasi-experimental study in which potted plants were introduced in one study room (intervention group) whereas nothing changed in another study room (control group). Cross-sectional data of the dependent and co-variables were collected among separate groups of students pre- and post-intervention using questionnaires (N~=~445) and recordings of students' duration of stay in the study room (N~=~1029). The pretest-posttest change in attractiveness (B~=~0.53, 95\% CI~=~0.33; 0.72) and comfort (B~=~0.29, 95\% CI~=~0.08; 0.51) was greater in the study room with potted plants than the pretest-posttest change in attractiveness and comfort in the study room without plants. Students' reasons to study in the room with potted plants next time they study included the perceived environmental quality, atmosphere, it being more relaxing, the homey feel, and indoor climate. Nevertheless, the pretest-posttest change in vigor (B~=~0.29, 95\% CI~=~-0.57; -0.01) was lower in the room with potted plants than the pretest-posttest change in vigor in the study room without plants, and no meaningful associations between the presence of potted plants and fatigue, cognitive performance, and duration of stay in the study room were found. Overall, findings suggest that students preferred the study room with potted plants to the one without. However, the findings do not support the hypothesis that adding potted plants to a study room improves mood or cognitive performance among students.},
  keywords = {Interior plants,Nature-based intervention,Restorative environments,University students}
}

@article{vandenbogerdGreeningRoomQuasiexperimental2021a,
  title = {Greening the Room: {{A}} Quasi-Experimental Study on the Presence of Potted Plants in Study Rooms on Mood, Cognitive Performance, and Perceived Environmental Quality among University Students},
  author = {{van den Bogerd}, Nicole and Dijkstra, S. Coosje and Koole, Sander L. and Seidell, Jacob C. and Maas, Jolanda},
  year = 2021,
  month = feb,
  journal = {Journal of Environmental Psychology},
  volume = {73},
  pages = {101557},
  issn = {0272-4944},
  doi = {10.1016/j.jenvp.2021.101557},
  abstract = {Due to mounting concerns about the psychological well-being of university students, it is useful to consider whether and how the quality of the physical study environment can improve students' functioning. The present study examined the presence of potted plants within a university library study room on students' self-reported mood (i.e., fatigue and vigor), self-reported cognitive performance (i.e., attention and productivity), perceived environmental quality (i.e., room satisfaction, attractiveness, and comfort), and recorded duration of stay in the study room. We conducted a real-life quasi-experimental study in which potted plants were introduced in one study room (intervention group) whereas nothing changed in another study room (control group). Cross-sectional data of the dependent and co-variables were collected among separate groups of students pre- and post-intervention using questionnaires (N~=~445) and recordings of students' duration of stay in the study room (N~=~1029). The pretest-posttest change in attractiveness (B~=~0.53, 95\% CI~=~0.33; 0.72) and comfort (B~=~0.29, 95\% CI~=~0.08; 0.51) was greater in the study room with potted plants than the pretest-posttest change in attractiveness and comfort in the study room without plants. Students' reasons to study in the room with potted plants next time they study included the perceived environmental quality, atmosphere, it being more relaxing, the homey feel, and indoor climate. Nevertheless, the pretest-posttest change in vigor (B~=~0.29, 95\% CI~=~-0.57; -0.01) was lower in the room with potted plants than the pretest-posttest change in vigor in the study room without plants, and no meaningful associations between the presence of potted plants and fatigue, cognitive performance, and duration of stay in the study room were found. Overall, findings suggest that students preferred the study room with potted plants to the one without. However, the findings do not support the hypothesis that adding potted plants to a study room improves mood or cognitive performance among students.},
  keywords = {Interior plants,Nature-based intervention,Restorative environments,University students}
}

@misc{VerlprojectVerl2026,
  title = {Verl-Project/Verl},
  year = 2026,
  month = feb,
  urldate = {2026-02-21},
  abstract = {verl: Volcano Engine Reinforcement Learning for LLMs},
  copyright = {Apache-2.0},
  howpublished = {verl-project}
}

@misc{VisibleLightScience,
  title = {Visible {{Light}} \textbar{} {{Science Mission Directorate}}},
  urldate = {2022-08-05},
  howpublished = {https://science.nasa.gov/ems/09\_visiblelight},
  file = {/Users/adam/Zotero/storage/V4MPPUYW/09_visiblelight.html}
}

@misc{VTComputerScience,
  title = {{{VT Computer Science}} - {{Courses}}},
  urldate = {2022-07-16},
  howpublished = {https://courses.cs.vt.edu/cs3114/},
  file = {/Users/adam/Zotero/storage/7LNKA6M7/cs3114.html}
}

@misc{wanCYBERSECEVAL3Advancing2024,
  title = {{{CYBERSECEVAL}} 3: {{Advancing}} the {{Evaluation}} of {{Cybersecurity Risks}} and {{Capabilities}} in {{Large Language Models}}},
  shorttitle = {{{CYBERSECEVAL}} 3},
  author = {Wan, Shengye and Nikolaidis, Cyrus and Song, Daniel and Molnar, David and Crnkovich, James and Grace, Jayson and Bhatt, Manish and Chennabasappa, Sahana and Whitman, Spencer and Ding, Stephanie and Ionescu, Vlad and Li, Yue and Saxe, Joshua},
  year = 2024,
  month = sep,
  number = {arXiv:2408.01605},
  eprint = {2408.01605},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2408.01605},
  urldate = {2024-11-15},
  abstract = {We are releasing a new suite of security benchmarks for LLMs, CYBERSECEVAL 3, to continue the conversation on empirically measuring LLM cybersecurity risks and capabilities. CYBERSECEVAL 3 assesses 8 different risks across two broad categories: risk to third parties, and risk to application developers and end users. Compared to previous work, we add new areas focused on offensive security capabilities: automated social engineering, scaling manual offensive cyber operations, and autonomous offensive cyber operations. In this paper we discuss applying these benchmarks to the Llama 3 models and a suite of contemporaneous state-of-the-art LLMs, enabling us to contextualize risks both with and without mitigations in place.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Machine Learning},
  file = {/Users/adam/Zotero/storage/ERSS5NB3/Wan et al. - 2024 - CYBERSECEVAL 3 Advancing the Evaluation of Cybers.pdf;/Users/adam/Zotero/storage/9NGMZJFN/2408.html}
}

@unpublished{wangKGAGeneralMachine2023,
  title = {{{KGA}}: {{A General Machine Unlearning Framework Based}} on {{Knowledge Gap Alignment}}},
  author = {Wang, Lingzhi and Chen, Tong and Yuan, Wei and Zeng, Xingshan and Wong, Kam-Fai and Yin, Hongzhi},
  year = 2023,
  month = may,
  journal = {arXiv [cs.CL]},
  abstract = {Recent legislation of the "right to be forgotten" has led to the interest in machine unlearning, where the learned models are endowed with the function to forget information about specific training instances as if they have never existed in the training set. Previous work mainly focuses on computer vision scenarios and largely ignores the essentials of unlearning in NLP field, where text data contains more explicit and sensitive personal information than images. In this paper, we propose a general unlearning framework called KGA to induce forgetfulness. Different from previous work that tries to recover gradients or forces models to perform close to one specific distribution, KGA maintains distribution differences (i.e., knowledge gap). This relaxes the distribution assumption. Furthermore, we first apply the unlearning method to various NLP tasks (i.e., classification, translation, response generation) and propose several unlearning evaluation metrics with pertinence. Experiments on large-scale datasets show that KGA yields comprehensive improvements over baselines, where extensive analyses further validate the effectiveness of KGA and provide insight into unlearning for NLP tasks.},
  isbn = {2305.06535}
}

@misc{wangLLMUnlearningLoss2024,
  title = {{{LLM Unlearning}} via {{Loss Adjustment}} with {{Only Forget Data}}},
  author = {Wang, Yaxuan and Wei, Jiaheng and Liu, Chris Yuhao and Pang, Jinlong and Liu, Quan and Shah, Ankit Parag and Bao, Yujia and Liu, Yang and Wei, Wei},
  year = 2024,
  month = oct,
  number = {arXiv:2410.11143},
  eprint = {2410.11143},
  publisher = {arXiv},
  urldate = {2024-10-21},
  abstract = {Unlearning in Large Language Models (LLMs) is essential for ensuring ethical and responsible AI use, especially in addressing privacy leak, bias, safety, and evolving regulations. Existing approaches to LLM unlearning often rely on retain data or a reference LLM, yet they struggle to adequately balance unlearning performance with overall model utility. This challenge arises because leveraging explicit retain data or implicit knowledge of retain data from a reference LLM to fine-tune the model tends to blur the boundaries between the forgotten and retain data, as different queries often elicit similar responses. In this work, we propose eliminating the need to retain data or the reference LLM for response calibration in LLM unlearning. Recognizing that directly applying gradient ascent on the forget data often leads to optimization instability and poor performance, our method guides the LLM on what not to respond to, and importantly, how to respond, based on the forget data. Hence, we introduce Forget data only Loss AjustmenT (FLAT), a "flat" loss adjustment approach which addresses these issues by maximizing f-divergence between the available template answer and the forget answer only w.r.t. the forget data. The variational form of the defined f-divergence theoretically provides a way of loss adjustment by assigning different importance weights for the learning w.r.t. template responses and the forgetting of responses subject to unlearning. Empirical results demonstrate that our approach not only achieves superior unlearning performance compared to existing methods but also minimizes the impact on the model's retained capabilities, ensuring high utility across diverse tasks, including copyrighted content unlearning on Harry Potter dataset and MUSE Benchmark, and entity unlearning on the TOFU dataset.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/adam/Zotero/storage/78T26ZX9/Wang et al. - 2024 - LLM Unlearning via Loss Adjustment with Only Forge.pdf;/Users/adam/Zotero/storage/HH37DJUG/2410.html}
}

@misc{wangPractitionersGuideMultiturn2025,
  title = {A {{Practitioner}}'s {{Guide}} to {{Multi-turn Agentic Reinforcement Learning}}},
  author = {Wang, Ruiyi and Ammanabrolu, Prithviraj},
  year = 2025,
  month = oct,
  journal = {arXiv.org},
  urldate = {2026-02-21},
  abstract = {We study what actually works and what doesn't for training large language models as agents via multi-turn reinforcement learning. Despite rapid progress, existing frameworks and definitions are fragmented, and there is no systematic formulation or analysis of which design choices matter across tasks. We address this gap by first breaking down the design space into three inter-related pillars -- environment, reward, and policy -- and empirically derive a recipe for training LLM agents in situated textual domains. In particular, we test TextWorld and ALFWorld, popular domains for testing situated embodied reasoning, as well as SWE-Gym for more software engineering style tasks. (i) For the environment, we analyze the impacts of task complexity in terms of sizes of the state and action spaces as well as optimal solution length, finding that even simple environments within a domain can provide signal on how well an agent can generalize to more complex tasks. (ii) For the reward, we ablate relative reward sparsity, observing that while dense turn-level rewards accelerate training, performance and stability is highly dependent on the choice of RL algorithm. (iii) And for the agent's policy, we explore the interplay between reward sparsity and biased (PPO, GRPO) and unbiased (RLOO) policy gradient methods in addition to showing how to find the optimal Supervised Fine-tuning (SFT) to RL training ratio given a fixed budget. We distill these findings into a training recipe that guides co-design across the three pillars, facilitating research and practical efforts in multi-turn agentic RL. Code: https://github.com/pearls-lab/meow-tea-taro},
  howpublished = {https://arxiv.org/abs/2510.01132v2},
  langid = {english},
  file = {/Users/adam/Zotero/storage/GVGVK75K/Wang and Ammanabrolu - 2025 - A Practitioner's Guide to Multi-turn Agentic Reinforcement Learning.pdf}
}

@inproceedings{wangReinforcementLearningDrivenLLM2024,
  title = {Reinforcement {{Learning-Driven LLM Agent}} for {{Automated Attacks}} on {{LLMs}}},
  booktitle = {Proceedings of the {{Fifth Workshop}} on {{Privacy}} in {{Natural Language Processing}}},
  author = {Wang, Xiangwen and Peng, Jie and Xu, Kaidi and Yao, Huaxiu and Chen, Tianlong},
  editor = {Habernal, Ivan and Ghanavati, Sepideh and Ravichander, Abhilasha and Jain, Vijayanta and Thaine, Patricia and Igamberdiev, Timour and Mireshghallah, Niloofar and Feyisetan, Oluwaseyi},
  year = 2024,
  month = aug,
  pages = {170--177},
  publisher = {Association for Computational Linguistics},
  address = {Bangkok, Thailand},
  abstract = {Recently, there has been a growing focus on conducting attacks on large language models (LLMs) to assess LLMs' safety. Yet, existing attack methods face challenges, including the need to access model weights or merely ensuring LLMs output harmful information without controlling the specific content of their output. Exactly control of the LLM output can produce more inconspicuous attacks which could reveal a new page for LLM security. To achieve this, we propose RLTA: the Reinforcement Learning Targeted Attack, a framework that is designed for attacking language models (LLMs) and is adaptable to both white box (weight accessible) and black box (weight inaccessible) scenarios. It is capable of automatically generating malicious prompts that trigger target LLMs to produce specific outputs. We demonstrate RLTA in two different scenarios: LLM trojan detection and jailbreaking. The comprehensive experimental results show the potential of RLTA in enhancing the security measures surrounding contemporary LLMs.}
}

@unpublished{wangSelectiveForgettingAdvancing2024,
  title = {Selective {{Forgetting}}: {{Advancing Machine Unlearning Techniques}} and {{Evaluation}} in {{Language Models}}},
  author = {Wang, Lingzhi and Zeng, Xingshan and Guo, Jinsong and Wong, Kam-Fai and Gottlob, Georg},
  year = 2024,
  month = feb,
  journal = {arXiv [cs.CL]},
  abstract = {The aim of this study is to investigate Machine Unlearning (MU), a burgeoning field focused on addressing concerns related to neural models inadvertently retaining personal or sensitive data. Here, a novel approach is introduced to achieve precise and selective forgetting within language models. Unlike previous methodologies that adopt completely opposing training objectives, this approach aims to mitigate adverse effects on language model performance, particularly in generation tasks. Furthermore, two innovative evaluation metrics are proposed: Sensitive Information Extraction Likelihood (S-EL) and Sensitive Information Memory Accuracy (S-MA), designed to gauge the effectiveness of sensitive information elimination. To reinforce the forgetting framework, an effective method for annotating sensitive scopes is presented, involving both online and offline strategies. The online selection mechanism leverages language probability scores to ensure computational efficiency, while the offline annotation entails a robust two-stage process based on Large Language Models (LLMs).},
  isbn = {2402.05813}
}

@misc{wangWhoEarthUsing,
  type = {Text/{{HTML}}},
  title = {Who on {{Earth Is Using Generative AI}} ?},
  author = {Wang, Yan-000529044,He, Liu},
  journal = {World Bank},
  urldate = {2024-11-28},
  abstract = {Who on Earth Is Using Generative AI ? (English)},
  howpublished = {https://documents.worldbank.org/en/publication/documents-reports/documentdetail/099720008192430535/IDU15f321eb5148701472d1a88813ab677be07b0},
  langid = {english},
  file = {/Users/adam/Zotero/storage/LR2WIHDD/idu15f321eb5148701472d1a88813ab677be07b0.html}
}

@article{wellingBayesianLearningStochastic,
  title = {Bayesian {{Learning}} via {{Stochastic Gradient Langevin Dynamics}}},
  author = {Welling, Max and Teh, Yee Whye},
  abstract = {In this paper we propose a new framework for learning from large scale datasets based on iterative learning from small mini-batches. By adding the right amount of noise to a standard stochastic gradient optimization algorithm we show that the iterates will converge to samples from the true posterior distribution as we anneal the stepsize. This seamless transition between optimization and Bayesian posterior sampling provides an inbuilt protection against overfitting. We also propose a practical method for Monte Carlo estimates of posterior statistics which monitors a ``sampling threshold'' and collects samples after it has been surpassed. We apply the method to three models: a mixture of Gaussians, logistic regression and ICA with natural gradients.},
  langid = {english},
  file = {/Users/adam/Zotero/storage/AZG9I2XS/Welling and Teh - Bayesian Learning via Stochastic Gradient Langevin.pdf}
}

@article{wellingBayesianLearningStochastica,
  title = {Bayesian {{Learning}} via {{Stochastic Gradient Langevin Dynamics}}},
  author = {Welling, Max and Teh, Yee Whye},
  abstract = {In this paper we propose a new framework for learning from large scale datasets based on iterative learning from small mini-batches. By adding the right amount of noise to a standard stochastic gradient optimization algorithm we show that the iterates will converge to samples from the true posterior distribution as we anneal the stepsize. This seamless transition between optimization and Bayesian posterior sampling provides an inbuilt protection against overfitting. We also propose a practical method for Monte Carlo estimates of posterior statistics which monitors a ``sampling threshold'' and collects samples after it has been surpassed. We apply the method to three models: a mixture of Gaussians, logistic regression and ICA with natural gradients.},
  langid = {english},
  file = {/Users/adam/Zotero/storage/IWRNJRJ5/Welling and Teh - Bayesian Learning via Stochastic Gradient Langevin.pdf}
}

@misc{WeonduCoffeeBean,
  title = { (Coffee Bean) (@wondoo.\_\_) {$\bullet$} {{Instagram}} Photos and Videos},
  urldate = {2024-07-17},
  abstract = {71K Followers, 1,283 Following, 1,701 Posts - See Instagram photos and videos from  (coffee bean) (@wondoo.\_\_)},
  howpublished = {https://www.instagram.com/wondoo.\_\_/},
  langid = {english},
  file = {/Users/adam/Zotero/storage/R959RKCX/wondoo.__.html}
}

@misc{westcottHowUsefulAid2022,
  title = {How Useful Is Aid to {{Africa}}?},
  author = {Westcott, Nick},
  year = 2022,
  month = aug,
  journal = {African Arguments},
  urldate = {2024-12-07},
  abstract = {Mills' latest book calls for donors to focus on democracies, but electoral systems can be as susceptible to corruption as autocracies. Since 1960, Africa has received billions of dollars in aid. In the last 30 years alone, one estimate puts aid to the continent at \$1.2 trillion, though distributed very unevenly across the continent. In this time, African economies have developed. Life expectancy and populations have both risen dramatically, and absolute poverty has fallen. Yet the rise in living standards has been relatively meagre, and few African countries have achieved the rapid economic growth seen in much of Asia -- [\dots ]},
  langid = {british}
}

@misc{WH1000XM4HelpGuide,
  title = {{{WH-1000XM4}} \textbar{} {{Help Guide}} \textbar{} {{Wearing}} the Headset},
  urldate = {2024-10-27},
  howpublished = {https://helpguide.sony.net/mdr/wh1000xm4/v1/en/contents/TP0002928786.html},
  file = {/Users/adam/Zotero/storage/4ACJURR6/TP0002928786.html}
}

@misc{WH1000XM4HelpGuidea,
  title = {{{WH-1000XM4}} \textbar{} {{Help Guide}} \textbar{} {{What}} You Can Do with the {{Bluetooth}} Function},
  urldate = {2024-10-27},
  howpublished = {https://helpguide.sony.net/mdr/wh1000xm4/v1/en/contents/TP0002752770.html},
  file = {/Users/adam/Zotero/storage/KQKDJLSL/TP0002752770.html}
}

@misc{WH1000XM4HelpGuideb,
  title = {{{WH-1000XM4}} \textbar{} {{Help Guide}} \textbar{} {{Location}} and Function of Parts},
  urldate = {2024-10-27},
  howpublished = {https://helpguide.sony.net/mdr/wh1000xm4/v1/en/contents/TP0002752814.html},
  file = {/Users/adam/Zotero/storage/GADQXF9E/TP0002752814.html}
}

@misc{WH1000XM4HelpGuidec,
  title = {{{WH-1000XM4}} \textbar{} {{Help Guide}} \textbar{} {{Charging}} the Headset},
  urldate = {2024-10-27},
  howpublished = {https://helpguide.sony.net/mdr/wh1000xm4/v1/en/contents/TP0002752803.html}
}

@misc{WH1000XM4HelpGuided,
  title = {{{WH-1000XM4}} \textbar{} {{Help Guide}} \textbar{} {{Listening}} to Music from a Device via {{Bluetooth}} Connection},
  urldate = {2024-10-27},
  howpublished = {https://helpguide.sony.net/mdr/wh1000xm4/v1/en/contents/TP0002752782.html}
}

@misc{WH1000XM4HelpGuidee,
  title = {{{WH-1000XM4}} \textbar{} {{Help Guide}} \textbar{} {{Controlling}} the Audio Device ({{Bluetooth}} Connection)},
  urldate = {2024-10-27},
  howpublished = {https://helpguide.sony.net/mdr/wh1000xm4/v1/en/contents/TP0002752781.html},
  file = {/Users/adam/Zotero/storage/EPJL2JP3/TP0002752781.html}
}

@misc{WH1000XM4HelpGuidef,
  title = {{{WH-1000XM4}} \textbar{} {{Help Guide}} \textbar{} {{Pairing}} and Connecting with an {{iPhone}}},
  urldate = {2024-10-27},
  howpublished = {https://helpguide.sony.net/mdr/wh1000xm4/v1/en/contents/TP0002752768.html},
  file = {/Users/adam/Zotero/storage/PUYT87YL/TP0002752768.html}
}

@misc{WH1000XM4HelpGuideg,
  title = {{{WH-1000XM4}} \textbar{} {{Help Guide}} \textbar{} {{Pairing}} and Connecting with an {{Android}} Smartphone},
  urldate = {2024-10-27},
  howpublished = {https://helpguide.sony.net/mdr/wh1000xm4/v1/en/contents/TP0002752774.html},
  file = {/Users/adam/Zotero/storage/BV36UFES/TP0002752774.html}
}

@misc{WhatAreCommon,
  title = {What {{Are Some Common Jobs}} in the {{Field}} of {{Marine Engineering}}?},
  urldate = {2022-07-14},
  abstract = {Marine engineering is the science of designing and building equipment that\&\#039;s used to navigate bodies of water and explore their depths. Common...},
  howpublished = {https://learn.org/articles/https\%3A\%2F\%2Flearn.org\%2Farticles\%2FWhat\_are\_Some\_Common\_Jobs\_in\_the\_Field\_of\_Marine\_Engineering.html},
  file = {/Users/adam/Zotero/storage/GGDBWK9G/What_are_Some_Common_Jobs_in_the_Field_of_Marine_Engineering.html}
}

@misc{WhatCanMaterial,
  title = {What Can {{I}} Do with a Material Science and Engineering Degree? \textbar{} {{Prospects}}.Ac.Uk},
  urldate = {2022-07-14},
  howpublished = {https://www.prospects.ac.uk/careers-advice/what-can-i-do-with-my-degree/materials-science-and-engineering},
  file = {/Users/adam/Zotero/storage/PZE2ZL7L/materials-science-and-engineering.html}
}

@misc{WhatCanMateriala,
  title = {What Can {{I}} Do with a Material Science and Engineering Degree? \textbar{} {{Prospects}}.Ac.Uk},
  urldate = {2022-07-16},
  howpublished = {https://www.prospects.ac.uk/careers-advice/what-can-i-do-with-my-degree/materials-science-and-engineering},
  file = {/Users/adam/Zotero/storage/L7FDXQP5/materials-science-and-engineering.html}
}

@misc{WhatComputerEngineer2022,
  title = {What Is a {{Computer Engineer}} \& {{What Do They Do}}? \textbar{} {{ComputerScience}}.Org},
  shorttitle = {What Is a {{Computer Engineer}} \& {{What Do They Do}}?},
  year = 2022,
  month = feb,
  journal = {Code a New Career \textbar{} ComputerScience.org},
  urldate = {2022-07-12},
  abstract = {Interested in a career as a computer engineer? Read on to learn about computer engineers, from salary to education, daily duties to helpful certifications.},
  howpublished = {https://www.computerscience.org/computer-engineering/careers/computer-engineer/},
  langid = {american},
  file = {/Users/adam/Zotero/storage/59SQ33WU/computer-engineer.html}
}

@misc{WhatComputerScience,
  title = {What Is {{Computer Science}}? \textbar{} {{Undergraduate Computer Science}} at {{UMD}}},
  urldate = {2022-07-12},
  howpublished = {https://undergrad.cs.umd.edu/what-computer-science},
  file = {/Users/adam/Zotero/storage/8YH42MIC/what-computer-science.html}
}

@misc{WhatCyberKill,
  title = {What Is {{The Cyber Kill Chain}} and {{How}} to {{Use}} It {{Effectively}}},
  urldate = {2024-03-12},
  abstract = {The cyber kill chain maps the stages of a cyberattack from the early reconnaissance stages to data exfiltration. The cyber kill chain helps us understand and combat ransomware, security breaches, and advanced persistent attacks (APTs).},
  howpublished = {https://www.varonis.com/blog/cyber-kill-chain},
  langid = {english},
  file = {/Users/adam/Zotero/storage/Q3HNWNNE/cyber-kill-chain.html}
}

@misc{WhatCyberKilla,
  title = {What Is the {{Cyber Kill Chain}}? \textbar{} {{A Comprehensive Guide}} 101},
  shorttitle = {What Is the {{Cyber Kill Chain}}?},
  journal = {SentinelOne},
  urldate = {2024-03-12},
  abstract = {Explore the Cyber Kill Chain: Understand its steps, examples, and how to effectively utilize it for enhanced cybersecurity strategies.},
  howpublished = {https://www.sentinelone.com/cybersecurity-101/cyber-kill-chain/},
  langid = {american},
  file = {/Users/adam/Zotero/storage/SQG9D6BK/cyber-kill-chain.html}
}

@misc{WhatCyberKillb,
  title = {What Is the {{Cyber Kill Chain}}? {{Introduction Guide}} - {{CrowdStrike}}},
  shorttitle = {What Is the {{Cyber Kill Chain}}?},
  journal = {crowdstrike.com},
  urldate = {2024-03-13},
  abstract = {The cyber kill chain is an adaptation of the military's kill chain, a step-by-step approach that identifies and stops enemy activity. Learn more here!},
  howpublished = {https://www.crowdstrike.com/cybersecurity-101/cyber-kill-chain/},
  langid = {english},
  file = {/Users/adam/Zotero/storage/XVINWKN6/cyber-kill-chain.html}
}

@misc{WhatUserDatagram,
  title = {What {{Is User Datagram Protocol}} ({{UDP}})?},
  journal = {Fortinet},
  urldate = {2023-10-31},
  abstract = {User Datagram Protocol (UDP) is a protocol used for communication throughout the internet. Learn how UDP works, the difference between UDP and TCP, and how it is used in DDoS attacks.},
  howpublished = {https://www.fortinet.com/resources/cyberglossary/user-datagram-protocol-udp},
  langid = {english},
  file = {/Users/adam/Zotero/storage/WWQHVJJT/user-datagram-protocol-udp.html}
}

@misc{WhatUserDatagrama,
  title = {What Is the {{User Datagram Protocol}} ({{UDP}})?},
  journal = {Cloudflare},
  urldate = {2023-10-31},
  abstract = {The User Datagram Protocol (UDP) is a connectionless communication protocol for transporting packets across networks. Learn all about UDP/IP.},
  howpublished = {https://www.cloudflare.com/learning/ddos/glossary/user-datagram-protocol-udp/},
  langid = {american},
  file = {/Users/adam/Zotero/storage/ZR5ZAJ9M/user-datagram-protocol-udp.html}
}

@misc{WhyAllAnimals2022,
  title = {Why Do All Animals Love Capybaras?},
  year = 2022,
  month = jan,
  journal = {Animal-Club.co.uk},
  urldate = {2024-07-06},
  abstract = {These large herbivores often live in groups of 10-20, making them highly sociable and causing other animals to, well, LOVE capybaras. And who can blame them?},
  howpublished = {https://animal-club.co.uk/why-do-all-animals-love-capybaras/},
  langid = {british},
  file = {/Users/adam/Zotero/storage/RLPS86JM/why-do-all-animals-love-capybaras.html}
}

@article{wiesingerPigmentBinderConcentrations2018,
  title = {Pigment and {{Binder Concentrations}} in {{Modern Paint Samples Determined}} by {{IR}} and {{Raman Spectroscopy}}},
  author = {Wiesinger, Rita and Pagnin, Laura and Anghelone, Marta and Moretto, Ligia M. and Orsega, Emilio F. and Schreiner, Manfred},
  year = 2018,
  month = jun,
  journal = {Angewandte Chemie International Edition},
  volume = {57},
  number = {25},
  pages = {7401--7407},
  publisher = {John Wiley \& Sons, Ltd},
  issn = {1433-7851},
  doi = {10.1002/anie.201713413},
  urldate = {2022-07-22},
  abstract = {Abstract Knowledge of the techniques employed by artists, such as the composition of the paints, colour palette, and painting style, is of crucial importance not only to attribute works of art to the workshop or artist but also to develop strategies and measures for the conservation and restoration of the art. While much research has been devoted to investigating the composition of an artist's materials from a qualitative point of view, little effort has been made in terms of quantitative analyses. This study aims to quantify the relative concentrations of binders (acrylic and alkyd) and inorganic pigments in different paint samples by IR and Raman spectroscopies. To perform this quantitative evaluation, reference samples of known concentrations were prepared to obtain calibration plots. In a further step, the quantification method was verified by additional test samples and commercially available paint tubes. The results obtained confirm that the quantitative method developed for IR and Raman spectroscopy is able to efficiently determine different pigment and binder concentrations of paint samples with high accuracy.},
  keywords = {acrylic and alkyd binders,FTIR spectroscopy,inorganic pigments,modern paints,Raman spectroscopy}
}

@article{wiesingerPigmentBinderConcentrations2018a,
  title = {Pigment and {{Binder Concentrations}} in {{Modern Paint Samples Determined}} by {{IR}} and {{Raman Spectroscopy}}},
  author = {Wiesinger, Rita and Pagnin, Laura and Anghelone, Marta and Moretto, Ligia M. and Orsega, Emilio F. and Schreiner, Manfred},
  year = 2018,
  month = jun,
  journal = {Angewandte Chemie International Edition},
  volume = {57},
  number = {25},
  pages = {7401--7407},
  publisher = {John Wiley \& Sons, Ltd},
  issn = {1433-7851},
  doi = {10.1002/anie.201713413},
  urldate = {2022-07-22},
  abstract = {Abstract Knowledge of the techniques employed by artists, such as the composition of the paints, colour palette, and painting style, is of crucial importance not only to attribute works of art to the workshop or artist but also to develop strategies and measures for the conservation and restoration of the art. While much research has been devoted to investigating the composition of an artist's materials from a qualitative point of view, little effort has been made in terms of quantitative analyses. This study aims to quantify the relative concentrations of binders (acrylic and alkyd) and inorganic pigments in different paint samples by IR and Raman spectroscopies. To perform this quantitative evaluation, reference samples of known concentrations were prepared to obtain calibration plots. In a further step, the quantification method was verified by additional test samples and commercially available paint tubes. The results obtained confirm that the quantitative method developed for IR and Raman spectroscopy is able to efficiently determine different pigment and binder concentrations of paint samples with high accuracy.},
  keywords = {acrylic and alkyd binders,FTIR spectroscopy,inorganic pigments,modern paints,Raman spectroscopy}
}

@misc{wolfeWhatArchitectLearn2022,
  title = {What {{Is An Architect}}? {{Learn More About What Architects Do}}},
  shorttitle = {What {{Is An Architect}}?},
  author = {Wolfe, Debbie},
  year = 2022,
  month = mar,
  journal = {Forbes Home},
  urldate = {2022-07-29},
  abstract = {Get expert advice on improvements to your home, including design tips, how much you'd expect to pay for a pro and what to ask when hiring experts.},
  chapter = {Contractor},
  howpublished = {https://www.forbes.com/home-improvement/contractor/what-is-an-architect/},
  langid = {american},
  file = {/Users/adam/Zotero/storage/6KGFHT7Q/what-is-an-architect.html}
}

@misc{WorldBankOpen,
  title = {World {{Bank Open Data}}},
  journal = {World Bank Open Data},
  urldate = {2024-11-08},
  abstract = {Free and open access to global development data},
  howpublished = {https://data.worldbank.org},
  langid = {english},
  file = {/Users/adam/Zotero/storage/7JW44W7L/SH.MED.PHYS.html}
}

@misc{WorldClockWorldwide,
  title = {The {{World Clock}} --- {{Worldwide}}},
  urldate = {2023-04-28},
  abstract = {World time and date for cities in all time zones. International time right now. Takes into account all DST clock changes.},
  howpublished = {https://www.timeanddate.com/worldclock/},
  langid = {english},
  file = {/Users/adam/Zotero/storage/ZVTI2MAF/worldclock.html}
}

@article{worldhealthorganizationGlobalAgefriendlyCities2007,
  title = {Global Age-Friendly Cities: A Guide},
  author = {{World Health Organization}},
  year = 2007,
  publisher = {World Health Organization},
  address = {Geneva},
  issn = {9789241547307},
  chapter = {iv, 76 p.},
  langid = {english},
  keywords = {Aged,Aging,City Planning,Quality of Life,Residence Characteristics,statistics,Urban Health}
}

@misc{worldLanguagesEveryCountry2021,
  title = {Languages of {{Every Country}}, {{Every Country}} in the {{World}}},
  author = {in the World, Every Country},
  year = 2021,
  month = may,
  journal = {Every Country in the World},
  urldate = {2023-04-28},
  abstract = {Below are the official languages of all 193 UN countries in the world.  That's all the members of the United Nations},
  langid = {british},
  file = {/Users/adam/Zotero/storage/XYKMDFQQ/languages.html}
}

@inproceedings{wozniakMaintainingProperConditions2012,
  title = {Maintaining {{Proper Conditions}} in {{Quiet Study Rooms}} with {{Ambient Influence}}},
  booktitle = {Proceedings of the 7th {{Nordic Conference}} on {{Human-Computer Interaction}}: {{Making Sense Through Design}}},
  author = {Wo{\'z}niak, Pawe{\textbackslash}l and Romanowski, Andrzej and Proborszcz, Filip and Borkowska, Martyna and Stozek, Dominik and Koczorowicz, Bartosz},
  year = 2012,
  series = {{{NordiCHI}} '12},
  pages = {787--788},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/2399016.2399148},
  abstract = {This paper focuses on a context-specific study of how interactive systems can affect human behaviours. In the subRosa project, we employed ambient display technology in order to foster proper learning conditions in a dedicated study room. SubRosa was evaluated through two proof-of-concept studies performed using a high-fidelity prototype. Besides proposing a system, we address a problem present in the study environment at our university. That is, due to a recent influx of new students, noise levels in a room designated for quiet study have become excessive. subRosa is aimed at making this space more education-friendly. The system will now be subject to a longer experience study that will determine its actual impact on the working environment. Furthermore, the studies will aim at determining the properties of an ambient display that are required to affect user behaviours.},
  isbn = {978-1-4503-1482-4},
  keywords = {ambient displays,ambient influence,behavioral change,in-the-wild evaluation,user studies}
}

@misc{WritingStyleGuide2024,
  title = {Writing Style Guide - {{The MDN Web Docs}} Project \textbar{} {{MDN}}},
  year = 2024,
  month = sep,
  urldate = {2024-09-29},
  abstract = {This writing style guide describes how content should be written, organized, spelled, and formatted on MDN Web Docs.},
  howpublished = {https://developer.mozilla.org/en-US/docs/MDN/Writing\_guidelines/Writing\_style\_guide},
  langid = {american},
  file = {/Users/adam/Zotero/storage/PQ2XHHKR/Writing_style_guide.html}
}

@misc{WSPGraduateCivil,
  title = {{{WSP Graduate Civil Engineer Job}} in {{Irvine}}, {{CA}}},
  journal = {Glassdoor},
  urldate = {2022-07-29},
  abstract = {WSP  is now hiring a  Graduate Civil Engineer in Irvine, CA. View job listing details and apply now.},
  howpublished = {https://www.glassdoor.com/job-listing/graduate-civil-engineer-wsp-JV\_IC1146798\_KO0,23\_KE24,27.htm?jl=1008034677310},
  langid = {english},
  file = {/Users/adam/Zotero/storage/TPUZCP34/graduate-civil-engineer-wsp-JV_IC1146798_KO0,23_KE24,27.html}
}

@unpublished{wuDEPNDetectingEditing2023,
  title = {{{DEPN}}: {{Detecting}} and {{Editing Privacy Neurons}} in {{Pretrained Language Models}}},
  author = {Wu, Xinwei and Li, Junzhuo and Xu, Minghui and Dong, Weilong and Wu, Shuangzhi and Bian, Chao and Xiong, Deyi},
  year = 2023,
  month = oct,
  journal = {arXiv [cs.CR]},
  abstract = {Large language models pretrained on a huge amount of data capture rich knowledge and information in the training data. The ability of data memorization and regurgitation in pretrained language models, revealed in previous studies, brings the risk of data leakage. In order to effectively reduce these risks, we propose a framework DEPN to Detect and Edit Privacy Neurons in pretrained language models, partially inspired by knowledge neurons and model editing. In DEPN, we introduce a novel method, termed as privacy neuron detector, to locate neurons associated with private information, and then edit these detected privacy neurons by setting their activations to zero. Furthermore, we propose a privacy neuron aggregator dememorize private information in a batch processing manner. Experimental results show that our method can significantly and efficiently reduce the exposure of private data leakage without deteriorating the performance of the model. Additionally, we empirically demonstrate the relationship between model memorization and privacy neurons, from multiple perspectives, including model size, training time, prompts, privacy neuron distribution, illustrating the robustness of our approach.},
  isbn = {2310.20138}
}

@unpublished{wuMetaRewardingLanguageModels2024,
  title = {Meta-{{Rewarding Language Models}}: {{Self-Improving Alignment}} with {{LLM-as-a-Meta-Judge}}},
  author = {Wu, Tianhao and Yuan, Weizhe and Golovneva, Olga and Xu, Jing and Tian, Yuandong and Jiao, Jiantao and Weston, Jason and Sukhbaatar, Sainbayar},
  year = 2024,
  month = jul,
  journal = {arXiv [cs.CL]},
  abstract = {Large Language Models (LLMs) are rapidly surpassing human knowledge in many domains. While improving these models traditionally relies on costly human data, recent self-rewarding mechanisms (Yuan et al., 2024) have shown that LLMs can improve by judging their own responses instead of relying on human labelers. However, existing methods have primarily focused on improving model responses rather than judgment capabilities, resulting in rapid saturation during iterative training. To address this issue, we introduce a novel Meta-Rewarding step to the self-improvement process, where the model judges its own judgements and uses that feedback to refine its judgment skills. Surprisingly, this unsupervised approach improves the model's ability to judge \textbraceleft\textbackslash em and\textbraceright{} follow instructions, as demonstrated by a win rate improvement of Llama-3-8B-Instruct from 22.9\% to 39.4\% on AlpacaEval 2, and 20.6\% to 29.1\% on Arena-Hard. These results strongly suggest the potential for self-improving models without human supervision.},
  isbn = {2407.19594}
}

@misc{xiAgentGymEvolvingLarge2024,
  title = {{{AgentGym}}: {{Evolving Large Language Model-based Agents}} across {{Diverse Environments}}},
  shorttitle = {{{AgentGym}}},
  author = {Xi, Zhiheng and Ding, Yiwen and Chen, Wenxiang and Hong, Boyang and Guo, Honglin and Wang, Junzhe and Yang, Dingwen and Liao, Chenyang and Guo, Xin and He, Wei and Gao, Songyang and Chen, Lu and Zheng, Rui and Zou, Yicheng and Gui, Tao and Zhang, Qi and Qiu, Xipeng and Huang, Xuanjing and Wu, Zuxuan and Jiang, Yu-Gang},
  year = 2024,
  month = jun,
  journal = {arXiv.org},
  urldate = {2026-02-21},
  abstract = {Building generalist agents that can handle diverse tasks and evolve themselves across different environments is a long-term goal in the AI community. Large language models (LLMs) are considered a promising foundation to build such agents due to their generalized capabilities. Current approaches either have LLM-based agents imitate expert-provided trajectories step-by-step, requiring human supervision, which is hard to scale and limits environmental exploration; or they let agents explore and learn in isolated environments, resulting in specialist agents with limited generalization. In this paper, we take the first step towards building generally-capable LLM-based agents with self-evolution ability. We identify a trinity of ingredients: 1) diverse environments for agent exploration and learning, 2) a trajectory set to equip agents with basic capabilities and prior knowledge, and 3) an effective and scalable evolution method. We propose AgentGym, a new framework featuring a variety of environments and tasks for broad, real-time, uni-format, and concurrent agent exploration. AgentGym also includes a database with expanded instructions, a benchmark suite, and high-quality trajectories across environments. Next, we propose a novel method, AgentEvol, to investigate the potential of agent self-evolution beyond previously seen data across tasks and environments. Experimental results show that the evolved agents can achieve results comparable to SOTA models. We release the AgentGym suite, including the platform, dataset, benchmark, checkpoints, and algorithm implementations. The AgentGym suite is available on https://github.com/WooooDyy/AgentGym.},
  howpublished = {https://arxiv.org/abs/2406.04151v1},
  langid = {english},
  file = {/Users/adam/Zotero/storage/IQF5XYIH/Xi et al. - 2024 - AgentGym Evolving Large Language Model-based Agents across Diverse Environments.pdf}
}

@misc{xieAdaptiveChameleonStubborn2024,
  title = {Adaptive {{Chameleon}} or {{Stubborn Sloth}}: {{Revealing}} the {{Behavior}} of {{Large Language Models}} in {{Knowledge Conflicts}}},
  shorttitle = {Adaptive {{Chameleon}} or {{Stubborn Sloth}}},
  author = {Xie, Jian and Zhang, Kai and Chen, Jiangjie and Lou, Renze and Su, Yu},
  year = 2024,
  month = feb,
  number = {arXiv:2305.13300},
  eprint = {2305.13300},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.13300},
  urldate = {2024-12-01},
  abstract = {By providing external information to large language models (LLMs), tool augmentation (including retrieval augmentation) has emerged as a promising solution for addressing the limitations of LLMs' static parametric memory. However, how receptive are LLMs to such external evidence, especially when the evidence conflicts with their parametric memory? We present the first comprehensive and controlled investigation into the behavior of LLMs when encountering knowledge conflicts. We propose a systematic framework to elicit high-quality parametric memory from LLMs and construct the corresponding counter-memory, which enables us to conduct a series of controlled experiments. Our investigation reveals seemingly contradicting behaviors of LLMs. On the one hand, different from prior wisdom, we find that LLMs can be highly receptive to external evidence even when that conflicts with their parametric memory, given that the external evidence is coherent and convincing. On the other hand, LLMs also demonstrate a strong confirmation bias when the external evidence contains some information that is consistent with their parametric memory, despite being presented with conflicting evidence at the same time. These results pose important implications that are worth careful consideration for the further development and deployment of tool- and retrieval-augmented LLMs. Resources are available at https://github.com/OSU-NLP-Group/LLM-Knowledge-Conflict.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/adam/Zotero/storage/VTIBY5PH/Xie et al. - 2024 - Adaptive Chameleon or Stubborn Sloth Revealing th.pdf;/Users/adam/Zotero/storage/GEPBZ2GY/2305.html}
}

@misc{xuEarthFlatBecause2024,
  title = {The {{Earth}} Is {{Flat}} Because...: {{Investigating LLMs}}' {{Belief}} towards {{Misinformation}} via {{Persuasive Conversation}}},
  shorttitle = {The {{Earth}} Is {{Flat}} Because...},
  author = {Xu, Rongwu and Lin, Brian S. and Yang, Shujian and Zhang, Tianqi and Shi, Weiyan and Zhang, Tianwei and Fang, Zhixuan and Xu, Wei and Qiu, Han},
  year = 2024,
  month = may,
  number = {arXiv:2312.09085},
  eprint = {2312.09085},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2312.09085},
  urldate = {2024-12-01},
  abstract = {Large language models (LLMs) encapsulate vast amounts of knowledge but still remain vulnerable to external misinformation. Existing research mainly studied this susceptibility behavior in a single-turn setting. However, belief can change during a multi-turn conversation, especially a persuasive one. Therefore, in this study, we delve into LLMs' susceptibility to persuasive conversations, particularly on factual questions that they can answer correctly. We first curate the Farm (i.e., Fact to Misinform) dataset, which contains factual questions paired with systematically generated persuasive misinformation. Then, we develop a testing framework to track LLMs' belief changes in a persuasive dialogue. Through extensive experiments, we find that LLMs' correct beliefs on factual knowledge can be easily manipulated by various persuasive strategies.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computers and Society,Computer Science - Cryptography and Security},
  file = {/Users/adam/Zotero/storage/8EMGCDIT/Xu et al. - 2024 - The Earth is Flat because... Investigating LLMs' .pdf;/Users/adam/Zotero/storage/DLRVQCJV/2312.html}
}

@misc{xuEarthFlatBecause2024a,
  title = {The {{Earth}} Is {{Flat}} Because...: {{Investigating LLMs}}' {{Belief}} towards {{Misinformation}} via {{Persuasive Conversation}}},
  shorttitle = {The {{Earth}} Is {{Flat}} Because...},
  author = {Xu, Rongwu and Lin, Brian S. and Yang, Shujian and Zhang, Tianqi and Shi, Weiyan and Zhang, Tianwei and Fang, Zhixuan and Xu, Wei and Qiu, Han},
  year = 2024,
  month = may,
  number = {arXiv:2312.09085},
  eprint = {2312.09085},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2312.09085},
  urldate = {2024-12-01},
  abstract = {Large language models (LLMs) encapsulate vast amounts of knowledge but still remain vulnerable to external misinformation. Existing research mainly studied this susceptibility behavior in a single-turn setting. However, belief can change during a multi-turn conversation, especially a persuasive one. Therefore, in this study, we delve into LLMs' susceptibility to persuasive conversations, particularly on factual questions that they can answer correctly. We first curate the Farm (i.e., Fact to Misinform) dataset, which contains factual questions paired with systematically generated persuasive misinformation. Then, we develop a testing framework to track LLMs' belief changes in a persuasive dialogue. Through extensive experiments, we find that LLMs' correct beliefs on factual knowledge can be easily manipulated by various persuasive strategies.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computers and Society,Computer Science - Cryptography and Security},
  file = {/Users/adam/Zotero/storage/8ANGJRXZ/Xu et al. - 2024 - The Earth is Flat because... Investigating LLMs' .pdf;/Users/adam/Zotero/storage/E54ZG3RR/2312.html}
}

@misc{xuHallucinationInevitableInnate2024,
  title = {Hallucination Is {{Inevitable}}: {{An Innate Limitation}} of {{Large Language Models}}},
  shorttitle = {Hallucination Is {{Inevitable}}},
  author = {Xu, Ziwei and Jain, Sanjay and Kankanhalli, Mohan},
  year = 2024,
  month = jan,
  number = {arXiv:2401.11817},
  eprint = {2401.11817},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2401.11817},
  urldate = {2024-11-28},
  abstract = {Hallucination has been widely recognized to be a significant drawback for large language models (LLMs). There have been many works that attempt to reduce the extent of hallucination. These efforts have mostly been empirical so far, which cannot answer the fundamental question whether it can be completely eliminated. In this paper, we formalize the problem and show that it is impossible to eliminate hallucination in LLMs. Specifically, we define a formal world where hallucination is defined as inconsistencies between a computable LLM and a computable ground truth function. By employing results from learning theory, we show that LLMs cannot learn all of the computable functions and will therefore always hallucinate. Since the formal world is a part of the real world which is much more complicated, hallucinations are also inevitable for real world LLMs. Furthermore, for real world LLMs constrained by provable time complexity, we describe the hallucination-prone tasks and empirically validate our claims. Finally, using the formal world framework, we discuss the possible mechanisms and efficacies of existing hallucination mitigators as well as the practical implications on the safe deployment of LLMs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/adam/Zotero/storage/WFZAI8S6/Xu et al. - 2024 - Hallucination is Inevitable An Innate Limitation .pdf;/Users/adam/Zotero/storage/XKBCFZMX/2401.html}
}

@misc{xuKDRLPostTrainingReasoning2025,
  title = {{{KDRL}}: {{Post-Training Reasoning LLMs}} via {{Unified Knowledge Distillation}} and {{Reinforcement Learning}}},
  shorttitle = {{{KDRL}}},
  author = {Xu, Hongling and Zhu, Qi and Deng, Heyuan and Li, Jinpeng and Hou, Lu and Wang, Yasheng and Shang, Lifeng and Xu, Ruifeng and Mi, Fei},
  year = 2025,
  month = jun,
  number = {arXiv:2506.02208},
  eprint = {2506.02208},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2506.02208},
  urldate = {2025-06-30},
  abstract = {Recent advances in large language model (LLM) post-training have leveraged two distinct paradigms to enhance reasoning capabilities: reinforcement learning (RL) and knowledge distillation (KD). While RL enables the emergence of complex reasoning behaviors, it often suffers from low sample efficiency when the initial policy struggles to explore high-reward trajectories. Conversely, KD improves learning efficiency via mimicking the teacher model but tends to generalize poorly to out-of-domain scenarios. In this work, we present \textbackslash textbf\textbraceleft KDRL\textbraceright, a \textbackslash textit\textbraceleft unified post-training framework\textbraceright{} that jointly optimizes a reasoning model through teacher supervision (KD) and self-exploration (RL). Specifically, KDRL leverages policy gradient optimization to simultaneously minimize the reverse Kullback-Leibler divergence (RKL) between the student and teacher distributions while maximizing the expected rule-based rewards. We first formulate a unified objective that integrates GRPO and KD, and systematically explore how different KL approximations, KL coefficients, and reward-guided KD strategies affect the overall post-training dynamics and performance. Empirical results on multiple reasoning benchmarks demonstrate that KDRL outperforms GRPO and various KD baselines while achieving a favorable balance between performance and reasoning token efficiency. These findings indicate that integrating KD and RL serves as an effective and efficient strategy to train reasoning LLMs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/adam/Zotero/storage/VPELSW22/Xu et al. - 2025 - KDRL Post-Training Reasoning LLMs via Unified Kno.pdf}
}

@article{yamamotoAnalysisJapaneseShrinking2019,
  title = {Analysis of Japanese Shrinking Cities and Policies to Tackle This Problem (the Case of Sammu City and Its Economic Gardening Project)},
  author = {Yamamoto, Takashi},
  year = 2019,
  month = oct,
  journal = {R-Economy},
  volume = {5},
  number = {3},
  pages = {123--136},
  issn = {2412-0731},
  doi = {10.15826/recon.2019.5.3.013},
  urldate = {2023-04-08},
  abstract = {This paper examined the problem of shrinking cities and evaluated the policies used to mitigate the impact of shrinkage. The analytical section of this paper discusses the definition of a shrinking city, Japan's depopulation in the coming decades on the national and municipality level, and the vicious circle of the population loss and the change of economic structure in shrinking Japanese cities. The second section of the paper examines the desired policy goals for shrinking cities, along with strategies and approaches to achieve them. It is shown that the strategies that the Japanese national government has realized since 2014 were inadequate and ineffective. An alternative initiative (for example, the economic gardening model) is necessary to complement governmental programs to empower SMEs in cities, create more jobs and boost the incomes of businesses and city residents. The case study section of this paper analyzed the case of Sammu -- a shrinking Japanese city, which has been engaged in an economic gardening project. Even though the outcomes of this project have not been officially confirmed, the available data show that the sales and employment of the local firms that participated in the program either improved or at least remained at the same level. The potential area for future research might be analysis of programs for revitalizing shrinking cities in resource-dependent regions, for instance, of Russia and Australia. Such studies could provide insightful suggestions for adequate policy formulation and implementation.},
  copyright = {Copyright (c) 2019 Takashi Yamamoto},
  langid = {english},
  file = {/Users/adam/Zotero/storage/2NSGLMDC/Yamamoto - 2019 - Analysis of japanese shrinking cities and policies.pdf;/Users/adam/Zotero/storage/IPH33LTC/4114.html}
}

@unpublished{yaoLargeLanguageModel2023,
  title = {Large {{Language Model Unlearning}}},
  author = {Yao, Yuanshun and Xu, Xiaojun and Liu, Yang},
  year = 2023,
  month = oct,
  journal = {arXiv [cs.CL]},
  abstract = {We study how to perform unlearning, i.e. forgetting undesirable misbehaviors, on large language models (LLMs). We show at least three scenarios of aligning LLMs with human preferences can benefit from unlearning: (1) removing harmful responses, (2) erasing copyright-protected content as requested, and (3) reducing hallucinations. Unlearning, as an alignment technique, has three advantages. (1) It only requires negative (e.g. harmful) examples, which are much easier and cheaper to collect (e.g. via red teaming or user reporting) than positive (e.g. helpful and often human-written) examples required in RLHF (RL from human feedback). (2) It is computationally efficient. (3) It is especially effective when we know which training samples cause the misbehavior. To the best of our knowledge, our work is among the first to explore LLM unlearning. We are also among the first to formulate the settings, goals, and evaluations in LLM unlearning. We show that if practitioners only have limited resources, and therefore the priority is to stop generating undesirable outputs rather than to try to generate desirable outputs, unlearning is particularly appealing. Despite only having negative samples, our ablation study shows that unlearning can still achieve better alignment performance than RLHF with just 2\% of its computational time.},
  isbn = {2310.10683}
}

@unpublished{yaoMachineUnlearningPretrained2024,
  title = {Machine {{Unlearning}} of {{Pre-trained Large Language Models}}},
  author = {Yao, Jin and Chien, Eli and Du, Minxin and Niu, Xinyao and Wang, Tianhao and Cheng, Zezhou and Yue, Xiang},
  year = 2024,
  month = feb,
  journal = {arXiv [cs.CL]},
  abstract = {This study investigates the concept of the `right to be forgotten' within the context of large language models (LLMs). We explore machine unlearning as a pivotal solution, with a focus on pre-trained models--a notably under-researched area. Our research delineates a comprehensive framework for machine unlearning in pre-trained LLMs, encompassing a critical analysis of seven diverse unlearning methods. Through rigorous evaluation using curated datasets from arXiv, books, and GitHub, we establish a robust benchmark for unlearning performance, demonstrating that these methods are over \$10\textasciicircum 5\$ times more computationally efficient than retraining. Our results show that integrating gradient ascent with gradient descent on in-distribution data improves hyperparameter robustness. We also provide detailed guidelines for efficient hyperparameter tuning in the unlearning process. Our findings advance the discourse on ethical AI practices, offering substantive insights into the mechanics of machine unlearning for pre-trained LLMs and underscoring the potential for responsible AI development.},
  isbn = {2402.15159}
}

@misc{yaoReActSynergizingReasoning2023,
  title = {{{ReAct}}: {{Synergizing Reasoning}} and {{Acting}} in {{Language Models}}},
  shorttitle = {{{ReAct}}},
  author = {Yao, Shunyu and Zhao, Jeffrey and Yu, Dian and Du, Nan and Shafran, Izhak and Narasimhan, Karthik and Cao, Yuan},
  year = 2023,
  month = mar,
  number = {arXiv:2210.03629},
  eprint = {2210.03629},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2210.03629},
  urldate = {2025-07-15},
  abstract = {While large language models (LLMs) have demonstrated impressive capabilities across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with external sources, such as knowledge bases or environments, to gather additional information. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines, as well as improved human interpretability and trustworthiness over methods without reasoning or acting components. Concretely, on question answering (HotpotQA) and fact verification (Fever), ReAct overcomes issues of hallucination and error propagation prevalent in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generates human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34\% and 10\% respectively, while being prompted with only one or two in-context examples. Project site with code: https://react-lm.github.io},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/adam/Zotero/storage/WQIA44CU/Yao et al. - 2023 - ReAct Synergizing Reasoning and Acting in Languag.pdf;/Users/adam/Zotero/storage/GXUD353S/2210.html}
}

@misc{yaoTreeThoughtsDeliberate2023,
  title = {Tree of {{Thoughts}}: {{Deliberate Problem Solving}} with {{Large Language Models}}},
  shorttitle = {Tree of {{Thoughts}}},
  author = {Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Thomas L. and Cao, Yuan and Narasimhan, Karthik},
  year = 2023,
  month = dec,
  number = {arXiv:2305.10601},
  eprint = {2305.10601},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.10601},
  urldate = {2025-07-15},
  abstract = {Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models' problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4\% of tasks, our method achieved a success rate of 74\%. Code repo with all prompts: https://github.com/princeton-nlp/tree-of-thought-llm.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/adam/Zotero/storage/LI9VNWUG/Yao et al. - 2023 - Tree of Thoughts Deliberate Problem Solving with .pdf;/Users/adam/Zotero/storage/HM879UIW/2305.html}
}

@misc{yeDifferentialTransformer2024,
  title = {Differential {{Transformer}}},
  author = {Ye, Tianzhu and Dong, Li and Xia, Yuqing and Sun, Yutao and Zhu, Yi and Huang, Gao and Wei, Furu},
  year = 2024,
  month = oct,
  number = {arXiv:2410.05258},
  eprint = {2410.05258},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2410.05258},
  urldate = {2024-10-15},
  abstract = {Transformer tends to overallocate attention to irrelevant context. In this work, we introduce Diff Transformer, which amplifies attention to the relevant context while canceling noise. Specifically, the differential attention mechanism calculates attention scores as the difference between two separate softmax attention maps. The subtraction cancels noise, promoting the emergence of sparse attention patterns. Experimental results on language modeling show that Diff Transformer outperforms Transformer in various settings of scaling up model size and training tokens. More intriguingly, it offers notable advantages in practical applications, such as long-context modeling, key information retrieval, hallucination mitigation, in-context learning, and reduction of activation outliers. By being less distracted by irrelevant context, Diff Transformer can mitigate hallucination in question answering and text summarization. For in-context learning, Diff Transformer not only enhances accuracy but is also more robust to order permutation, which was considered as a chronic robustness issue. The results position Diff Transformer as a highly effective and promising architecture to advance large language models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/adam/Zotero/storage/BR89855T/Ye et al. - 2024 - Differential Transformer.pdf;/Users/adam/Zotero/storage/G5HTI9CJ/2410.html}
}

@article{yeEquitableAccessCOVID192022,
  title = {Equitable Access to {{COVID-19}} Vaccines Makes a Life-Saving Difference to All Countries},
  author = {Ye, Yang and Zhang, Qingpeng and Wei, Xuan and Cao, Zhidong and Yuan, Hsiang-Yu and Zeng, Daniel Dajun},
  year = 2022,
  month = feb,
  journal = {Nature Human Behaviour},
  volume = {6},
  number = {2},
  pages = {207--216},
  publisher = {Nature Publishing Group},
  issn = {2397-3374},
  doi = {10.1038/s41562-022-01289-8},
  urldate = {2024-11-08},
  abstract = {Despite broad agreement on the negative consequences of vaccine inequity, the distribution of COVID-19 vaccines is imbalanced. Access to vaccines in high-income countries (HICs) is far greater than in low- and middle-income countries (LMICs). As a result, there continue to be high rates of COVID-19 infections and deaths in LMICs. In addition, recent mutant COVID-19 outbreaks may counteract advances in epidemic control and economic recovery in HICs. To explore the consequences of vaccine (in)equity in the face of evolving COVID-19 strains, we examine vaccine allocation strategies using a multistrain metapopulation model. Our results show that vaccine inequity provides only limited and short-term benefits to HICs. Sharper disparities in vaccine allocation between HICs and LMICs lead to earlier and larger outbreaks of new waves. Equitable vaccine allocation strategies, in contrast, substantially curb the spread of new strains. For HICs, making immediate and generous vaccine donations to LMICs is a practical pathway to protect everyone.},
  copyright = {2022 The Author(s), under exclusive licence to Springer Nature Limited},
  langid = {english},
  keywords = {Complex networks},
  file = {/Users/adam/Zotero/storage/UCJJPMJZ/Ye et al. - 2022 - Equitable access to COVID-19 vaccines makes a life.pdf}
}

@misc{yoshikuniRIETILowestlowFertility,
  title = {{{RIETI}} - ``{{Lowest-low Fertility}}'' \& {{Gender Inequality}}: {{Japan}} from a {{Comparative Perspective}}},
  author = {Yoshikuni, {\relax ONO} and Uchikoshi, Fumiya},
  journal = {Research Institute of Economy, Trade And Industry},
  urldate = {2023-04-08},
  howpublished = {https://www.rieti.go.jp/en/papers/contribution/ono-yoshikuni/02.html},
  file = {/Users/adam/Zotero/storage/R34UUSAY/02.html}
}

@article{yuChitosanDerivedPorousActivated2019,
  title = {Chitosan-{{Derived Porous Activated Carbon}} for the {{Removal}} of the {{Chemical Warfare Agent Simulant Dimethyl Methylphosphonate}}},
  author = {Yu, H and Son, {\relax YR} and Yoo, H and Cha, {\relax HG} and Lee, H and Kim, {\relax HS}},
  year = 2019,
  month = dec,
  journal = {NANOMATERIALS},
  volume = {9},
  number = {12},
  issn = {2079-4991},
  doi = {10.3390/nano9121703},
  abstract = {Methods for the rapid removal of chemical warfare agents are of critical importance. In this work, a porous activated carbon material (C-PAC) was prepared from chitosan flakes via single-step potassium carbonate (K2CO3) activation for the prompt adsorption of dimethyl methylphosphonate (DMMP). C-PAC samples were prepared using different carbonization temperatures (350, 550, and 750 degrees C) at a constant K2CO3/chitosan ratio (1:2) and using different activator ratios (K2CO3/chitosan ratios of 1:0.5, 1:1, 1:2, and 1:3) at 750 degrees C. Furthermore, we evaluated the effect of preparation conditions on the adsorption capacities of the various C-PAC materials for DMMP under ambient conditions (25 degrees C). Notably, for the C-PAC material prepared at 750 degrees C using a K2CO3/chitosan ratio of 1:2, the DMMP adsorption was saturated at approximately 412 mgg(-1) carbon after 48 h. The good performance of this material makes it a potential candidate for use in remedial applications or protective gear.},
  langid = {english},
  keywords = {activated carbon,adsorption,ADSORPTION,chemical warfare,chitosan,GRAPHENE OXIDE,GRAPHITE,METHYLENE-BLUE}
}

@article{yuChitosanDerivedPorousActivated2019a,
  title = {Chitosan-{{Derived Porous Activated Carbon}} for the {{Removal}} of the {{Chemical Warfare Agent Simulant Dimethyl Methylphosphonate}}},
  author = {Yu, H and Son, {\relax YR} and Yoo, H and Cha, {\relax HG} and Lee, H and Kim, {\relax HS}},
  year = 2019,
  month = dec,
  journal = {NANOMATERIALS},
  volume = {9},
  number = {12},
  issn = {2079-4991},
  doi = {10.3390/nano9121703},
  abstract = {Methods for the rapid removal of chemical warfare agents are of critical importance. In this work, a porous activated carbon material (C-PAC) was prepared from chitosan flakes via single-step potassium carbonate (K2CO3) activation for the prompt adsorption of dimethyl methylphosphonate (DMMP). C-PAC samples were prepared using different carbonization temperatures (350, 550, and 750 degrees C) at a constant K2CO3/chitosan ratio (1:2) and using different activator ratios (K2CO3/chitosan ratios of 1:0.5, 1:1, 1:2, and 1:3) at 750 degrees C. Furthermore, we evaluated the effect of preparation conditions on the adsorption capacities of the various C-PAC materials for DMMP under ambient conditions (25 degrees C). Notably, for the C-PAC material prepared at 750 degrees C using a K2CO3/chitosan ratio of 1:2, the DMMP adsorption was saturated at approximately 412 mgg(-1) carbon after 48 h. The good performance of this material makes it a potential candidate for use in remedial applications or protective gear.},
  langid = {english},
  keywords = {activated carbon,adsorption,ADSORPTION,chemical warfare,chitosan,GRAPHENE OXIDE,GRAPHITE,METHYLENE-BLUE}
}

@unpublished{yuGPTFUZZERRedTeaming2023,
  title = {{{GPTFUZZER}}: {{Red Teaming Large Language Models}} with {{Auto-Generated Jailbreak Prompts}}},
  author = {Yu, Jiahao and Lin, Xingwei and Yu, Zheng and Xing, Xinyu},
  year = 2023,
  month = sep,
  journal = {arXiv [cs.AI]},
  abstract = {Large language models (LLMs) have recently experienced tremendous popularity and are widely used from casual conversations to AI-driven programming. However, despite their considerable success, LLMs are not entirely reliable and can give detailed guidance on how to conduct harmful or illegal activities. While safety measures can reduce the risk of such outputs, adversarial jailbreak attacks can still exploit LLMs to produce harmful content. These jailbreak templates are typically manually crafted, making large-scale testing challenging. In this paper, we introduce GPTFuzz, a novel black-box jailbreak fuzzing framework inspired by the AFL fuzzing framework. Instead of manual engineering, GPTFuzz automates the generation of jailbreak templates for red-teaming LLMs. At its core, GPTFuzz starts with human-written templates as initial seeds, then mutates them to produce new templates. We detail three key components of GPTFuzz: a seed selection strategy for balancing efficiency and variability, mutate operators for creating semantically equivalent or similar sentences, and a judgment model to assess the success of a jailbreak attack. We evaluate GPTFuzz against various commercial and open-source LLMs, including ChatGPT, LLaMa-2, and Vicuna, under diverse attack scenarios. Our results indicate that GPTFuzz consistently produces jailbreak templates with a high success rate, surpassing human-crafted templates. Remarkably, GPTFuzz achieves over 90\% attack success rates against ChatGPT and Llama-2 models, even with suboptimal initial seed templates. We anticipate that GPTFuzz will be instrumental for researchers and practitioners in examining LLM robustness and will encourage further exploration into enhancing LLM safety.},
  isbn = {2309.10253}
}

@misc{zambelliPhotoGustavoZambelli2021,
  title = {Photo by {{Gustavo Zambelli}} on {{Unsplash}}},
  author = {Zambelli, Gustavo},
  year = 2021,
  month = apr,
  urldate = {2024-07-18},
  abstract = {Download this photo by Gustavo Zambelli on Unsplash},
  howpublished = {https://unsplash.com/photos/white-and-black-guinea-pig-on-brown-wooden-table-mwwRDU\_ekjw},
  langid = {american},
  file = {/Users/adam/Zotero/storage/7HBEY6LY/white-and-black-guinea-pig-on-brown-wooden-table-mwwRDU_ekjw.html}
}

@misc{zengEvalTreeProfilingLanguage2025,
  title = {{{EvalTree}}: {{Profiling Language Model Weaknesses}} via {{Hierarchical Capability Trees}}},
  shorttitle = {{{EvalTree}}},
  author = {Zeng, Zhiyuan and Wang, Yizhong and Hajishirzi, Hannaneh and Koh, Pang Wei},
  year = 2025,
  month = jul,
  number = {arXiv:2503.08893},
  eprint = {2503.08893},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2503.08893},
  urldate = {2025-08-09},
  abstract = {An ideal model evaluation should achieve two goals: identifying where the model fails and providing actionable improvement guidance. Toward these goals for language model (LM) evaluations, we formulate the problem of generating a weakness profile, a set of weaknesses expressed in natural language, given an LM's performance on every individual instance in a benchmark. We introduce a suite of quantitative assessments to compare different weakness profiling methods. We also introduce a weakness profiling method EvalTree. EvalTree constructs a capability tree where each node represents a capability described in natural language and is linked to a subset of benchmark instances that specifically evaluate this capability; it then extracts nodes where the LM performs poorly to generate a weakness profile. On the MATH and WildChat benchmarks, we show that EvalTree outperforms baseline weakness profiling methods by identifying weaknesses more precisely and comprehensively. Weakness profiling further enables weakness-guided data collection, and training data collection guided by EvalTree-identified weaknesses improves LM performance more than other data collection strategies. We also show how EvalTree exposes flaws in Chatbot Arena's human-voter-based evaluation practice. To facilitate future work, we provide an interface that allows practitioners to interactively explore the capability trees built by EvalTree.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/adam/Zotero/storage/BJ7UD2L2/Zeng et al. - 2025 - EvalTree Profiling Language Model Weaknesses via .pdf;/Users/adam/Zotero/storage/FT94SLGL/2503.html}
}

@misc{zengEvalTreeProfilingLanguage2025a,
  title = {{{EvalTree}}: {{Profiling Language Model Weaknesses}} via {{Hierarchical Capability Trees}}},
  shorttitle = {{{EvalTree}}},
  author = {Zeng, Zhiyuan and Wang, Yizhong and Hajishirzi, Hannaneh and Koh, Pang Wei},
  year = 2025,
  month = jul,
  number = {arXiv:2503.08893},
  eprint = {2503.08893},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2503.08893},
  urldate = {2025-08-11},
  abstract = {An ideal model evaluation should achieve two goals: identifying where the model fails and providing actionable improvement guidance. Toward these goals for language model (LM) evaluations, we formulate the problem of generating a weakness profile, a set of weaknesses expressed in natural language, given an LM's performance on every individual instance in a benchmark. We introduce a suite of quantitative assessments to compare different weakness profiling methods. We also introduce a weakness profiling method EvalTree. EvalTree constructs a capability tree where each node represents a capability described in natural language and is linked to a subset of benchmark instances that specifically evaluate this capability; it then extracts nodes where the LM performs poorly to generate a weakness profile. On the MATH and WildChat benchmarks, we show that EvalTree outperforms baseline weakness profiling methods by identifying weaknesses more precisely and comprehensively. Weakness profiling further enables weakness-guided data collection, and training data collection guided by EvalTree-identified weaknesses improves LM performance more than other data collection strategies. We also show how EvalTree exposes flaws in Chatbot Arena's human-voter-based evaluation practice. To facilitate future work, we provide an interface that allows practitioners to interactively explore the capability trees built by EvalTree.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/adam/Zotero/storage/FLWWBBBF/Zeng et al. - 2025 - EvalTree Profiling Language Model Weaknesses via .pdf;/Users/adam/Zotero/storage/7H4M9TBA/2503.html}
}

@misc{zhangBacktrackingImprovesGeneration2024,
  title = {Backtracking {{Improves Generation Safety}}},
  author = {Zhang, Yiming and Chi, Jianfeng and Nguyen, Hailey and Upasani, Kartikeya and Bikel, Daniel M. and Weston, Jason and Smith, Eric Michael},
  year = 2024,
  month = sep,
  number = {arXiv:2409.14586},
  eprint = {2409.14586},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2409.14586},
  urldate = {2025-07-08},
  abstract = {Text generation has a fundamental limitation almost by definition: there is no taking back tokens that have been generated, even when they are clearly problematic. In the context of language model safety, when a partial unsafe generation is produced, language models by their nature tend to happily keep on generating similarly unsafe additional text. This is in fact how safety alignment of frontier models gets circumvented in the wild, despite great efforts in improving their safety. Deviating from the paradigm of approaching safety alignment as prevention (decreasing the probability of harmful responses), we propose backtracking, a technique that allows language models to "undo" and recover from their own unsafe generation through the introduction of a special [RESET] token. Our method can be incorporated into either SFT or DPO training to optimize helpfulness and harmlessness. We show that models trained to backtrack are consistently safer than baseline models: backtracking Llama-3-8B is four times more safe than the baseline model (6.1\textbackslash\% \$\textbackslash to\$ 1.5\textbackslash\%) in our evaluations without regression in helpfulness. Our method additionally provides protection against four adversarial attacks including an adaptive attack, despite not being trained to do so.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/adam/Zotero/storage/AS8MDK6M/Zhang et al. - 2024 - Backtracking Improves Generation Safety.pdf}
}

@misc{zhangBacktrackingImprovesGeneration2024a,
  title = {Backtracking {{Improves Generation Safety}}},
  author = {Zhang, Yiming and Chi, Jianfeng and Nguyen, Hailey and Upasani, Kartikeya and Bikel, Daniel M. and Weston, Jason and Smith, Eric Michael},
  year = 2024,
  month = sep,
  number = {arXiv:2409.14586},
  eprint = {2409.14586},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2409.14586},
  urldate = {2025-07-08},
  abstract = {Text generation has a fundamental limitation almost by definition: there is no taking back tokens that have been generated, even when they are clearly problematic. In the context of language model safety, when a partial unsafe generation is produced, language models by their nature tend to happily keep on generating similarly unsafe additional text. This is in fact how safety alignment of frontier models gets circumvented in the wild, despite great efforts in improving their safety. Deviating from the paradigm of approaching safety alignment as prevention (decreasing the probability of harmful responses), we propose backtracking, a technique that allows language models to "undo" and recover from their own unsafe generation through the introduction of a special [RESET] token. Our method can be incorporated into either SFT or DPO training to optimize helpfulness and harmlessness. We show that models trained to backtrack are consistently safer than baseline models: backtracking Llama-3-8B is four times more safe than the baseline model (6.1\textbackslash\% \$\textbackslash to\$ 1.5\textbackslash\%) in our evaluations without regression in helpfulness. Our method additionally provides protection against four adversarial attacks including an adaptive attack, despite not being trained to do so.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/adam/Zotero/storage/PDPS6H4J/Zhang et al. - 2024 - Backtracking Improves Generation Safety.pdf;/Users/adam/Zotero/storage/SP4VVLXF/2409.html}
}

@unpublished{zhangNegativePreferenceOptimization2024,
  title = {Negative {{Preference Optimization}}: {{From Catastrophic Collapse}} to {{Effective Unlearning}}},
  author = {Zhang, Ruiqi and Lin, Licong and Bai, Yu and Mei, Song},
  year = 2024,
  month = apr,
  journal = {arXiv [cs.LG]},
  abstract = {Large Language Models (LLMs) often memorize sensitive, private, or copyrighted data during pre-training. LLM unlearning aims to eliminate the influence of undesirable data from the pre-trained model while preserving the model's utilities on other tasks. Several practical methods have recently been proposed for LLM unlearning, mostly based on gradient ascent (GA) on the loss of undesirable data. However, on certain unlearning tasks, these methods either fail to effectively unlearn the target data or suffer from catastrophic collapse -- a drastic degradation of the model's utilities. In this paper, we propose Negative Preference Optimization (NPO), a simple alignment-inspired method that could efficiently and effectively unlearn a target dataset. We theoretically show that the progression toward catastrophic collapse by minimizing the NPO loss is exponentially slower than GA. Through experiments on synthetic data and the benchmark TOFU dataset, we demonstrate that NPO-based methods achieve a better balance between unlearning the undesirable data and maintaining the model's utilities. We also observe that NPO-based methods generate more sensible outputs than GA-based methods, whose outputs are often gibberish. Remarkably, on TOFU, NPO-based methods are the first to achieve reasonable unlearning results in forgetting 50\% (or more) of the training data, whereas existing methods already struggle with forgetting 10\% of training data.},
  isbn = {2404.05868}
}

@unpublished{zhaoWhatMakesUnlearning2024,
  title = {What Makes Unlearning Hard and What to Do about It},
  author = {Zhao, Kairan and Kurmanji, Meghdad and B{\u a}rbulescu, George-Octavian and Triantafillou, Eleni and Triantafillou, Peter},
  year = 2024,
  month = jun,
  journal = {arXiv [cs.LG]},
  abstract = {Machine unlearning is the problem of removing the effect of a subset of training data (the ''forget set'') from a trained model without damaging the model's utility e.g. to comply with users' requests to delete their data, or remove mislabeled, poisoned or otherwise problematic data. With unlearning research still being at its infancy, many fundamental open questions exist: Are there interpretable characteristics of forget sets that substantially affect the difficulty of the problem? How do these characteristics affect different state-of-the-art algorithms? With this paper, we present the first investigation aiming to answer these questions. We identify two key factors affecting unlearning difficulty and the performance of unlearning algorithms. Evaluation on forget sets that isolate these identified factors reveals previously-unknown behaviours of state-of-the-art algorithms that don't materialize on random forget sets. Based on our insights, we develop a framework coined Refined-Unlearning Meta-algorithm (RUM) that encompasses: (i) refining the forget set into homogenized subsets, according to different characteristics; and (ii) a meta-algorithm that employs existing algorithms to unlearn each subset and finally delivers a model that has unlearned the overall forget set. We find that RUM substantially improves top-performing unlearning algorithms. Overall, we view our work as an important step in (i) deepening our scientific understanding of unlearning and (ii) revealing new pathways to improving the state-of-the-art.},
  isbn = {2406.01257}
}

@misc{zhengJudgingLLMJudgeMTBench2023,
  title = {Judging {{LLM-as-a-Judge}} with {{MT-Bench}} and {{Chatbot Arena}}},
  author = {Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric P. and Zhang, Hao and Gonzalez, Joseph E. and Stoica, Ion},
  year = 2023,
  month = dec,
  number = {arXiv:2306.05685},
  eprint = {2306.05685},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2306.05685},
  urldate = {2024-11-30},
  abstract = {Evaluating large language model (LLM) based chat assistants is challenging due to their broad capabilities and the inadequacy of existing benchmarks in measuring human preferences. To address this, we explore using strong LLMs as judges to evaluate these models on more open-ended questions. We examine the usage and limitations of LLM-as-a-judge, including position, verbosity, and self-enhancement biases, as well as limited reasoning ability, and propose solutions to mitigate some of them. We then verify the agreement between LLM judges and human preferences by introducing two benchmarks: MT-bench, a multi-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our results reveal that strong LLM judges like GPT-4 can match both controlled and crowdsourced human preferences well, achieving over 80\% agreement, the same level of agreement between humans. Hence, LLM-as-a-judge is a scalable and explainable way to approximate human preferences, which are otherwise very expensive to obtain. Additionally, we show our benchmark and traditional benchmarks complement each other by evaluating several variants of LLaMA and Vicuna. The MT-bench questions, 3K expert votes, and 30K conversations with human preferences are publicly available at https://github.com/lm-sys/FastChat/tree/main/fastchat/llm\_judge.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/adam/Zotero/storage/623GRMDR/Zheng et al. - 2023 - Judging LLM-as-a-Judge with MT-Bench and Chatbot A.pdf;/Users/adam/Zotero/storage/CNC63DFI/2306.html}
}

@misc{zhengLargeLanguageModels2024,
  title = {Large {{Language Models Are Not Robust Multiple Choice Selectors}}},
  author = {Zheng, Chujie and Zhou, Hao and Meng, Fandong and Zhou, Jie and Huang, Minlie},
  year = 2024,
  month = feb,
  number = {arXiv:2309.03882},
  eprint = {2309.03882},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-07-20},
  abstract = {Multiple choice questions (MCQs) serve as a common yet important task format in the evaluation of large language models (LLMs). This work shows that modern LLMs are vulnerable to option position changes in MCQs due to their inherent "selection bias", namely, they prefer to select specific option IDs as answers (like "Option A"). Through extensive empirical analyses with 20 LLMs on three benchmarks, we pinpoint that this behavioral bias primarily stems from LLMs' token bias, where the model a priori assigns more probabilistic mass to specific option ID tokens (e.g., A/B/C/D) when predicting answers from the option IDs. To mitigate selection bias, we propose a label-free, inference-time debiasing method, called PriDe, which separates the model's prior bias for option IDs from the overall prediction distribution. PriDe first estimates the prior by permutating option contents on a small number of test samples, and then applies the estimated prior to debias the remaining samples. We demonstrate that it achieves interpretable and transferable debiasing with high computational efficiency. We hope this work can draw broader research attention to the bias and robustness of modern LLMs.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/adam/Zotero/storage/WRG688EL/Zheng et al. - 2024 - Large Language Models Are Not Robust Multiple Choi.pdf}
}

@misc{zhouImproveCrossArchitectureGeneralization2024,
  title = {Improve {{Cross-Architecture Generalization}} on {{Dataset Distillation}}},
  author = {Zhou, Binglin and Zhong, Linhao and Chen, Wentao},
  year = 2024,
  month = feb,
  number = {arXiv:2402.13007},
  eprint = {2402.13007},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2402.13007},
  urldate = {2025-07-08},
  abstract = {Dataset distillation, a pragmatic approach in machine learning, aims to create a smaller synthetic dataset from a larger existing dataset. However, existing distillation methods primarily adopt a model-based paradigm, where the synthetic dataset inherits model-specific biases, limiting its generalizability to alternative models. In response to this constraint, we propose a novel methodology termed "model pool". This approach involves selecting models from a diverse model pool based on a specific probability distribution during the data distillation process. Additionally, we integrate our model pool with the established knowledge distillation approach and apply knowledge distillation to the test process of the distilled dataset. Our experimental results validate the effectiveness of the model pool approach across a range of existing models while testing, demonstrating superior performance compared to existing methodologies.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/adam/Zotero/storage/7N5ZCMNF/Zhou et al. - 2024 - Improve Cross-Architecture Generalization on Datas.pdf;/Users/adam/Zotero/storage/E88WDHLB/2402.html}
}

@article{zhouQuantifyingAnalyzingEntityLevel2024,
  title = {Quantifying and {{Analyzing Entity-Level Memorization}} in {{Large Language Models}}},
  author = {Zhou, Zhenhong and Xiang, Jiuyang and Chen, Chaomeng and Su, Sen},
  year = 2024,
  month = mar,
  journal = {AAAI},
  volume = {38},
  number = {17},
  pages = {19741--19749},
  issn = {2374-3468},
  doi = {10.1609/aaai.v38i17.29948},
  urldate = {2024-07-19},
  abstract = {Large language models (LLMs) have been proven capable of memorizing their training data, which can be extracted through specifically designed prompts. As the scale of datasets continues to grow, privacy risks arising from memorization have attracted increasing attention. Quantifying language model memorization helps evaluate potential privacy risks. However, prior works on quantifying memorization require access to the precise original data or incur substantial computational overhead, making it difficult for applications in real-world language models. To this end, we propose a fine-grained, entity-level definition to quantify memorization with conditions and metrics closer to real-world scenarios. In addition, we also present an approach for efficiently extracting sensitive entities from autoregressive language models. We conduct extensive experiments based on the proposed, probing language models' ability to reconstruct sensitive entities under different settings. We find that language models have strong memorization at the entity level and are able to reproduce the training data even with partial leakages. The results demonstrate that LLMs not only memorize their training data but also understand associations between entities. These findings necessitate that trainers of LLMs exercise greater prudence regarding model memorization, adopting memorization mitigation techniques to preclude privacy violations.},
  keywords = {NLP: (Large) Language Models,NLP: Ethics -- Bias Fairness Transparency & Privacy}
}

@misc{ziaHowLLMUnlearning2024,
  title = {How {{LLM Unlearning Is Shaping}} the {{Future}} of {{AI Privacy}}},
  author = {Zia, Dr Tehseen},
  year = 2024,
  month = oct,
  journal = {Unite.AI},
  urldate = {2024-11-26},
  abstract = {The rapid development of Large Language Models (LLMs) has brought about significant advancements in artificial intelligence (AI). From automating content creation to providing support in healthcare, law, and finance, LLMs are reshaping industries with their capacity to understand and generate human-like text. However, as these models expand in use, so do concerns over privacy and [\dots ]},
  chapter = {Artificial Intelligence},
  howpublished = {https://www.unite.ai/how-llm-unlearning-is-shaping-the-future-of-ai-privacy/},
  langid = {american},
  file = {/Users/adam/Zotero/storage/4N3W8AHD/how-llm-unlearning-is-shaping-the-future-of-ai-privacy.html}
}

@article{zotero-109,
  type = {Article}
}

@misc{zotero-113,
  journal = {CPlusPlus.com},
  urldate = {2022-11-01},
  howpublished = {https://cplusplus.com/doc/tutorial/program\_structure/},
  file = {/Users/adam/Zotero/storage/FD5B48YY/program_structure.html}
}

@misc{zotero-114,
  urldate = {2022-10-31},
  howpublished = {https://cplusplus.com/doc/tutorial/}
}

@misc{zotero-117,
  urldate = {2022-10-31},
  howpublished = {https://cplusplus.com/doc/tutorial/},
  file = {/Users/adam/Zotero/storage/T3ZA2SZC/tutorial.html}
}

@misc{zotero-16,
  urldate = {2023-10-31},
  howpublished = {https://homepages.uc.edu/\textasciitilde thomam/Net1/Packet\_Formats/udp.html},
  file = {/Users/adam/Zotero/storage/HPWWB6UG/udp.html}
}

@misc{zotero-198,
  type = {Misc}
}

@misc{ZoteroConnectorNot,
  title = {Zotero Connector Not Working - {{Google Search}}},
  urldate = {2022-09-25},
  howpublished = {https://www.google.com/search?q=zotero+connector+not+working\&oq=zotero+connector+not+working\&aqs=chrome.0.0i512l2j0i22i30l5j0i15i22i30j0i10i22i30j0i390.4506j0j7\&sourceid=chrome\&ie=UTF-8},
  file = {/Users/adam/Zotero/storage/8FAGMXRQ/search.html}
}

@unpublished{zouUniversalTransferableAdversarial2023,
  title = {Universal and {{Transferable Adversarial Attacks}} on {{Aligned Language Models}}},
  author = {Zou, Andy and Wang, Zifan and Carlini, Nicholas and Nasr, Milad and Zico Kolter, J and Fredrikson, Matt},
  year = 2023,
  month = jul,
  journal = {arXiv [cs.CL]},
  abstract = {Because "out-of-the-box" large language models are capable of generating a great deal of objectionable content, recent work has focused on aligning these models in an attempt to prevent undesirable generation. While there has been some success at circumventing these measures -- so-called "jailbreaks" against LLMs -- these attacks have required significant human ingenuity and are brittle in practice. In this paper, we propose a simple and effective attack method that causes aligned language models to generate objectionable behaviors. Specifically, our approach finds a suffix that, when attached to a wide range of queries for an LLM to produce objectionable content, aims to maximize the probability that the model produces an affirmative response (rather than refusing to answer). However, instead of relying on manual engineering, our approach automatically produces these adversarial suffixes by a combination of greedy and gradient-based search techniques, and also improves over past automatic prompt generation methods. Surprisingly, we find that the adversarial prompts generated by our approach are quite transferable, including to black-box, publicly released LLMs. Specifically, we train an adversarial attack suffix on multiple prompts (i.e., queries asking for many different types of objectionable content), as well as multiple models (in our case, Vicuna-7B and 13B). When doing so, the resulting attack suffix is able to induce objectionable content in the public interfaces to ChatGPT, Bard, and Claude, as well as open source LLMs such as LLaMA-2-Chat, Pythia, Falcon, and others. In total, this work significantly advances the state-of-the-art in adversarial attacks against aligned language models, raising important questions about how such systems can be prevented from producing objectionable information. Code is available at github.com/llm-attacks/llm-attacks.},
  isbn = {2307.15043}
}

@misc{zouUniversalTransferableAdversarial2023a,
  title = {Universal and {{Transferable Adversarial Attacks}} on {{Aligned Language Models}}},
  author = {Zou, Andy and Wang, Zifan and Carlini, Nicholas and Nasr, Milad and Kolter, J. Zico and Fredrikson, Matt},
  year = 2023,
  month = dec,
  number = {arXiv:2307.15043},
  eprint = {2307.15043},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2307.15043},
  urldate = {2025-01-31},
  abstract = {Because "out-of-the-box" large language models are capable of generating a great deal of objectionable content, recent work has focused on aligning these models in an attempt to prevent undesirable generation. While there has been some success at circumventing these measures -- so-called "jailbreaks" against LLMs -- these attacks have required significant human ingenuity and are brittle in practice. In this paper, we propose a simple and effective attack method that causes aligned language models to generate objectionable behaviors. Specifically, our approach finds a suffix that, when attached to a wide range of queries for an LLM to produce objectionable content, aims to maximize the probability that the model produces an affirmative response (rather than refusing to answer). However, instead of relying on manual engineering, our approach automatically produces these adversarial suffixes by a combination of greedy and gradient-based search techniques, and also improves over past automatic prompt generation methods. Surprisingly, we find that the adversarial prompts generated by our approach are quite transferable, including to black-box, publicly released LLMs. Specifically, we train an adversarial attack suffix on multiple prompts (i.e., queries asking for many different types of objectionable content), as well as multiple models (in our case, Vicuna-7B and 13B). When doing so, the resulting attack suffix is able to induce objectionable content in the public interfaces to ChatGPT, Bard, and Claude, as well as open source LLMs such as LLaMA-2-Chat, Pythia, Falcon, and others. In total, this work significantly advances the state-of-the-art in adversarial attacks against aligned language models, raising important questions about how such systems can be prevented from producing objectionable information. Code is available at github.com/llm-attacks/llm-attacks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Cryptography and Security,Computer Science - Machine Learning},
  file = {/Users/adam/Zotero/storage/3YSDXW9A/Zou et al. - 2023 - Universal and Transferable Adversarial Attacks on .pdf;/Users/adam/Zotero/storage/LXR28XS4/2307.html}
}

@misc{zweigerSelfAdaptingLanguageModels2025,
  title = {Self-{{Adapting Language Models}}},
  author = {Zweiger, Adam and Pari, Jyothish and Guo, Han and Aky{\"u}rek, Ekin and Kim, Yoon and Agrawal, Pulkit},
  year = 2025,
  month = jun,
  number = {arXiv:2506.10943},
  eprint = {2506.10943},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2506.10943},
  urldate = {2025-06-30},
  abstract = {Large language models (LLMs) are powerful but static; they lack mechanisms to adapt their weights in response to new tasks, knowledge, or examples. We introduce Self-Adapting LLMs (SEAL), a framework that enables LLMs to self-adapt by generating their own finetuning data and update directives. Given a new input, the model produces a self-edit-a generation that may restructure the information in different ways, specify optimization hyperparameters, or invoke tools for data augmentation and gradient-based updates. Through supervised finetuning (SFT), these self-edits result in persistent weight updates, enabling lasting adaptation. To train the model to produce effective self-edits, we use a reinforcement learning loop with the downstream performance of the updated model as the reward signal. Unlike prior approaches that rely on separate adaptation modules or auxiliary networks, SEAL directly uses the model's own generation to control its adaptation process. Experiments on knowledge incorporation and few-shot generalization show that SEAL is a promising step toward language models capable of self-directed adaptation. Our website and code is available at https://jyopari.github.io/posts/seal.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/adam/Zotero/storage/QDBIEAGQ/Zweiger et al. - 2025 - Self-Adapting Language Models.pdf;/Users/adam/Zotero/storage/QQ9SWGLL/2506.html}
}
